{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline # Same, but with the latter it is not necessary to name estimator and transformer\n",
    "#from imblearn.pipeline import Pipeline as Imb_Pipe\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, GenericUnivariateSelect, mutual_info_classif\n",
    "import eli5\n",
    "\n",
    "# Predictive Modeling (Models)\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_predict, cross_val_score, cross_validate, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, PassiveAggressiveRegressor, ElasticNet, SGDRegressor, RANSACRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor, VotingClassifier, RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer, fbeta_score, accuracy_score, confusion_matrix, f1_score, precision_recall_curve, recall_score, precision_score\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import data_engineered\n",
    "data = pd.read_pickle(\"saves/data_engineered.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Alternative: Import from csv\n",
    "#data_types_engineered = pd.read_csv('saves/types_engineered.csv')['types']\n",
    "#data = pd.read_csv(\"saves/data_engineered.csv\", dtype=data_types_engineered.to_dict())\n",
    "#data.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Dashboard\n",
    "target = 'occupancy_class' # for regression: 'occupancy_rate', 'price_log' | for classification: 'occupancy_class'\n",
    "drop_cols = ['occupancy_rate'] # additional columns to drop\n",
    "scoring = 'f1' # for regression: 'neg_mean_squared_error' | for classification: \"f1\", \"recall\", \"precision\", \"accuracy\", \"roc_auc\"\n",
    "test_size = 0.2\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (Train/Test Split and Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cancellation_policy', 'property_type', 'room_type']"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list for categorical predictors/features (used in \"Scaling with Preprocessing Pipeline\") \n",
    "cat_features = list(data.columns[data.dtypes==object])\n",
    "#cat_features.remove(\"neighbourhood\")\n",
    "#cat_features.remove(\"zipcode\")\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accommodates_per_bed',\n",
       " 'accommodates_per_room',\n",
       " 'am_balcony',\n",
       " 'am_breakfast',\n",
       " 'am_child_friendly',\n",
       " 'am_elevator',\n",
       " 'am_essentials',\n",
       " 'am_nature_and_views',\n",
       " 'am_pets_allowed',\n",
       " 'am_private_entrance',\n",
       " 'am_smoking_allowed',\n",
       " 'am_tv',\n",
       " 'am_white_goods',\n",
       " 'bathrooms_log',\n",
       " 'bedrooms',\n",
       " 'binary_chg',\n",
       " 'calculated_host_listings_count',\n",
       " 'host_is_superhost',\n",
       " 'instant_bookable',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'maximum_nights',\n",
       " 'minimum_nights_chg',\n",
       " 'minimum_nights_log',\n",
       " 'numeric_chg',\n",
       " 'price_chg_2020_01',\n",
       " 'price_extra_fees_sqrt',\n",
       " 'price_extra_people',\n",
       " 'price_log',\n",
       " 'review_scores_rating_sqrt',\n",
       " 'text_len_chg',\n",
       " 'text_len_sqrt',\n",
       " 'wk_mth_discount']"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list for numerical predictors/features (removing target column, used in \"Scaling with Preprocessing Pipeline\")\n",
    "num_features = list(data.columns[data.dtypes!=object])\n",
    "num_features.remove(target)\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Build preprocessor pipeline\n",
    "# Pipeline for numerical features\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features \n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('1hot', OneHotEncoder(drop='first', handle_unknown='error'))\n",
    "])\n",
    "\n",
    "# Complete pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Function for getting column names after preprocessing\n",
    "def get_column_names_from_ColumnTransformer(column_transformer):    \n",
    "    col_name = []\n",
    "    for transformer_in_columns in column_transformer.transformers_[:-1]:#the last transformer is ColumnTransformer's 'remainder'\n",
    "        raw_col_name = transformer_in_columns[2]\n",
    "        if isinstance(transformer_in_columns[1],Pipeline): \n",
    "            transformer = transformer_in_columns[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = transformer_in_columns[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError: # if no 'get_feature_names' function, use raw column name\n",
    "            names = raw_col_name\n",
    "        if isinstance(names,np.ndarray): # eg.\n",
    "            col_name += names.tolist()\n",
    "        elif isinstance(names,list):\n",
    "            col_name += names    \n",
    "        elif isinstance(names,str):\n",
    "            col_name.append(names)\n",
    "    return col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define predictors and target variable\n",
    "X = data.drop([target], axis=1)\n",
    "y = data[target]\n",
    "X = X.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "#                                                   stratify=y) # Use stratify=y if labels are inbalanced (e.g. most wines are 5 or 6; check with value_counts()!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving preprocessed X_train and X_test\n",
    "X_train_prep_preprocessor = preprocessor.fit(X_train)\n",
    "X_train_prep_cols = get_column_names_from_ColumnTransformer(X_train_prep_preprocessor)\n",
    "\n",
    "X_train_prep = X_train_prep_preprocessor.transform(X_train)\n",
    "X_train_num_prep = num_pipeline.fit_transform(X_train[num_features])\n",
    "X_test_prep = X_train_prep_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "#train_outl = num_pipeline.fit_transform(X_train[num_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Fit DBSCAN model\n",
    "#outl_model = DBSCAN(eps=3.0, min_samples=10).fit(train_outl)\n",
    "#outl_labels = outl_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display results (# of outliers)\n",
    "#pd.Series(outl_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Illustrate results\n",
    "#plt.figure(figsize=(10,10))\n",
    "#\n",
    "#unique_labels = set(outl_labels)\n",
    "#colors = ['blue', 'red']\n",
    "#\n",
    "#for color,label in zip(colors, unique_labels):\n",
    "#    sample_mask = [True if l == label else False for l in outl_labels]\n",
    "#    plt.plot(train_outl[:,0][sample_mask], train_outl[:, 1][sample_mask], 'o', color=color);\n",
    "#plt.xlabel('accommodates_per_bed');\n",
    "#plt.ylabel('accommodates_per_room');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the results:\n",
    "\n",
    "- https://www.kaggle.com/kevinarvai/outlier-detection-practice-uni-multivariate\n",
    "- https://datascience.stackexchange.com/questions/46092/how-do-we-interpret-the-outputs-of-dbscan-clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Feature Selection (add most useful to modeling pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Set X_fs to desired variable\n",
    "X_fs = X_train[num_features]    # X_train_prep, X_train_num_prep, X_train[num_features]\n",
    "#X_fs = pd.DataFrame(X_fs, columns = X_train_prep_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GenericUnivariateSelect** (Classification and Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Apply GenericUnivariateSelect\n",
    "trans_GUS = GenericUnivariateSelect(score_func=lambda X, y: X.mean(axis=0), mode='k_best', param=15) #mode='percentile', 'k_best'\n",
    "X_train_GUS = trans_GUS.fit_transform(X_fs, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mutual_info_classif** (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-498-5c9e05164d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit mutual_info_classif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_mic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutual_info_classif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/feature_selection/_mutual_info.py\u001b[0m in \u001b[0;36mmutual_info_classif\u001b[0;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    444\u001b[0m            \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mRandom\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProbl\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPeredachi\u001b[0m \u001b[0mInf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1987\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     return _estimate_mi(X, y, discrete_features, True, n_neighbors,\n\u001b[1;32m    448\u001b[0m                         copy, random_state)\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "# Fit mutual_info_classif\n",
    "X_train_mic = mutual_info_classif(X_fs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.subplots(1, figsize=(26, 1))\n",
    "sns.heatmap(X_train_mic[:, np.newaxis].T, cmap='Blues', cbar=False, linewidths=1, annot=True)\n",
    "plt.yticks([], [])\n",
    "plt.gca().set_xticklabels(X_fs.columns, rotation=45, ha='right', fontsize=12)\n",
    "plt.suptitle(\"Feature Importance (mutual_info_classif)\", fontsize=18, y=1.2)\n",
    "plt.gcf().subplots_adjust(wspace=0.2)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Apply GenericUnivariateSelect to reduce features (optional)\n",
    "trans_mic = GenericUnivariateSelect(score_func=mutual_info_classif, mode='k_best', param=15) #mode='percentile', 'k_best', \n",
    "X_train_mic_GUS = trans_mic.fit_transform(X_fs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Print kept features\n",
    "print(\"We started with {0} features but retained only {1} of them!\".format(\n",
    "    X_fs.shape[1] - 1, X_train_mic_GUS.shape[1]))\n",
    "\n",
    "columns_retained_Select = X_fs.columns[trans_mic.get_support()].values\n",
    "pd.DataFrame(X_train_mic_GUS, columns=columns_retained_Select).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chi2** (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mutual_info_regression** (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling: Classification (\"occupancy_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using \u001b[1mOCCUPANCY_CLASS\u001b[0m as the target\n"
     ]
    }
   ],
   "source": [
    "# Print current setting for TARGET\n",
    "target_upper = target.upper()\n",
    "print(\"You are currently using \" + f\"\\033[1m{target_upper}\\033[0m\" + \" as the target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Select dataset to use\n",
    "#X_train = X_train_mic_GUS_red       # X_train, X_train_GUS, X_train_mic, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Select models for comparison\n",
    "models={'Baseline': DummyClassifier(strategy='most_frequent'),\n",
    "        'LogReg': LogisticRegression(max_iter=1000),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'SVC': SVC(kernel='rbf', C=1E6),\n",
    "        'Decision Tree': DecisionTreeClassifier(criterion=\"gini\", max_depth=3,random_state=random_state),\n",
    "        'Random Forest': RandomForestClassifier(random_state=random_state, max_features='sqrt', n_jobs=-1),\n",
    "        'Gradient Boost': GradientBoostingClassifier(random_state=random_state),\n",
    "        'XGBoost': XGBClassifier(),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=random_state)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.6s finished\n",
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Baseline: \n",
      "[[5740    0    0    0    0    0]\n",
      " [3857    0    0    0    0    0]\n",
      " [2090    0    0    0    0    0]\n",
      " [1200    0    0    0    0    0]\n",
      " [ 855    0    0    0    0    0]\n",
      " [2794    0    0    0    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix LogReg: \n",
      "[[4551  887   22    4    1  275]\n",
      " [1832 1628   33    4    4  356]\n",
      " [ 797  768   52    9    4  460]\n",
      " [ 362  325   32    7    2  472]\n",
      " [ 204  208   22    4    3  414]\n",
      " [ 565  434   47   10    1 1737]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    7.6s remaining:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   11.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix KNN: \n",
      "[[4317  921  233   54   20  195]\n",
      " [1930 1255  318   80   43  231]\n",
      " [ 856  576  218   88   58  294]\n",
      " [ 380  283  146   75   50  266]\n",
      " [ 254  166   99   62   49  225]\n",
      " [ 671  357  240  162  122 1242]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.6min remaining:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix SVC: \n",
      "[[3613 1159  403  184  119  262]\n",
      " [1383 1398  556  207   87  226]\n",
      " [ 565  636  359  176  110  244]\n",
      " [ 300  254  193  123   96  234]\n",
      " [ 174  176  118  118   80  189]\n",
      " [ 427  330  342  278  217 1200]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished\n",
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Decision Tree: \n",
      "[[2913 2730    0    0    0   97]\n",
      " [ 123 3625    0    0    0  109]\n",
      " [  31 1810    0    0    0  249]\n",
      " [   4  885    0    0    0  311]\n",
      " [   2  577    0    0    0  276]\n",
      " [   8 1589    0    0    0 1197]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    4.0s remaining:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Random Forest: \n",
      "[[4828  780   36    3    0   93]\n",
      " [1120 2291  180   20    2  244]\n",
      " [ 413 1061  154   22   11  429]\n",
      " [ 189  424   92   47   16  432]\n",
      " [ 107  241   57   25   15  410]\n",
      " [ 294  427   94   24   28 1927]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   32.8s remaining:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   53.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Gradient Boost: \n",
      "[[4638  966   27    4    2  103]\n",
      " [ 822 2614  144   10    8  259]\n",
      " [ 258 1199  148   34   16  435]\n",
      " [ 122  495   83   27    8  465]\n",
      " [  67  287   52   19   10  420]\n",
      " [ 174  520  109   22   14 1955]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   35.6s remaining:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   52.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix XGBoost: \n",
      "[[4652  972   10    3    0  103]\n",
      " [ 900 2624   61    6    3  263]\n",
      " [ 318 1206   85   13    1  467]\n",
      " [ 157  495   59   14    1  474]\n",
      " [  88  290   34    6    1  436]\n",
      " [ 234  528   68   15    0 1949]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix AdaBoost: \n",
      "[[4232 1309   13    2    0  184]\n",
      " [ 663 2700  126    9    0  359]\n",
      " [ 260 1153  128   23    0  526]\n",
      " [ 103  501   70   10    0  516]\n",
      " [  62  299   44   10    0  440]\n",
      " [ 165  558   90    9    0 1972]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.7s finished\n",
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.443336</td>\n",
       "      <td>2.538373</td>\n",
       "      <td>-0.916536</td>\n",
       "      <td>1.755382</td>\n",
       "      <td>0.347121</td>\n",
       "      <td>0.347121</td>\n",
       "      <td>0.120493</td>\n",
       "      <td>0.178890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>3.464623</td>\n",
       "      <td>1.861350</td>\n",
       "      <td>-0.030534</td>\n",
       "      <td>1.132620</td>\n",
       "      <td>0.482463</td>\n",
       "      <td>0.482463</td>\n",
       "      <td>0.413757</td>\n",
       "      <td>0.415630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>3.545053</td>\n",
       "      <td>1.882831</td>\n",
       "      <td>-0.054457</td>\n",
       "      <td>1.198657</td>\n",
       "      <td>0.432753</td>\n",
       "      <td>0.432753</td>\n",
       "      <td>0.385876</td>\n",
       "      <td>0.397818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC</td>\n",
       "      <td>3.398948</td>\n",
       "      <td>1.843624</td>\n",
       "      <td>-0.010999</td>\n",
       "      <td>1.203495</td>\n",
       "      <td>0.409591</td>\n",
       "      <td>0.409591</td>\n",
       "      <td>0.399741</td>\n",
       "      <td>0.403270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>2.850871</td>\n",
       "      <td>1.688452</td>\n",
       "      <td>0.152023</td>\n",
       "      <td>1.040639</td>\n",
       "      <td>0.467767</td>\n",
       "      <td>0.467767</td>\n",
       "      <td>0.493910</td>\n",
       "      <td>0.421824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>2.427854</td>\n",
       "      <td>1.558157</td>\n",
       "      <td>0.277848</td>\n",
       "      <td>0.876209</td>\n",
       "      <td>0.560111</td>\n",
       "      <td>0.560111</td>\n",
       "      <td>0.502214</td>\n",
       "      <td>0.505851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boost</td>\n",
       "      <td>2.314163</td>\n",
       "      <td>1.521237</td>\n",
       "      <td>0.311664</td>\n",
       "      <td>0.850931</td>\n",
       "      <td>0.567973</td>\n",
       "      <td>0.567973</td>\n",
       "      <td>0.514871</td>\n",
       "      <td>0.516032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>2.445997</td>\n",
       "      <td>1.563968</td>\n",
       "      <td>0.272451</td>\n",
       "      <td>0.878387</td>\n",
       "      <td>0.563921</td>\n",
       "      <td>0.563921</td>\n",
       "      <td>0.503958</td>\n",
       "      <td>0.502115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>2.586538</td>\n",
       "      <td>1.608272</td>\n",
       "      <td>0.230648</td>\n",
       "      <td>0.920356</td>\n",
       "      <td>0.546807</td>\n",
       "      <td>0.546807</td>\n",
       "      <td>0.493645</td>\n",
       "      <td>0.495031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model       MSE      RMSE        R2       MAE  Accuracy    Recall  \\\n",
       "0        Baseline  6.443336  2.538373 -0.916536  1.755382  0.347121  0.347121   \n",
       "1          LogReg  3.464623  1.861350 -0.030534  1.132620  0.482463  0.482463   \n",
       "2             KNN  3.545053  1.882831 -0.054457  1.198657  0.432753  0.432753   \n",
       "3             SVC  3.398948  1.843624 -0.010999  1.203495  0.409591  0.409591   \n",
       "4   Decision Tree  2.850871  1.688452  0.152023  1.040639  0.467767  0.467767   \n",
       "5   Random Forest  2.427854  1.558157  0.277848  0.876209  0.560111  0.560111   \n",
       "6  Gradient Boost  2.314163  1.521237  0.311664  0.850931  0.567973  0.567973   \n",
       "7         XGBoost  2.445997  1.563968  0.272451  0.878387  0.563921  0.563921   \n",
       "8        AdaBoost  2.586538  1.608272  0.230648  0.920356  0.546807  0.546807   \n",
       "\n",
       "   Precision  F1 Score  \n",
       "0   0.120493  0.178890  \n",
       "1   0.413757  0.415630  \n",
       "2   0.385876  0.397818  \n",
       "3   0.399741  0.403270  \n",
       "4   0.493910  0.421824  \n",
       "5   0.502214  0.505851  \n",
       "6   0.514871  0.516032  \n",
       "7   0.503958  0.502115  \n",
       "8   0.493645  0.495031  "
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and display results\n",
    "results = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'R2'])\n",
    "i = 0\n",
    "for m in models.items():\n",
    "    # Building a full pipeline with our preprocessor and a Classifier\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), (m[0], m[1])])\n",
    "    # Making predictions on the training set using cross validation as well as calculating the probabilities\n",
    "    y_train_pred = cross_val_predict(pipe,\n",
    "                                     X_train,\n",
    "                                     y_train.values.ravel(),\n",
    "                                     cv=5,\n",
    "                                     verbose=4,\n",
    "                                     n_jobs=-1)\n",
    "    # Calculating metrices\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            'Model': m[0],\n",
    "            'MSE': mean_squared_error(y_train, y_train_pred),\n",
    "            'RMSE': mean_squared_error(y_train, y_train_pred, squared=False),\n",
    "            'MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "            'R2': r2_score(y_train, y_train_pred),\n",
    "            'Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'Recall': recall_score(y_train, y_train_pred, average=\"weighted\"),\n",
    "            'Precision': precision_score(y_train, y_train_pred, average=\"weighted\"),\n",
    "            'F1 Score': f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "        },\n",
    "        index=[i])\n",
    "    print(f\"Confusion Matrix {m[0]}: \\n\" + str(confusion_matrix(y_train, y_train_pred)))\n",
    "    i += 1\n",
    "    results = pd.concat([results, temp])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define param_grid (TO-DO)\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation with Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Transform X_test for final evaluation\n",
    "#X_test_prep = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "#y_pred_rf_clf = best_model_rf_clf.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred_rf_clf)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred_rf_clf)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred_rf_clf)))\n",
    "#print(\"F1 Score: {:.2f}\".format(f1_score(y_test, y_pred_rf_clf)))\n",
    "#print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_test, y_pred_rf_clf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling: Regression (\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using \u001b[1mPRICE_LOG\u001b[0m as the target\n"
     ]
    }
   ],
   "source": [
    "# Print current setting for TARGET\n",
    "target_upper = target.upper()\n",
    "print(\"You are currently using \" + f\"\\033[1m{target_upper}\\033[0m\" + \" as the target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-506-fc5729bfa757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Transform X_train_prep and y_train to required format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_prep_ols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_prep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# OneHotEncoder outputs csr_matrix, which gives an error when trying to add a constant. Hence, transformed into numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train_prep_ols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_prep_ols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train_ols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "# Transform X_train_prep and y_train to required format\n",
    "X_train_prep_ols = X_train_prep.toarray()            # OneHotEncoder outputs csr_matrix, which gives an error when trying to add a constant. Hence, transformed into numpy array\n",
    "X_train_prep_ols = sm.add_constant(X_train_prep_ols)\n",
    "y_train_ols = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize and fit model\n",
    "reg_ols = sm.OLS(y_train_ols, X_train_prep_ols).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.545</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.538</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   86.43</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 22 Jul 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:56:52</td>     <th>  Log-Likelihood:    </th> <td> -8469.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 16914</td>      <th>  AIC:               </th> <td>1.740e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 16682</td>      <th>  BIC:               </th> <td>1.920e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   231</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    3.9341</td> <td>    0.050</td> <td>   78.766</td> <td> 0.000</td> <td>    3.836</td> <td>    4.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.1442</td> <td>    0.005</td> <td>   27.533</td> <td> 0.000</td> <td>    0.134</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0272</td> <td>    0.004</td> <td>    7.538</td> <td> 0.000</td> <td>    0.020</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0122</td> <td>    0.003</td> <td>    3.847</td> <td> 0.000</td> <td>    0.006</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0031</td> <td>    0.003</td> <td>    0.884</td> <td> 0.377</td> <td>   -0.004</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0035</td> <td>    0.004</td> <td>    0.905</td> <td> 0.365</td> <td>   -0.004</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.0379</td> <td>    0.003</td> <td>   11.034</td> <td> 0.000</td> <td>    0.031</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.0104</td> <td>    0.003</td> <td>    3.196</td> <td> 0.001</td> <td>    0.004</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.0009</td> <td>    0.003</td> <td>   -0.282</td> <td> 0.778</td> <td>   -0.007</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0169</td> <td>    0.004</td> <td>    4.492</td> <td> 0.000</td> <td>    0.010</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.0097</td> <td>    0.003</td> <td>   -3.014</td> <td> 0.003</td> <td>   -0.016</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    0.0249</td> <td>    0.003</td> <td>    7.373</td> <td> 0.000</td> <td>    0.018</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>   -0.0216</td> <td>    0.003</td> <td>   -6.524</td> <td> 0.000</td> <td>   -0.028</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    0.0394</td> <td>    0.004</td> <td>   11.239</td> <td> 0.000</td> <td>    0.033</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    0.0009</td> <td>    0.003</td> <td>    0.279</td> <td> 0.781</td> <td>   -0.005</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>    0.0988</td> <td>    0.004</td> <td>   25.350</td> <td> 0.000</td> <td>    0.091</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.0321</td> <td>    0.003</td> <td>    9.490</td> <td> 0.000</td> <td>    0.025</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    0.0771</td> <td>    0.009</td> <td>    8.195</td> <td> 0.000</td> <td>    0.059</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -0.0229</td> <td>    0.009</td> <td>   -2.530</td> <td> 0.011</td> <td>   -0.041</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>   -0.0315</td> <td>    0.004</td> <td>   -7.536</td> <td> 0.000</td> <td>   -0.040</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    0.0036</td> <td>    0.006</td> <td>    0.656</td> <td> 0.512</td> <td>   -0.007</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.0042</td> <td>    0.006</td> <td>    0.727</td> <td> 0.467</td> <td>   -0.007</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>-3.958e-05</td> <td>    0.004</td> <td>   -0.011</td> <td> 0.991</td> <td>   -0.007</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>   -0.0459</td> <td>    0.004</td> <td>  -12.724</td> <td> 0.000</td> <td>   -0.053</td> <td>   -0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.0229</td> <td>    0.004</td> <td>    6.264</td> <td> 0.000</td> <td>    0.016</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>    0.0008</td> <td>    0.004</td> <td>    0.211</td> <td> 0.833</td> <td>   -0.006</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.0016</td> <td>    0.003</td> <td>   -0.484</td> <td> 0.629</td> <td>   -0.008</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   -0.0078</td> <td>    0.004</td> <td>   -2.036</td> <td> 0.042</td> <td>   -0.015</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.0090</td> <td>    0.009</td> <td>   -0.988</td> <td> 0.323</td> <td>   -0.027</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>   -0.0044</td> <td>    0.009</td> <td>   -0.493</td> <td> 0.622</td> <td>   -0.022</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>    0.0076</td> <td>    0.003</td> <td>    2.411</td> <td> 0.016</td> <td>    0.001</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>   -0.0524</td> <td>    0.003</td> <td>  -15.523</td> <td> 0.000</td> <td>   -0.059</td> <td>   -0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>   -0.0015</td> <td>    0.004</td> <td>   -0.341</td> <td> 0.733</td> <td>   -0.010</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.0143</td> <td>    0.004</td> <td>    3.595</td> <td> 0.000</td> <td>    0.007</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>   -0.0028</td> <td>    0.004</td> <td>   -0.779</td> <td> 0.436</td> <td>   -0.010</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>    0.0484</td> <td>    0.020</td> <td>    2.419</td> <td> 0.016</td> <td>    0.009</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.1249</td> <td>    0.020</td> <td>   -6.251</td> <td> 0.000</td> <td>   -0.164</td> <td>   -0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>    0.0318</td> <td>    0.003</td> <td>    9.768</td> <td> 0.000</td> <td>    0.025</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    0.0072</td> <td>    0.004</td> <td>    1.651</td> <td> 0.099</td> <td>   -0.001</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>   -0.0050</td> <td>    0.004</td> <td>   -1.352</td> <td> 0.176</td> <td>   -0.012</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>   -0.0065</td> <td>    0.004</td> <td>   -1.659</td> <td> 0.097</td> <td>   -0.014</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>   -0.0118</td> <td>    0.005</td> <td>   -2.612</td> <td> 0.009</td> <td>   -0.021</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>    0.0213</td> <td>    0.008</td> <td>    2.638</td> <td> 0.008</td> <td>    0.005</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>    0.0320</td> <td>    0.009</td> <td>    3.697</td> <td> 0.000</td> <td>    0.015</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>    0.1298</td> <td>    0.059</td> <td>    2.188</td> <td> 0.029</td> <td>    0.014</td> <td>    0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>   -0.0785</td> <td>    0.115</td> <td>   -0.683</td> <td> 0.495</td> <td>   -0.304</td> <td>    0.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>   -0.2444</td> <td>    0.131</td> <td>   -1.863</td> <td> 0.062</td> <td>   -0.501</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>   -0.0093</td> <td>    0.147</td> <td>   -0.063</td> <td> 0.950</td> <td>   -0.298</td> <td>    0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>   -0.1694</td> <td>    0.047</td> <td>   -3.635</td> <td> 0.000</td> <td>   -0.261</td> <td>   -0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>   -0.2124</td> <td>    0.166</td> <td>   -1.283</td> <td> 0.199</td> <td>   -0.537</td> <td>    0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>    0.1895</td> <td>    0.111</td> <td>    1.714</td> <td> 0.087</td> <td>   -0.027</td> <td>    0.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>    0.3754</td> <td>    0.068</td> <td>    5.543</td> <td> 0.000</td> <td>    0.243</td> <td>    0.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>    0.0662</td> <td>    0.203</td> <td>    0.326</td> <td> 0.745</td> <td>   -0.332</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>    0.0452</td> <td>    0.136</td> <td>    0.332</td> <td> 0.740</td> <td>   -0.222</td> <td>    0.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>   -0.1292</td> <td>    0.054</td> <td>   -2.381</td> <td> 0.017</td> <td>   -0.236</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>    0.1356</td> <td>    0.095</td> <td>    1.429</td> <td> 0.153</td> <td>   -0.050</td> <td>    0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>    0.1407</td> <td>    0.154</td> <td>    0.913</td> <td> 0.361</td> <td>   -0.162</td> <td>    0.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>    0.2268</td> <td>    0.059</td> <td>    3.864</td> <td> 0.000</td> <td>    0.112</td> <td>    0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>    0.2289</td> <td>    0.113</td> <td>    2.020</td> <td> 0.043</td> <td>    0.007</td> <td>    0.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>    0.0216</td> <td>    0.108</td> <td>    0.200</td> <td> 0.842</td> <td>   -0.191</td> <td>    0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>    0.3021</td> <td>    0.112</td> <td>    2.699</td> <td> 0.007</td> <td>    0.083</td> <td>    0.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>    0.0832</td> <td>    0.127</td> <td>    0.656</td> <td> 0.512</td> <td>   -0.165</td> <td>    0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>   -0.4091</td> <td>    0.283</td> <td>   -1.447</td> <td> 0.148</td> <td>   -0.963</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>    0.2020</td> <td>    0.314</td> <td>    0.644</td> <td> 0.520</td> <td>   -0.413</td> <td>    0.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>    0.2353</td> <td>    0.048</td> <td>    4.947</td> <td> 0.000</td> <td>    0.142</td> <td>    0.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>    0.0043</td> <td>    0.163</td> <td>    0.027</td> <td> 0.979</td> <td>   -0.315</td> <td>    0.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>   -0.2653</td> <td>    0.101</td> <td>   -2.627</td> <td> 0.009</td> <td>   -0.463</td> <td>   -0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>    0.1163</td> <td>    0.130</td> <td>    0.891</td> <td> 0.373</td> <td>   -0.140</td> <td>    0.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>    0.0641</td> <td>    0.073</td> <td>    0.880</td> <td> 0.379</td> <td>   -0.079</td> <td>    0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>   -0.1542</td> <td>    0.055</td> <td>   -2.829</td> <td> 0.005</td> <td>   -0.261</td> <td>   -0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>    0.0482</td> <td>    0.079</td> <td>    0.610</td> <td> 0.542</td> <td>   -0.107</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>   -0.0043</td> <td>    0.129</td> <td>   -0.034</td> <td> 0.973</td> <td>   -0.256</td> <td>    0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>    0.3711</td> <td>    0.047</td> <td>    7.864</td> <td> 0.000</td> <td>    0.279</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>    0.0992</td> <td>    0.106</td> <td>    0.940</td> <td> 0.347</td> <td>   -0.108</td> <td>    0.306</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>    0.1710</td> <td>    0.051</td> <td>    3.366</td> <td> 0.001</td> <td>    0.071</td> <td>    0.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>   -0.2701</td> <td>    0.398</td> <td>   -0.679</td> <td> 0.497</td> <td>   -1.049</td> <td>    0.509</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>    0.1894</td> <td>    0.201</td> <td>    0.942</td> <td> 0.346</td> <td>   -0.205</td> <td>    0.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>    0.3831</td> <td>    0.398</td> <td>    0.963</td> <td> 0.336</td> <td>   -0.397</td> <td>    1.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>    0.2743</td> <td>    0.086</td> <td>    3.176</td> <td> 0.001</td> <td>    0.105</td> <td>    0.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>   -0.1802</td> <td>    0.140</td> <td>   -1.290</td> <td> 0.197</td> <td>   -0.454</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>    0.4649</td> <td>    0.078</td> <td>    5.949</td> <td> 0.000</td> <td>    0.312</td> <td>    0.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td>    0.3204</td> <td>    0.048</td> <td>    6.730</td> <td> 0.000</td> <td>    0.227</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td>   -0.0055</td> <td>    0.241</td> <td>   -0.023</td> <td> 0.982</td> <td>   -0.478</td> <td>    0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>   <td>   -0.1423</td> <td>    0.102</td> <td>   -1.395</td> <td> 0.163</td> <td>   -0.342</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>   <td>   -0.2290</td> <td>    0.100</td> <td>   -2.280</td> <td> 0.023</td> <td>   -0.426</td> <td>   -0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>   <td>    0.3014</td> <td>    0.138</td> <td>    2.178</td> <td> 0.029</td> <td>    0.030</td> <td>    0.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>   <td>    0.0964</td> <td>    0.142</td> <td>    0.681</td> <td> 0.496</td> <td>   -0.181</td> <td>    0.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>   <td>    0.1145</td> <td>    0.057</td> <td>    2.009</td> <td> 0.045</td> <td>    0.003</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>   <td>    0.0599</td> <td>    0.142</td> <td>    0.421</td> <td> 0.674</td> <td>   -0.219</td> <td>    0.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>   <td>    0.1231</td> <td>    0.088</td> <td>    1.394</td> <td> 0.163</td> <td>   -0.050</td> <td>    0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>   <td>    0.0145</td> <td>    0.131</td> <td>    0.111</td> <td> 0.912</td> <td>   -0.241</td> <td>    0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>   <td>    0.3168</td> <td>    0.098</td> <td>    3.247</td> <td> 0.001</td> <td>    0.126</td> <td>    0.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>   <td>    0.1974</td> <td>    0.080</td> <td>    2.467</td> <td> 0.014</td> <td>    0.041</td> <td>    0.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>   <td>   -0.0915</td> <td>    0.207</td> <td>   -0.443</td> <td> 0.658</td> <td>   -0.496</td> <td>    0.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>   <td>    0.1356</td> <td>    0.066</td> <td>    2.049</td> <td> 0.041</td> <td>    0.006</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>   <td>    0.1680</td> <td>    0.234</td> <td>    0.719</td> <td> 0.472</td> <td>   -0.290</td> <td>    0.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>   <td>    0.0592</td> <td>    0.092</td> <td>    0.641</td> <td> 0.522</td> <td>   -0.122</td> <td>    0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>   <td>    0.3182</td> <td>    0.104</td> <td>    3.065</td> <td> 0.002</td> <td>    0.115</td> <td>    0.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>   <td>    0.0073</td> <td>    0.068</td> <td>    0.108</td> <td> 0.914</td> <td>   -0.126</td> <td>    0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>   <td>   -0.1663</td> <td>    0.099</td> <td>   -1.683</td> <td> 0.092</td> <td>   -0.360</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th>  <td>    0.0428</td> <td>    0.087</td> <td>    0.491</td> <td> 0.624</td> <td>   -0.128</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th>  <td>   -0.0044</td> <td>    0.054</td> <td>   -0.081</td> <td> 0.936</td> <td>   -0.110</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th>  <td>    0.0743</td> <td>    0.065</td> <td>    1.150</td> <td> 0.250</td> <td>   -0.052</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th>  <td>    0.0830</td> <td>    0.029</td> <td>    2.870</td> <td> 0.004</td> <td>    0.026</td> <td>    0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th>  <td>    0.1103</td> <td>    0.022</td> <td>    4.929</td> <td> 0.000</td> <td>    0.066</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th>  <td>    0.0132</td> <td>    0.044</td> <td>    0.298</td> <td> 0.766</td> <td>   -0.074</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th>  <td>    0.0438</td> <td>    0.090</td> <td>    0.488</td> <td> 0.626</td> <td>   -0.132</td> <td>    0.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th>  <td>    0.2692</td> <td>    0.046</td> <td>    5.818</td> <td> 0.000</td> <td>    0.178</td> <td>    0.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th>  <td>   -0.3997</td> <td>    0.008</td> <td>  -50.112</td> <td> 0.000</td> <td>   -0.415</td> <td>   -0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x109</th>  <td>   -0.5780</td> <td>    0.030</td> <td>  -19.041</td> <td> 0.000</td> <td>   -0.637</td> <td>   -0.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x110</th>  <td>    0.0706</td> <td>    0.033</td> <td>    2.116</td> <td> 0.034</td> <td>    0.005</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x111</th>  <td>    0.0904</td> <td>    0.028</td> <td>    3.250</td> <td> 0.001</td> <td>    0.036</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x112</th>  <td>   -0.0418</td> <td>    0.035</td> <td>   -1.181</td> <td> 0.237</td> <td>   -0.111</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x113</th>  <td>   -0.0024</td> <td>    0.034</td> <td>   -0.072</td> <td> 0.942</td> <td>   -0.068</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x114</th>  <td>   -0.0161</td> <td>    0.063</td> <td>   -0.256</td> <td> 0.798</td> <td>   -0.140</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x115</th>  <td>   -0.0389</td> <td>    0.061</td> <td>   -0.638</td> <td> 0.524</td> <td>   -0.159</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x116</th>  <td>   -0.0738</td> <td>    0.061</td> <td>   -1.215</td> <td> 0.224</td> <td>   -0.193</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x117</th>  <td>   -0.0793</td> <td>    0.061</td> <td>   -1.296</td> <td> 0.195</td> <td>   -0.199</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x118</th>  <td>   -0.2116</td> <td>    0.155</td> <td>   -1.368</td> <td> 0.171</td> <td>   -0.515</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x119</th>  <td>   -0.3133</td> <td>    0.150</td> <td>   -2.090</td> <td> 0.037</td> <td>   -0.607</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x120</th>  <td>   -0.2782</td> <td>    0.317</td> <td>   -0.878</td> <td> 0.380</td> <td>   -0.900</td> <td>    0.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x121</th>  <td>   -0.4126</td> <td>    0.182</td> <td>   -2.264</td> <td> 0.024</td> <td>   -0.770</td> <td>   -0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x122</th>  <td>   -0.1356</td> <td>    0.144</td> <td>   -0.939</td> <td> 0.348</td> <td>   -0.419</td> <td>    0.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x123</th>  <td>   -0.1712</td> <td>    0.143</td> <td>   -1.197</td> <td> 0.231</td> <td>   -0.452</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x124</th>  <td>   -0.1436</td> <td>    0.157</td> <td>   -0.913</td> <td> 0.361</td> <td>   -0.452</td> <td>    0.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x125</th>  <td>   -0.0432</td> <td>    0.042</td> <td>   -1.037</td> <td> 0.300</td> <td>   -0.125</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x126</th>  <td>   -0.1007</td> <td>    0.045</td> <td>   -2.258</td> <td> 0.024</td> <td>   -0.188</td> <td>   -0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x127</th>  <td>   -0.2170</td> <td>    0.054</td> <td>   -4.022</td> <td> 0.000</td> <td>   -0.323</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x128</th>  <td>    0.0564</td> <td>    0.038</td> <td>    1.474</td> <td> 0.140</td> <td>   -0.019</td> <td>    0.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x129</th>  <td>   -0.0511</td> <td>    0.041</td> <td>   -1.256</td> <td> 0.209</td> <td>   -0.131</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x130</th>  <td>   -0.1835</td> <td>    0.042</td> <td>   -4.380</td> <td> 0.000</td> <td>   -0.266</td> <td>   -0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x131</th>  <td>   -0.0339</td> <td>    0.118</td> <td>   -0.287</td> <td> 0.774</td> <td>   -0.265</td> <td>    0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x132</th>  <td>   -0.1324</td> <td>    0.121</td> <td>   -1.098</td> <td> 0.272</td> <td>   -0.369</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x133</th>  <td>   -0.0619</td> <td>    0.119</td> <td>   -0.521</td> <td> 0.603</td> <td>   -0.295</td> <td>    0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x134</th>  <td>    0.0265</td> <td>    0.119</td> <td>    0.222</td> <td> 0.824</td> <td>   -0.207</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x135</th>  <td>    0.0206</td> <td>    0.116</td> <td>    0.177</td> <td> 0.860</td> <td>   -0.208</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x136</th>  <td>   -0.2074</td> <td>    0.093</td> <td>   -2.234</td> <td> 0.026</td> <td>   -0.389</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x137</th>  <td>   -0.3812</td> <td>    0.095</td> <td>   -4.007</td> <td> 0.000</td> <td>   -0.568</td> <td>   -0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x138</th>  <td>   -0.4488</td> <td>    0.095</td> <td>   -4.748</td> <td> 0.000</td> <td>   -0.634</td> <td>   -0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x139</th>  <td>   -0.0302</td> <td>    0.098</td> <td>   -0.309</td> <td> 0.757</td> <td>   -0.222</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x140</th>  <td>   -0.2647</td> <td>    0.099</td> <td>   -2.674</td> <td> 0.007</td> <td>   -0.459</td> <td>   -0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x141</th>  <td>   -0.2552</td> <td>    0.091</td> <td>   -2.802</td> <td> 0.005</td> <td>   -0.434</td> <td>   -0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x142</th>  <td>   -0.0824</td> <td>    0.096</td> <td>   -0.861</td> <td> 0.389</td> <td>   -0.270</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x143</th>  <td>    0.1893</td> <td>    0.091</td> <td>    2.080</td> <td> 0.038</td> <td>    0.011</td> <td>    0.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x144</th>  <td>    0.0079</td> <td>    0.107</td> <td>    0.074</td> <td> 0.941</td> <td>   -0.202</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x145</th>  <td>   -0.2428</td> <td>    0.119</td> <td>   -2.035</td> <td> 0.042</td> <td>   -0.477</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x146</th>  <td>    0.0272</td> <td>    0.101</td> <td>    0.269</td> <td> 0.788</td> <td>   -0.171</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x147</th>  <td>    0.0600</td> <td>    0.091</td> <td>    0.659</td> <td> 0.510</td> <td>   -0.118</td> <td>    0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x148</th>  <td>    0.0314</td> <td>    0.094</td> <td>    0.335</td> <td> 0.738</td> <td>   -0.152</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x149</th>  <td>    0.2261</td> <td>    0.088</td> <td>    2.567</td> <td> 0.010</td> <td>    0.053</td> <td>    0.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x150</th>  <td>    0.0936</td> <td>    0.076</td> <td>    1.238</td> <td> 0.216</td> <td>   -0.055</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x151</th>  <td>    0.0908</td> <td>    0.099</td> <td>    0.921</td> <td> 0.357</td> <td>   -0.102</td> <td>    0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x152</th>  <td>    0.0489</td> <td>    0.081</td> <td>    0.601</td> <td> 0.548</td> <td>   -0.111</td> <td>    0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x153</th>  <td>    0.0143</td> <td>    0.084</td> <td>    0.169</td> <td> 0.866</td> <td>   -0.151</td> <td>    0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x154</th>  <td>    0.0492</td> <td>    0.088</td> <td>    0.558</td> <td> 0.577</td> <td>   -0.124</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x155</th>  <td>    0.1088</td> <td>    0.086</td> <td>    1.265</td> <td> 0.206</td> <td>   -0.060</td> <td>    0.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x156</th>  <td>    0.0835</td> <td>    0.100</td> <td>    0.836</td> <td> 0.403</td> <td>   -0.112</td> <td>    0.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x157</th>  <td>    0.0638</td> <td>    0.083</td> <td>    0.764</td> <td> 0.445</td> <td>   -0.100</td> <td>    0.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x158</th>  <td>    0.0867</td> <td>    0.101</td> <td>    0.856</td> <td> 0.392</td> <td>   -0.112</td> <td>    0.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x159</th>  <td>   -0.0806</td> <td>    0.077</td> <td>   -1.040</td> <td> 0.298</td> <td>   -0.232</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x160</th>  <td>    0.0200</td> <td>    0.076</td> <td>    0.263</td> <td> 0.793</td> <td>   -0.129</td> <td>    0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x161</th>  <td>   -0.0137</td> <td>    0.059</td> <td>   -0.233</td> <td> 0.816</td> <td>   -0.129</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x162</th>  <td>    0.0127</td> <td>    0.060</td> <td>    0.212</td> <td> 0.832</td> <td>   -0.105</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x163</th>  <td>   -0.0202</td> <td>    0.059</td> <td>   -0.341</td> <td> 0.733</td> <td>   -0.136</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x164</th>  <td>    0.0360</td> <td>    0.057</td> <td>    0.630</td> <td> 0.529</td> <td>   -0.076</td> <td>    0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x165</th>  <td>    0.0626</td> <td>    0.061</td> <td>    1.020</td> <td> 0.308</td> <td>   -0.058</td> <td>    0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x166</th>  <td>    0.0569</td> <td>    0.058</td> <td>    0.983</td> <td> 0.326</td> <td>   -0.057</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x167</th>  <td>   -0.0193</td> <td>    0.057</td> <td>   -0.337</td> <td> 0.736</td> <td>   -0.132</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x168</th>  <td>   -0.0672</td> <td>    0.062</td> <td>   -1.079</td> <td> 0.281</td> <td>   -0.189</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x169</th>  <td>   -0.0322</td> <td>    0.062</td> <td>   -0.518</td> <td> 0.604</td> <td>   -0.154</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x170</th>  <td>    0.0369</td> <td>    0.062</td> <td>    0.600</td> <td> 0.549</td> <td>   -0.084</td> <td>    0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x171</th>  <td>   -0.0579</td> <td>    0.061</td> <td>   -0.942</td> <td> 0.346</td> <td>   -0.178</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x172</th>  <td>   -0.0969</td> <td>    0.061</td> <td>   -1.581</td> <td> 0.114</td> <td>   -0.217</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x173</th>  <td>   -0.0666</td> <td>    0.063</td> <td>   -1.065</td> <td> 0.287</td> <td>   -0.189</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x174</th>  <td>   -0.1509</td> <td>    0.063</td> <td>   -2.394</td> <td> 0.017</td> <td>   -0.275</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x175</th>  <td>   -0.2476</td> <td>    0.083</td> <td>   -2.999</td> <td> 0.003</td> <td>   -0.409</td> <td>   -0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x176</th>  <td>   -0.0727</td> <td>    0.063</td> <td>   -1.148</td> <td> 0.251</td> <td>   -0.197</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x177</th>  <td>   -0.2722</td> <td>    0.114</td> <td>   -2.382</td> <td> 0.017</td> <td>   -0.496</td> <td>   -0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x178</th>  <td>   -0.2316</td> <td>    0.118</td> <td>   -1.960</td> <td> 0.050</td> <td>   -0.463</td> <td> 4.49e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x179</th>  <td>   -0.2986</td> <td>    0.116</td> <td>   -2.570</td> <td> 0.010</td> <td>   -0.526</td> <td>   -0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x180</th>  <td>   -0.3549</td> <td>    0.113</td> <td>   -3.136</td> <td> 0.002</td> <td>   -0.577</td> <td>   -0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x181</th>  <td>   -0.2277</td> <td>    0.094</td> <td>   -2.431</td> <td> 0.015</td> <td>   -0.411</td> <td>   -0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x182</th>  <td>   -0.0787</td> <td>    0.108</td> <td>   -0.729</td> <td> 0.466</td> <td>   -0.290</td> <td>    0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x183</th>  <td>   -0.0408</td> <td>    0.116</td> <td>   -0.353</td> <td> 0.724</td> <td>   -0.268</td> <td>    0.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x184</th>  <td>   -0.0081</td> <td>    0.112</td> <td>   -0.073</td> <td> 0.942</td> <td>   -0.227</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x185</th>  <td>   -0.2037</td> <td>    0.130</td> <td>   -1.565</td> <td> 0.118</td> <td>   -0.459</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x186</th>  <td>    0.0237</td> <td>    0.136</td> <td>    0.174</td> <td> 0.862</td> <td>   -0.243</td> <td>    0.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x187</th>  <td>   -0.1010</td> <td>    0.111</td> <td>   -0.910</td> <td> 0.363</td> <td>   -0.318</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x188</th>  <td>    0.0699</td> <td>    0.185</td> <td>    0.378</td> <td> 0.706</td> <td>   -0.293</td> <td>    0.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x189</th>  <td>    0.0811</td> <td>    0.195</td> <td>    0.415</td> <td> 0.678</td> <td>   -0.302</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x190</th>  <td>    0.3226</td> <td>    0.140</td> <td>    2.302</td> <td> 0.021</td> <td>    0.048</td> <td>    0.597</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x191</th>  <td>   -0.0672</td> <td>    0.155</td> <td>   -0.434</td> <td> 0.664</td> <td>   -0.370</td> <td>    0.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x192</th>  <td>    0.1189</td> <td>    0.414</td> <td>    0.287</td> <td> 0.774</td> <td>   -0.693</td> <td>    0.931</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x193</th>  <td>   -0.5971</td> <td>    0.410</td> <td>   -1.458</td> <td> 0.145</td> <td>   -1.400</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x194</th>  <td>    0.1719</td> <td>    0.313</td> <td>    0.550</td> <td> 0.582</td> <td>   -0.441</td> <td>    0.784</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x195</th>  <td>   -0.0143</td> <td>    0.114</td> <td>   -0.125</td> <td> 0.901</td> <td>   -0.238</td> <td>    0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x196</th>  <td>   -0.1581</td> <td>    0.138</td> <td>   -1.149</td> <td> 0.251</td> <td>   -0.428</td> <td>    0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x197</th>  <td>   -0.0085</td> <td>    0.189</td> <td>   -0.045</td> <td> 0.964</td> <td>   -0.379</td> <td>    0.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x198</th>  <td>    0.0123</td> <td>    0.194</td> <td>    0.063</td> <td> 0.950</td> <td>   -0.369</td> <td>    0.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x199</th>  <td>   -0.1969</td> <td>    0.264</td> <td>   -0.745</td> <td> 0.457</td> <td>   -0.715</td> <td>    0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x200</th>  <td>   -0.1542</td> <td>    0.055</td> <td>   -2.829</td> <td> 0.005</td> <td>   -0.261</td> <td>   -0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x201</th>  <td>    0.0093</td> <td>    0.165</td> <td>    0.056</td> <td> 0.955</td> <td>   -0.314</td> <td>    0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x202</th>  <td>   -0.1694</td> <td>    0.047</td> <td>   -3.635</td> <td> 0.000</td> <td>   -0.261</td> <td>   -0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x203</th>  <td>   -0.0460</td> <td>    0.150</td> <td>   -0.306</td> <td> 0.759</td> <td>   -0.340</td> <td>    0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x204</th>  <td>   -0.2127</td> <td>    0.235</td> <td>   -0.904</td> <td> 0.366</td> <td>   -0.674</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x205</th>  <td>   -0.1323</td> <td>    0.241</td> <td>   -0.550</td> <td> 0.582</td> <td>   -0.604</td> <td>    0.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x206</th>  <td>   -0.3297</td> <td>    0.128</td> <td>   -2.568</td> <td> 0.010</td> <td>   -0.581</td> <td>   -0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x207</th>  <td>   -0.1292</td> <td>    0.054</td> <td>   -2.381</td> <td> 0.017</td> <td>   -0.236</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x208</th>  <td>   -0.1701</td> <td>    0.216</td> <td>   -0.788</td> <td> 0.431</td> <td>   -0.593</td> <td>    0.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x209</th>  <td>   -0.3007</td> <td>    0.092</td> <td>   -3.269</td> <td> 0.001</td> <td>   -0.481</td> <td>   -0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x210</th>  <td>   -0.2818</td> <td>    0.089</td> <td>   -3.182</td> <td> 0.001</td> <td>   -0.455</td> <td>   -0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x211</th>  <td>   -0.0984</td> <td>    0.074</td> <td>   -1.323</td> <td> 0.186</td> <td>   -0.244</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x212</th>  <td>   -0.2162</td> <td>    0.090</td> <td>   -2.390</td> <td> 0.017</td> <td>   -0.393</td> <td>   -0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x213</th>  <td>   -0.2332</td> <td>    0.082</td> <td>   -2.839</td> <td> 0.005</td> <td>   -0.394</td> <td>   -0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x214</th>  <td>   -0.1291</td> <td>    0.074</td> <td>   -1.744</td> <td> 0.081</td> <td>   -0.274</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x215</th>  <td>   -0.0201</td> <td>    0.083</td> <td>   -0.241</td> <td> 0.809</td> <td>   -0.183</td> <td>    0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x216</th>  <td>   -0.1223</td> <td>    0.074</td> <td>   -1.653</td> <td> 0.098</td> <td>   -0.267</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x217</th>  <td>   -0.2088</td> <td>    0.075</td> <td>   -2.793</td> <td> 0.005</td> <td>   -0.355</td> <td>   -0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x218</th>  <td>   -0.0979</td> <td>    0.137</td> <td>   -0.715</td> <td> 0.474</td> <td>   -0.366</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x219</th>  <td>   -0.0011</td> <td>    0.116</td> <td>   -0.009</td> <td> 0.993</td> <td>   -0.228</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x220</th>  <td>    0.0053</td> <td>    0.112</td> <td>    0.048</td> <td> 0.962</td> <td>   -0.214</td> <td>    0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x221</th>  <td>   -0.0373</td> <td>    0.161</td> <td>   -0.231</td> <td> 0.817</td> <td>   -0.353</td> <td>    0.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x222</th>  <td>   -0.5389</td> <td>    0.167</td> <td>   -3.224</td> <td> 0.001</td> <td>   -0.867</td> <td>   -0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x223</th>  <td>   -0.0265</td> <td>    0.123</td> <td>   -0.215</td> <td> 0.830</td> <td>   -0.268</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x224</th>  <td>    0.1153</td> <td>    0.127</td> <td>    0.910</td> <td> 0.363</td> <td>   -0.133</td> <td>    0.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x225</th>  <td>   -0.1121</td> <td>    0.123</td> <td>   -0.912</td> <td> 0.362</td> <td>   -0.353</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x226</th>  <td>   -0.1430</td> <td>    0.093</td> <td>   -1.543</td> <td> 0.123</td> <td>   -0.325</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x227</th>  <td>   -0.2735</td> <td>    0.089</td> <td>   -3.088</td> <td> 0.002</td> <td>   -0.447</td> <td>   -0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x228</th>  <td>    0.1448</td> <td>    0.226</td> <td>    0.641</td> <td> 0.522</td> <td>   -0.298</td> <td>    0.588</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x229</th>  <td>    0.0470</td> <td>    0.139</td> <td>    0.339</td> <td> 0.734</td> <td>   -0.225</td> <td>    0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x230</th>  <td>    0.0400</td> <td>    0.134</td> <td>    0.297</td> <td> 0.766</td> <td>   -0.223</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x231</th>  <td>    0.0868</td> <td>    0.185</td> <td>    0.470</td> <td> 0.638</td> <td>   -0.275</td> <td>    0.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x232</th>  <td>    0.0770</td> <td>    0.099</td> <td>    0.778</td> <td> 0.436</td> <td>   -0.117</td> <td>    0.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x233</th>  <td>   -0.0389</td> <td>    0.172</td> <td>   -0.226</td> <td> 0.821</td> <td>   -0.376</td> <td>    0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x234</th>  <td>   -0.0801</td> <td>    0.043</td> <td>   -1.872</td> <td> 0.061</td> <td>   -0.164</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x235</th>  <td>   -0.1144</td> <td>    0.079</td> <td>   -1.454</td> <td> 0.146</td> <td>   -0.269</td> <td>    0.040</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>765.981</td> <th>  Durbin-Watson:     </th> <td>   2.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2359.470</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.157</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.802</td>  <th>  Cond. No.          </th> <td>2.49e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.64e-28. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.545\n",
       "Model:                            OLS   Adj. R-squared:                  0.538\n",
       "Method:                 Least Squares   F-statistic:                     86.43\n",
       "Date:                Wed, 22 Jul 2020   Prob (F-statistic):               0.00\n",
       "Time:                        18:56:52   Log-Likelihood:                -8469.6\n",
       "No. Observations:               16914   AIC:                         1.740e+04\n",
       "Df Residuals:                   16682   BIC:                         1.920e+04\n",
       "Df Model:                         231                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.9341      0.050     78.766      0.000       3.836       4.032\n",
       "x1             0.1442      0.005     27.533      0.000       0.134       0.154\n",
       "x2             0.0272      0.004      7.538      0.000       0.020       0.034\n",
       "x3             0.0122      0.003      3.847      0.000       0.006       0.018\n",
       "x4             0.0031      0.003      0.884      0.377      -0.004       0.010\n",
       "x5             0.0035      0.004      0.905      0.365      -0.004       0.011\n",
       "x6             0.0379      0.003     11.034      0.000       0.031       0.045\n",
       "x7             0.0104      0.003      3.196      0.001       0.004       0.017\n",
       "x8            -0.0009      0.003     -0.282      0.778      -0.007       0.006\n",
       "x9             0.0169      0.004      4.492      0.000       0.010       0.024\n",
       "x10           -0.0097      0.003     -3.014      0.003      -0.016      -0.003\n",
       "x11            0.0249      0.003      7.373      0.000       0.018       0.031\n",
       "x12           -0.0216      0.003     -6.524      0.000      -0.028      -0.015\n",
       "x13            0.0394      0.004     11.239      0.000       0.033       0.046\n",
       "x14            0.0009      0.003      0.279      0.781      -0.005       0.007\n",
       "x15            0.0988      0.004     25.350      0.000       0.091       0.106\n",
       "x16            0.0321      0.003      9.490      0.000       0.025       0.039\n",
       "x17            0.0771      0.009      8.195      0.000       0.059       0.096\n",
       "x18           -0.0229      0.009     -2.530      0.011      -0.041      -0.005\n",
       "x19           -0.0315      0.004     -7.536      0.000      -0.040      -0.023\n",
       "x20            0.0036      0.006      0.656      0.512      -0.007       0.015\n",
       "x21            0.0042      0.006      0.727      0.467      -0.007       0.016\n",
       "x22        -3.958e-05      0.004     -0.011      0.991      -0.007       0.007\n",
       "x23           -0.0459      0.004    -12.724      0.000      -0.053      -0.039\n",
       "x24            0.0229      0.004      6.264      0.000       0.016       0.030\n",
       "x25            0.0008      0.004      0.211      0.833      -0.006       0.008\n",
       "x26           -0.0016      0.003     -0.484      0.629      -0.008       0.005\n",
       "x27           -0.0078      0.004     -2.036      0.042      -0.015      -0.000\n",
       "x28           -0.0090      0.009     -0.988      0.323      -0.027       0.009\n",
       "x29           -0.0044      0.009     -0.493      0.622      -0.022       0.013\n",
       "x30            0.0076      0.003      2.411      0.016       0.001       0.014\n",
       "x31           -0.0524      0.003    -15.523      0.000      -0.059      -0.046\n",
       "x32           -0.0015      0.004     -0.341      0.733      -0.010       0.007\n",
       "x33            0.0143      0.004      3.595      0.000       0.007       0.022\n",
       "x34           -0.0028      0.004     -0.779      0.436      -0.010       0.004\n",
       "x35            0.0484      0.020      2.419      0.016       0.009       0.088\n",
       "x36           -0.1249      0.020     -6.251      0.000      -0.164      -0.086\n",
       "x37            0.0318      0.003      9.768      0.000       0.025       0.038\n",
       "x38            0.0072      0.004      1.651      0.099      -0.001       0.016\n",
       "x39           -0.0050      0.004     -1.352      0.176      -0.012       0.002\n",
       "x40           -0.0065      0.004     -1.659      0.097      -0.014       0.001\n",
       "x41           -0.0118      0.005     -2.612      0.009      -0.021      -0.003\n",
       "x42            0.0213      0.008      2.638      0.008       0.005       0.037\n",
       "x43            0.0320      0.009      3.697      0.000       0.015       0.049\n",
       "x44            0.1298      0.059      2.188      0.029       0.014       0.246\n",
       "x45           -0.0785      0.115     -0.683      0.495      -0.304       0.147\n",
       "x46           -0.2444      0.131     -1.863      0.062      -0.501       0.013\n",
       "x47           -0.0093      0.147     -0.063      0.950      -0.298       0.280\n",
       "x48           -0.1694      0.047     -3.635      0.000      -0.261      -0.078\n",
       "x49           -0.2124      0.166     -1.283      0.199      -0.537       0.112\n",
       "x50            0.1895      0.111      1.714      0.087      -0.027       0.406\n",
       "x51            0.3754      0.068      5.543      0.000       0.243       0.508\n",
       "x52            0.0662      0.203      0.326      0.745      -0.332       0.464\n",
       "x53            0.0452      0.136      0.332      0.740      -0.222       0.312\n",
       "x54           -0.1292      0.054     -2.381      0.017      -0.236      -0.023\n",
       "x55            0.1356      0.095      1.429      0.153      -0.050       0.322\n",
       "x56            0.1407      0.154      0.913      0.361      -0.162       0.443\n",
       "x57            0.2268      0.059      3.864      0.000       0.112       0.342\n",
       "x58            0.2289      0.113      2.020      0.043       0.007       0.451\n",
       "x59            0.0216      0.108      0.200      0.842      -0.191       0.234\n",
       "x60            0.3021      0.112      2.699      0.007       0.083       0.521\n",
       "x61            0.0832      0.127      0.656      0.512      -0.165       0.332\n",
       "x62           -0.4091      0.283     -1.447      0.148      -0.963       0.145\n",
       "x63            0.2020      0.314      0.644      0.520      -0.413       0.817\n",
       "x64            0.2353      0.048      4.947      0.000       0.142       0.329\n",
       "x65            0.0043      0.163      0.027      0.979      -0.315       0.324\n",
       "x66           -0.2653      0.101     -2.627      0.009      -0.463      -0.067\n",
       "x67            0.1163      0.130      0.891      0.373      -0.140       0.372\n",
       "x68            0.0641      0.073      0.880      0.379      -0.079       0.207\n",
       "x69           -0.1542      0.055     -2.829      0.005      -0.261      -0.047\n",
       "x70            0.0482      0.079      0.610      0.542      -0.107       0.203\n",
       "x71           -0.0043      0.129     -0.034      0.973      -0.256       0.248\n",
       "x72            0.3711      0.047      7.864      0.000       0.279       0.464\n",
       "x73            0.0992      0.106      0.940      0.347      -0.108       0.306\n",
       "x74            0.1710      0.051      3.366      0.001       0.071       0.271\n",
       "x75           -0.2701      0.398     -0.679      0.497      -1.049       0.509\n",
       "x76            0.1894      0.201      0.942      0.346      -0.205       0.583\n",
       "x77            0.3831      0.398      0.963      0.336      -0.397       1.163\n",
       "x78            0.2743      0.086      3.176      0.001       0.105       0.444\n",
       "x79           -0.1802      0.140     -1.290      0.197      -0.454       0.094\n",
       "x80            0.4649      0.078      5.949      0.000       0.312       0.618\n",
       "x81            0.3204      0.048      6.730      0.000       0.227       0.414\n",
       "x82           -0.0055      0.241     -0.023      0.982      -0.478       0.467\n",
       "x83           -0.1423      0.102     -1.395      0.163      -0.342       0.058\n",
       "x84           -0.2290      0.100     -2.280      0.023      -0.426      -0.032\n",
       "x85            0.3014      0.138      2.178      0.029       0.030       0.573\n",
       "x86            0.0964      0.142      0.681      0.496      -0.181       0.374\n",
       "x87            0.1145      0.057      2.009      0.045       0.003       0.226\n",
       "x88            0.0599      0.142      0.421      0.674      -0.219       0.339\n",
       "x89            0.1231      0.088      1.394      0.163      -0.050       0.296\n",
       "x90            0.0145      0.131      0.111      0.912      -0.241       0.270\n",
       "x91            0.3168      0.098      3.247      0.001       0.126       0.508\n",
       "x92            0.1974      0.080      2.467      0.014       0.041       0.354\n",
       "x93           -0.0915      0.207     -0.443      0.658      -0.496       0.313\n",
       "x94            0.1356      0.066      2.049      0.041       0.006       0.265\n",
       "x95            0.1680      0.234      0.719      0.472      -0.290       0.626\n",
       "x96            0.0592      0.092      0.641      0.522      -0.122       0.240\n",
       "x97            0.3182      0.104      3.065      0.002       0.115       0.522\n",
       "x98            0.0073      0.068      0.108      0.914      -0.126       0.140\n",
       "x99           -0.1663      0.099     -1.683      0.092      -0.360       0.027\n",
       "x100           0.0428      0.087      0.491      0.624      -0.128       0.214\n",
       "x101          -0.0044      0.054     -0.081      0.936      -0.110       0.101\n",
       "x102           0.0743      0.065      1.150      0.250      -0.052       0.201\n",
       "x103           0.0830      0.029      2.870      0.004       0.026       0.140\n",
       "x104           0.1103      0.022      4.929      0.000       0.066       0.154\n",
       "x105           0.0132      0.044      0.298      0.766      -0.074       0.100\n",
       "x106           0.0438      0.090      0.488      0.626      -0.132       0.219\n",
       "x107           0.2692      0.046      5.818      0.000       0.178       0.360\n",
       "x108          -0.3997      0.008    -50.112      0.000      -0.415      -0.384\n",
       "x109          -0.5780      0.030    -19.041      0.000      -0.637      -0.518\n",
       "x110           0.0706      0.033      2.116      0.034       0.005       0.136\n",
       "x111           0.0904      0.028      3.250      0.001       0.036       0.145\n",
       "x112          -0.0418      0.035     -1.181      0.237      -0.111       0.028\n",
       "x113          -0.0024      0.034     -0.072      0.942      -0.068       0.064\n",
       "x114          -0.0161      0.063     -0.256      0.798      -0.140       0.107\n",
       "x115          -0.0389      0.061     -0.638      0.524      -0.159       0.081\n",
       "x116          -0.0738      0.061     -1.215      0.224      -0.193       0.045\n",
       "x117          -0.0793      0.061     -1.296      0.195      -0.199       0.041\n",
       "x118          -0.2116      0.155     -1.368      0.171      -0.515       0.092\n",
       "x119          -0.3133      0.150     -2.090      0.037      -0.607      -0.019\n",
       "x120          -0.2782      0.317     -0.878      0.380      -0.900       0.343\n",
       "x121          -0.4126      0.182     -2.264      0.024      -0.770      -0.055\n",
       "x122          -0.1356      0.144     -0.939      0.348      -0.419       0.147\n",
       "x123          -0.1712      0.143     -1.197      0.231      -0.452       0.109\n",
       "x124          -0.1436      0.157     -0.913      0.361      -0.452       0.165\n",
       "x125          -0.0432      0.042     -1.037      0.300      -0.125       0.038\n",
       "x126          -0.1007      0.045     -2.258      0.024      -0.188      -0.013\n",
       "x127          -0.2170      0.054     -4.022      0.000      -0.323      -0.111\n",
       "x128           0.0564      0.038      1.474      0.140      -0.019       0.131\n",
       "x129          -0.0511      0.041     -1.256      0.209      -0.131       0.029\n",
       "x130          -0.1835      0.042     -4.380      0.000      -0.266      -0.101\n",
       "x131          -0.0339      0.118     -0.287      0.774      -0.265       0.197\n",
       "x132          -0.1324      0.121     -1.098      0.272      -0.369       0.104\n",
       "x133          -0.0619      0.119     -0.521      0.603      -0.295       0.171\n",
       "x134           0.0265      0.119      0.222      0.824      -0.207       0.260\n",
       "x135           0.0206      0.116      0.177      0.860      -0.208       0.249\n",
       "x136          -0.2074      0.093     -2.234      0.026      -0.389      -0.025\n",
       "x137          -0.3812      0.095     -4.007      0.000      -0.568      -0.195\n",
       "x138          -0.4488      0.095     -4.748      0.000      -0.634      -0.264\n",
       "x139          -0.0302      0.098     -0.309      0.757      -0.222       0.162\n",
       "x140          -0.2647      0.099     -2.674      0.007      -0.459      -0.071\n",
       "x141          -0.2552      0.091     -2.802      0.005      -0.434      -0.077\n",
       "x142          -0.0824      0.096     -0.861      0.389      -0.270       0.105\n",
       "x143           0.1893      0.091      2.080      0.038       0.011       0.368\n",
       "x144           0.0079      0.107      0.074      0.941      -0.202       0.217\n",
       "x145          -0.2428      0.119     -2.035      0.042      -0.477      -0.009\n",
       "x146           0.0272      0.101      0.269      0.788      -0.171       0.226\n",
       "x147           0.0600      0.091      0.659      0.510      -0.118       0.238\n",
       "x148           0.0314      0.094      0.335      0.738      -0.152       0.215\n",
       "x149           0.2261      0.088      2.567      0.010       0.053       0.399\n",
       "x150           0.0936      0.076      1.238      0.216      -0.055       0.242\n",
       "x151           0.0908      0.099      0.921      0.357      -0.102       0.284\n",
       "x152           0.0489      0.081      0.601      0.548      -0.111       0.208\n",
       "x153           0.0143      0.084      0.169      0.866      -0.151       0.180\n",
       "x154           0.0492      0.088      0.558      0.577      -0.124       0.222\n",
       "x155           0.1088      0.086      1.265      0.206      -0.060       0.277\n",
       "x156           0.0835      0.100      0.836      0.403      -0.112       0.279\n",
       "x157           0.0638      0.083      0.764      0.445      -0.100       0.227\n",
       "x158           0.0867      0.101      0.856      0.392      -0.112       0.285\n",
       "x159          -0.0806      0.077     -1.040      0.298      -0.232       0.071\n",
       "x160           0.0200      0.076      0.263      0.793      -0.129       0.169\n",
       "x161          -0.0137      0.059     -0.233      0.816      -0.129       0.102\n",
       "x162           0.0127      0.060      0.212      0.832      -0.105       0.130\n",
       "x163          -0.0202      0.059     -0.341      0.733      -0.136       0.096\n",
       "x164           0.0360      0.057      0.630      0.529      -0.076       0.148\n",
       "x165           0.0626      0.061      1.020      0.308      -0.058       0.183\n",
       "x166           0.0569      0.058      0.983      0.326      -0.057       0.170\n",
       "x167          -0.0193      0.057     -0.337      0.736      -0.132       0.093\n",
       "x168          -0.0672      0.062     -1.079      0.281      -0.189       0.055\n",
       "x169          -0.0322      0.062     -0.518      0.604      -0.154       0.089\n",
       "x170           0.0369      0.062      0.600      0.549      -0.084       0.157\n",
       "x171          -0.0579      0.061     -0.942      0.346      -0.178       0.063\n",
       "x172          -0.0969      0.061     -1.581      0.114      -0.217       0.023\n",
       "x173          -0.0666      0.063     -1.065      0.287      -0.189       0.056\n",
       "x174          -0.1509      0.063     -2.394      0.017      -0.275      -0.027\n",
       "x175          -0.2476      0.083     -2.999      0.003      -0.409      -0.086\n",
       "x176          -0.0727      0.063     -1.148      0.251      -0.197       0.051\n",
       "x177          -0.2722      0.114     -2.382      0.017      -0.496      -0.048\n",
       "x178          -0.2316      0.118     -1.960      0.050      -0.463    4.49e-05\n",
       "x179          -0.2986      0.116     -2.570      0.010      -0.526      -0.071\n",
       "x180          -0.3549      0.113     -3.136      0.002      -0.577      -0.133\n",
       "x181          -0.2277      0.094     -2.431      0.015      -0.411      -0.044\n",
       "x182          -0.0787      0.108     -0.729      0.466      -0.290       0.133\n",
       "x183          -0.0408      0.116     -0.353      0.724      -0.268       0.186\n",
       "x184          -0.0081      0.112     -0.073      0.942      -0.227       0.211\n",
       "x185          -0.2037      0.130     -1.565      0.118      -0.459       0.051\n",
       "x186           0.0237      0.136      0.174      0.862      -0.243       0.291\n",
       "x187          -0.1010      0.111     -0.910      0.363      -0.318       0.117\n",
       "x188           0.0699      0.185      0.378      0.706      -0.293       0.433\n",
       "x189           0.0811      0.195      0.415      0.678      -0.302       0.464\n",
       "x190           0.3226      0.140      2.302      0.021       0.048       0.597\n",
       "x191          -0.0672      0.155     -0.434      0.664      -0.370       0.236\n",
       "x192           0.1189      0.414      0.287      0.774      -0.693       0.931\n",
       "x193          -0.5971      0.410     -1.458      0.145      -1.400       0.206\n",
       "x194           0.1719      0.313      0.550      0.582      -0.441       0.784\n",
       "x195          -0.0143      0.114     -0.125      0.901      -0.238       0.210\n",
       "x196          -0.1581      0.138     -1.149      0.251      -0.428       0.112\n",
       "x197          -0.0085      0.189     -0.045      0.964      -0.379       0.362\n",
       "x198           0.0123      0.194      0.063      0.950      -0.369       0.393\n",
       "x199          -0.1969      0.264     -0.745      0.457      -0.715       0.322\n",
       "x200          -0.1542      0.055     -2.829      0.005      -0.261      -0.047\n",
       "x201           0.0093      0.165      0.056      0.955      -0.314       0.332\n",
       "x202          -0.1694      0.047     -3.635      0.000      -0.261      -0.078\n",
       "x203          -0.0460      0.150     -0.306      0.759      -0.340       0.248\n",
       "x204          -0.2127      0.235     -0.904      0.366      -0.674       0.249\n",
       "x205          -0.1323      0.241     -0.550      0.582      -0.604       0.339\n",
       "x206          -0.3297      0.128     -2.568      0.010      -0.581      -0.078\n",
       "x207          -0.1292      0.054     -2.381      0.017      -0.236      -0.023\n",
       "x208          -0.1701      0.216     -0.788      0.431      -0.593       0.253\n",
       "x209          -0.3007      0.092     -3.269      0.001      -0.481      -0.120\n",
       "x210          -0.2818      0.089     -3.182      0.001      -0.455      -0.108\n",
       "x211          -0.0984      0.074     -1.323      0.186      -0.244       0.047\n",
       "x212          -0.2162      0.090     -2.390      0.017      -0.393      -0.039\n",
       "x213          -0.2332      0.082     -2.839      0.005      -0.394      -0.072\n",
       "x214          -0.1291      0.074     -1.744      0.081      -0.274       0.016\n",
       "x215          -0.0201      0.083     -0.241      0.809      -0.183       0.143\n",
       "x216          -0.1223      0.074     -1.653      0.098      -0.267       0.023\n",
       "x217          -0.2088      0.075     -2.793      0.005      -0.355      -0.062\n",
       "x218          -0.0979      0.137     -0.715      0.474      -0.366       0.170\n",
       "x219          -0.0011      0.116     -0.009      0.993      -0.228       0.226\n",
       "x220           0.0053      0.112      0.048      0.962      -0.214       0.225\n",
       "x221          -0.0373      0.161     -0.231      0.817      -0.353       0.278\n",
       "x222          -0.5389      0.167     -3.224      0.001      -0.867      -0.211\n",
       "x223          -0.0265      0.123     -0.215      0.830      -0.268       0.215\n",
       "x224           0.1153      0.127      0.910      0.363      -0.133       0.364\n",
       "x225          -0.1121      0.123     -0.912      0.362      -0.353       0.129\n",
       "x226          -0.1430      0.093     -1.543      0.123      -0.325       0.039\n",
       "x227          -0.2735      0.089     -3.088      0.002      -0.447      -0.100\n",
       "x228           0.1448      0.226      0.641      0.522      -0.298       0.588\n",
       "x229           0.0470      0.139      0.339      0.734      -0.225       0.319\n",
       "x230           0.0400      0.134      0.297      0.766      -0.223       0.303\n",
       "x231           0.0868      0.185      0.470      0.638      -0.275       0.449\n",
       "x232           0.0770      0.099      0.778      0.436      -0.117       0.271\n",
       "x233          -0.0389      0.172     -0.226      0.821      -0.376       0.298\n",
       "x234          -0.0801      0.043     -1.872      0.061      -0.164       0.004\n",
       "x235          -0.1144      0.079     -1.454      0.146      -0.269       0.040\n",
       "==============================================================================\n",
       "Omnibus:                      765.981   Durbin-Watson:                   2.000\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2359.470\n",
       "Skew:                           0.157   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.802   Cond. No.                     2.49e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.64e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model summary\n",
    "reg_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Predict target (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply linear regression w/o preprocessing/differently (to roughly see feature importance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get num_features as string as basis for model\n",
    "num_feat_list = str(num_features)\n",
    "num_feat_list = num_feat_list.strip(\"[\").strip(\"]\").replace(\"'\",\"\").replace(\", \", \" + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define target and features for model\n",
    "model = f'{target} ~ {num_feat_list}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize and fit model\n",
    "reg_ols_wo = smf.ols(formula=model, data=data).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>price_log</td>    <th>  R-squared:         </th> <td>   0.378</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.378</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   512.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 22 Jul 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:57:01</td>     <th>  Log-Likelihood:    </th> <td> -15450.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 23623</td>      <th>  AIC:               </th> <td>3.096e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 23594</td>      <th>  BIC:               </th> <td>3.119e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    28</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                   <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                      <td>   -2.4444</td> <td>    5.219</td> <td>   -0.468</td> <td> 0.640</td> <td>  -12.674</td> <td>    7.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>accommodates_per_bed</th>           <td>    0.0616</td> <td>    0.005</td> <td>   12.707</td> <td> 0.000</td> <td>    0.052</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>accommodates_per_room</th>          <td>    0.1134</td> <td>    0.003</td> <td>   37.419</td> <td> 0.000</td> <td>    0.107</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_balcony</th>                     <td>    0.0962</td> <td>    0.009</td> <td>   11.240</td> <td> 0.000</td> <td>    0.079</td> <td>    0.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_breakfast</th>                   <td>    0.0711</td> <td>    0.014</td> <td>    5.262</td> <td> 0.000</td> <td>    0.045</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_child_friendly</th>              <td>    0.0227</td> <td>    0.007</td> <td>    3.132</td> <td> 0.002</td> <td>    0.008</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_elevator</th>                    <td>    0.1620</td> <td>    0.008</td> <td>   21.469</td> <td> 0.000</td> <td>    0.147</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_essentials</th>                  <td>    0.0607</td> <td>    0.012</td> <td>    5.247</td> <td> 0.000</td> <td>    0.038</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_nature_and_views</th>            <td>   -0.0461</td> <td>    0.020</td> <td>   -2.363</td> <td> 0.018</td> <td>   -0.084</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_pets_allowed</th>                <td>   -0.0247</td> <td>    0.008</td> <td>   -2.947</td> <td> 0.003</td> <td>   -0.041</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_private_entrance</th>            <td>    0.1741</td> <td>    0.009</td> <td>   20.290</td> <td> 0.000</td> <td>    0.157</td> <td>    0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_smoking_allowed</th>             <td>   -0.1370</td> <td>    0.008</td> <td>  -16.761</td> <td> 0.000</td> <td>   -0.153</td> <td>   -0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_tv</th>                          <td>    0.1674</td> <td>    0.007</td> <td>   25.584</td> <td> 0.000</td> <td>    0.155</td> <td>    0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am_white_goods</th>                 <td>   -0.0471</td> <td>    0.009</td> <td>   -5.502</td> <td> 0.000</td> <td>   -0.064</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms_log</th>                  <td>    0.0204</td> <td>    0.015</td> <td>    1.333</td> <td> 0.183</td> <td>   -0.010</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>                       <td>    0.3997</td> <td>    0.006</td> <td>   65.361</td> <td> 0.000</td> <td>    0.388</td> <td>    0.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>calculated_host_listings_count</th> <td>   -0.0024</td> <td>    0.001</td> <td>   -3.592</td> <td> 0.000</td> <td>   -0.004</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>host_is_superhost</th>              <td>    0.0682</td> <td>    0.009</td> <td>    7.215</td> <td> 0.000</td> <td>    0.050</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instant_bookable</th>               <td>    0.0134</td> <td>    0.007</td> <td>    2.025</td> <td> 0.043</td> <td>    0.000</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>latitude</th>                       <td>    0.1134</td> <td>    0.097</td> <td>    1.168</td> <td> 0.243</td> <td>   -0.077</td> <td>    0.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>longitude</th>                      <td>   -0.0579</td> <td>    0.052</td> <td>   -1.115</td> <td> 0.265</td> <td>   -0.160</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>maximum_nights</th>                 <td> 3.933e-05</td> <td> 5.59e-06</td> <td>    7.031</td> <td> 0.000</td> <td> 2.84e-05</td> <td> 5.03e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>minimum_nights_log</th>             <td>   -0.0267</td> <td>    0.002</td> <td>  -11.076</td> <td> 0.000</td> <td>   -0.031</td> <td>   -0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>occupancy_class</th>                <td>   -0.0099</td> <td>    0.002</td> <td>   -4.946</td> <td> 0.000</td> <td>   -0.014</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>price_extra_fees_sqrt</th>          <td>    0.0073</td> <td>    0.000</td> <td>   19.096</td> <td> 0.000</td> <td>    0.007</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>price_extra_people</th>             <td>   -0.0048</td> <td>    0.000</td> <td>  -13.323</td> <td> 0.000</td> <td>   -0.006</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>review_scores_rating_sqrt</th>      <td>    0.0186</td> <td>    0.002</td> <td>    8.434</td> <td> 0.000</td> <td>    0.014</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>text_len_sqrt</th>                  <td>    0.0785</td> <td>    0.020</td> <td>    3.869</td> <td> 0.000</td> <td>    0.039</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wk_mth_discount</th>                <td>    0.4165</td> <td>    0.035</td> <td>   12.000</td> <td> 0.000</td> <td>    0.348</td> <td>    0.484</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>486.861</td> <th>  Durbin-Watson:     </th> <td>   1.874</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1002.504</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.094</td>  <th>  Prob(JB):          </th> <td>2.04e-218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.992</td>  <th>  Cond. No.          </th> <td>1.39e+06</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.39e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              price_log   R-squared:                       0.378\n",
       "Model:                            OLS   Adj. R-squared:                  0.378\n",
       "Method:                 Least Squares   F-statistic:                     512.8\n",
       "Date:                Wed, 22 Jul 2020   Prob (F-statistic):               0.00\n",
       "Time:                        18:57:01   Log-Likelihood:                -15450.\n",
       "No. Observations:               23623   AIC:                         3.096e+04\n",
       "Df Residuals:                   23594   BIC:                         3.119e+04\n",
       "Df Model:                          28                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==================================================================================================\n",
       "                                     coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------------\n",
       "Intercept                         -2.4444      5.219     -0.468      0.640     -12.674       7.785\n",
       "accommodates_per_bed               0.0616      0.005     12.707      0.000       0.052       0.071\n",
       "accommodates_per_room              0.1134      0.003     37.419      0.000       0.107       0.119\n",
       "am_balcony                         0.0962      0.009     11.240      0.000       0.079       0.113\n",
       "am_breakfast                       0.0711      0.014      5.262      0.000       0.045       0.098\n",
       "am_child_friendly                  0.0227      0.007      3.132      0.002       0.008       0.037\n",
       "am_elevator                        0.1620      0.008     21.469      0.000       0.147       0.177\n",
       "am_essentials                      0.0607      0.012      5.247      0.000       0.038       0.083\n",
       "am_nature_and_views               -0.0461      0.020     -2.363      0.018      -0.084      -0.008\n",
       "am_pets_allowed                   -0.0247      0.008     -2.947      0.003      -0.041      -0.008\n",
       "am_private_entrance                0.1741      0.009     20.290      0.000       0.157       0.191\n",
       "am_smoking_allowed                -0.1370      0.008    -16.761      0.000      -0.153      -0.121\n",
       "am_tv                              0.1674      0.007     25.584      0.000       0.155       0.180\n",
       "am_white_goods                    -0.0471      0.009     -5.502      0.000      -0.064      -0.030\n",
       "bathrooms_log                      0.0204      0.015      1.333      0.183      -0.010       0.050\n",
       "bedrooms                           0.3997      0.006     65.361      0.000       0.388       0.412\n",
       "calculated_host_listings_count    -0.0024      0.001     -3.592      0.000      -0.004      -0.001\n",
       "host_is_superhost                  0.0682      0.009      7.215      0.000       0.050       0.087\n",
       "instant_bookable                   0.0134      0.007      2.025      0.043       0.000       0.026\n",
       "latitude                           0.1134      0.097      1.168      0.243      -0.077       0.304\n",
       "longitude                         -0.0579      0.052     -1.115      0.265      -0.160       0.044\n",
       "maximum_nights                  3.933e-05   5.59e-06      7.031      0.000    2.84e-05    5.03e-05\n",
       "minimum_nights_log                -0.0267      0.002    -11.076      0.000      -0.031      -0.022\n",
       "occupancy_class                   -0.0099      0.002     -4.946      0.000      -0.014      -0.006\n",
       "price_extra_fees_sqrt              0.0073      0.000     19.096      0.000       0.007       0.008\n",
       "price_extra_people                -0.0048      0.000    -13.323      0.000      -0.006      -0.004\n",
       "review_scores_rating_sqrt          0.0186      0.002      8.434      0.000       0.014       0.023\n",
       "text_len_sqrt                      0.0785      0.020      3.869      0.000       0.039       0.118\n",
       "wk_mth_discount                    0.4165      0.035     12.000      0.000       0.348       0.484\n",
       "==============================================================================\n",
       "Omnibus:                      486.861   Durbin-Watson:                   1.874\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1002.504\n",
       "Skew:                           0.094   Prob(JB):                    2.04e-218\n",
       "Kurtosis:                       3.992   Cond. No.                     1.39e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.39e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model summary\n",
    "reg_ols_wo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Feature Importance (R_Squared) for all features in \"data\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Select explanatory variables\n",
    "explanatory_vars = list(X.columns)\n",
    "explanatory_vars = [e for e in explanatory_vars] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared for each possible feature:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEATURE</th>\n",
       "      <th>R_SQUARED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>room_type</td>\n",
       "      <td>0.311276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.171072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zipcode</td>\n",
       "      <td>0.117642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neighbourhood</td>\n",
       "      <td>0.098260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>am_tv</td>\n",
       "      <td>0.092606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>price_extra_fees_sqrt</td>\n",
       "      <td>0.063289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>am_private_entrance</td>\n",
       "      <td>0.047946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>accommodates_per_room</td>\n",
       "      <td>0.045129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cancellation_policy</td>\n",
       "      <td>0.043728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>am_child_friendly</td>\n",
       "      <td>0.040059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>am_smoking_allowed</td>\n",
       "      <td>0.034097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>am_elevator</td>\n",
       "      <td>0.033502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>am_balcony</td>\n",
       "      <td>0.030051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bathrooms_log</td>\n",
       "      <td>0.029616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_len_sqrt</td>\n",
       "      <td>0.023283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>accommodates_per_bed</td>\n",
       "      <td>0.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>property_type</td>\n",
       "      <td>0.012352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>host_is_superhost</td>\n",
       "      <td>0.012267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>price_extra_people</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>occupancy_class</td>\n",
       "      <td>0.010728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  FEATURE  R_SQUARED\n",
       "3               room_type   0.311276\n",
       "18               bedrooms   0.171072\n",
       "0                 zipcode   0.117642\n",
       "9           neighbourhood   0.098260\n",
       "21                  am_tv   0.092606\n",
       "7   price_extra_fees_sqrt   0.063289\n",
       "23    am_private_entrance   0.047946\n",
       "31  accommodates_per_room   0.045129\n",
       "16    cancellation_policy   0.043728\n",
       "28      am_child_friendly   0.040059\n",
       "22     am_smoking_allowed   0.034097\n",
       "27            am_elevator   0.033502\n",
       "30             am_balcony   0.030051\n",
       "19          bathrooms_log   0.029616\n",
       "2           text_len_sqrt   0.023283\n",
       "32   accommodates_per_bed   0.022547\n",
       "5           property_type   0.012352\n",
       "15      host_is_superhost   0.012267\n",
       "6      price_extra_people   0.010753\n",
       "8         occupancy_class   0.010728"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Top 10 sorted R_Squared among possible features\n",
    "feat_imp_ols = pd.DataFrame([[\"baseline\", 0.0]])\n",
    "print('R squared for each possible feature:' )\n",
    "for explanatory_var in explanatory_vars:\n",
    "    model = '{target} ~ {feature}'.format(target=target, feature=explanatory_var)\n",
    "    rs = smf.ols(formula=model, data=data).fit().rsquared\n",
    "    new_row = pd.DataFrame([[model.split(\" ~ \")[-1], rs]])\n",
    "    feat_imp_ols = pd.concat([new_row, feat_imp_ols], ignore_index=True)\n",
    "feat_imp_ols.columns = [\"FEATURE\", \"R_SQUARED\"]\n",
    "feat_imp_ols.sort_values(by=[\"R_SQUARED\"], axis=0, ascending=False, inplace=True)\n",
    "feat_imp_ols.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using \u001b[1mPRICE_LOG\u001b[0m as the target\n"
     ]
    }
   ],
   "source": [
    "# Print current setting for TARGET\n",
    "target_upper = target.upper()\n",
    "print(\"You are currently using \" + f\"\\033[1m{target_upper}\\033[0m\" + \" as the target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Select models for comparison\n",
    "models={'Baseline': DummyRegressor(strategy='mean'),\n",
    "        'LinReg': LinearRegression(),\n",
    "        'Passive Aggressive' : PassiveAggressiveRegressor(),\n",
    "        'RANSAC' : RANSACRegressor(),\n",
    "        'ElasticNet' : ElasticNet(),\n",
    "        'Stochastic Gradient Descent': SGDRegressor(max_iter=1000, tol=1e-3),\n",
    "        'Decision Tree': DecisionTreeRegressor(criterion=\"mse\", max_depth=3,random_state=random_state),\n",
    "        'Random Forest': RandomForestRegressor(random_state=random_state, max_features='sqrt', n_jobs=-1),\n",
    "        'Gradient Boost': GradientBoostingRegressor(random_state=random_state),\n",
    "        'XGBoost': XGBRegressor(),\n",
    "        'AdaBoost': AdaBoostRegressor(random_state=random_state)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.6s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.2s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    4.3s remaining:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    5.1s remaining:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    7.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    5.0s remaining:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    8.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.3s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.345622</td>\n",
       "      <td>0.587896</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.467291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>0.185058</td>\n",
       "      <td>0.430183</td>\n",
       "      <td>0.464551</td>\n",
       "      <td>0.332322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Passive Aggressive</td>\n",
       "      <td>0.351310</td>\n",
       "      <td>0.592714</td>\n",
       "      <td>-0.016488</td>\n",
       "      <td>0.455845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RANSAC</td>\n",
       "      <td>0.350483</td>\n",
       "      <td>0.592016</td>\n",
       "      <td>-0.014096</td>\n",
       "      <td>0.443267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.345622</td>\n",
       "      <td>0.587896</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.467291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.432805</td>\n",
       "      <td>0.458005</td>\n",
       "      <td>0.334492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.212829</td>\n",
       "      <td>0.461334</td>\n",
       "      <td>0.384195</td>\n",
       "      <td>0.357838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.154618</td>\n",
       "      <td>0.393215</td>\n",
       "      <td>0.552624</td>\n",
       "      <td>0.299658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gradient Boost</td>\n",
       "      <td>0.154271</td>\n",
       "      <td>0.392774</td>\n",
       "      <td>0.553628</td>\n",
       "      <td>0.300539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.154499</td>\n",
       "      <td>0.393064</td>\n",
       "      <td>0.552969</td>\n",
       "      <td>0.300764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.216637</td>\n",
       "      <td>0.465443</td>\n",
       "      <td>0.373178</td>\n",
       "      <td>0.365512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model       MSE      RMSE        R2       MAE\n",
       "0                      Baseline  0.345622  0.587896 -0.000030  0.467291\n",
       "1                        LinReg  0.185058  0.430183  0.464551  0.332322\n",
       "2            Passive Aggressive  0.351310  0.592714 -0.016488  0.455845\n",
       "3                        RANSAC  0.350483  0.592016 -0.014096  0.443267\n",
       "4                    ElasticNet  0.345622  0.587896 -0.000030  0.467291\n",
       "5   Stochastic Gradient Descent  0.187320  0.432805  0.458005  0.334492\n",
       "6                 Decision Tree  0.212829  0.461334  0.384195  0.357838\n",
       "7                 Random Forest  0.154618  0.393215  0.552624  0.299658\n",
       "8                Gradient Boost  0.154271  0.392774  0.553628  0.300539\n",
       "9                       XGBoost  0.154499  0.393064  0.552969  0.300764\n",
       "10                     AdaBoost  0.216637  0.465443  0.373178  0.365512"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and display results\n",
    "results = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'R2'])\n",
    "i = 0\n",
    "for m in models.items():\n",
    "    # Building a full pipeline with our preprocessor and a Classifier\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), (m[0], m[1])])\n",
    "    # Making predictions on the training set using cross validation as well as calculating the probabilities\n",
    "    y_train_pred = cross_val_predict(pipe,\n",
    "                                     X_train,\n",
    "                                     y_train.values.ravel(),\n",
    "                                     cv=5,\n",
    "                                     verbose=4,\n",
    "                                     n_jobs=-1)\n",
    "    # Calculating metrices\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            'Model': m[0],\n",
    "            'MSE': mean_squared_error(y_train, y_train_pred),\n",
    "            'RMSE': mean_squared_error(\n",
    "                y_train, y_train_pred, squared=False),\n",
    "            'MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "            'R2': r2_score(y_train, y_train_pred)\n",
    "        },\n",
    "        index=[i])\n",
    "    i += 1\n",
    "    results = pd.concat([results, temp])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Pre-Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create pipeline to use in GridSearchCV\n",
    "pipeline_rf_reg = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rf_reg', RandomForestRegressor(n_estimators=110,\n",
    "                              random_state=random_state,\n",
    "                              max_depth=5,\n",
    "                              max_features=20,\n",
    "                              n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'ccp_alpha', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display possible hyperparameters for RandomForestRegressor\n",
    "test_rf_reg = RandomForestRegressor()\n",
    "test_rf_reg.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default values for RandomForestRegressor** (as base for hyperparameter search):\n",
    "\n",
    "n_estimators=100, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter distribution\n",
    "param_distribs_rf_reg = {\n",
    "        'rf_reg__n_estimators': randint(low=10, high=200),\n",
    "        'rf_reg__max_features': randint(low=1, high=50),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "# Create and fit RandomizedSearchCV, save \"best_model\"\n",
    "rnd_rf_reg = RandomizedSearchCV(pipeline_rf_reg, param_distribs_rf_reg, cv=5, scoring=scoring, n_iter=20, \n",
    "                           return_train_score=True, verbose=4, n_jobs=-1, random_state=random_state)\n",
    "\n",
    "best_model_rnd_rf_reg = rnd_rf_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "-0.19\n",
      "Best parameters:\n",
      "{'rf_reg__max_features': 21, 'rf_reg__n_estimators': 170}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(rnd_rf_reg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(rnd_rf_reg.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf_reg_fi = pipeline_rf_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns = list(\n",
    "    pipeline_rf_reg_fi.named_steps['preprocessor'].named_transformers_['cat'].\n",
    "    named_steps['1hot'].get_feature_names(input_features=cat_features))\n",
    "features_prep_list = list(num_features)\n",
    "features_prep_list.extend(onehot_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.4685\n",
       "                \n",
       "                    &plusmn; 0.3670\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                room_type_Private room\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.92%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.2279\n",
       "                \n",
       "                    &plusmn; 0.2896\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                bedrooms\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.32%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0589\n",
       "                \n",
       "                    &plusmn; 0.1531\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_tv\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.45%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0566\n",
       "                \n",
       "                    &plusmn; 0.1074\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                accommodates_per_room\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.95%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0478\n",
       "                \n",
       "                    &plusmn; 0.0562\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                bathrooms_log\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.06%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0303\n",
       "                \n",
       "                    &plusmn; 0.1061\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                price_extra_fees_sqrt\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.93%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0183\n",
       "                \n",
       "                    &plusmn; 0.0288\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                room_type_Shared room\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.05%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0168\n",
       "                \n",
       "                    &plusmn; 0.0261\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                calculated_host_listings_count\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.52%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0114\n",
       "                \n",
       "                    &plusmn; 0.0201\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                property_type_Boutique hotel\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.66%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0099\n",
       "                \n",
       "                    &plusmn; 0.0196\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                room_type_Hotel room\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.69%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0096\n",
       "                \n",
       "                    &plusmn; 0.0262\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                accommodates_per_bed\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.77%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0087\n",
       "                \n",
       "                    &plusmn; 0.0143\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                latitude\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.82%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0082\n",
       "                \n",
       "                    &plusmn; 0.0132\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                minimum_nights_log\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.92%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0072\n",
       "                \n",
       "                    &plusmn; 0.0161\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_elevator\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.48%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0026\n",
       "                \n",
       "                    &plusmn; 0.0059\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                longitude\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.48%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0025\n",
       "                \n",
       "                    &plusmn; 0.0213\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_private_entrance\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.48%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0025\n",
       "                \n",
       "                    &plusmn; 0.0059\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                text_len_sqrt\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0024\n",
       "                \n",
       "                    &plusmn; 0.0069\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                price_extra_people\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.63%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0016\n",
       "                \n",
       "                    &plusmn; 0.0047\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                occupancy_class\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.66%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0014\n",
       "                \n",
       "                    &plusmn; 0.0196\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_child_friendly\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.69%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0012\n",
       "                \n",
       "                    &plusmn; 0.0068\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_balcony\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.70%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0011\n",
       "                \n",
       "                    &plusmn; 0.0110\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_smoking_allowed\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.70%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0011\n",
       "                \n",
       "                    &plusmn; 0.0073\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                host_is_superhost\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0010\n",
       "                \n",
       "                    &plusmn; 0.0026\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                maximum_nights\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.77%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0031\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                wk_mth_discount\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.78%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0025\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                review_scores_rating_sqrt\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.88%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0015\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_breakfast\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.93%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0011\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_white_goods\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.93%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0012\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                cancellation_policy_strict\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.94%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0010\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                instant_bookable\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.95%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0012\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_essentials\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.96%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0008\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_pets_allowed\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.97%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0000\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                property_type_Bed and breakfast\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.99%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0000\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                cancellation_policy_moderate\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.99%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0000\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                property_type_House\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                property_type_Secondary unit\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                cancellation_policy_super_strict\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                am_nature_and_views\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                property_type_Unique space\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "Explanation(estimator='RandomForestRegressor(max_depth=5, max_features=20, n_estimators=110, n_jobs=-1,\\n                      random_state=42)', description='\\nRandom forest feature importances; values are numbers 0 <= x <= 1;\\nall values sum to 1.\\n', error=None, method='feature importances', is_regression=True, targets=None, feature_importances=FeatureImportances(importances=[FeatureWeight(feature='room_type_Private room', weight=0.468469533110897, std=0.18349840376392576, value=None), FeatureWeight(feature='bedrooms', weight=0.22785716072641576, std=0.14480435553571516, value=None), FeatureWeight(feature='am_tv', weight=0.05889343295318714, std=0.07657279212156898, value=None), FeatureWeight(feature='accommodates_per_room', weight=0.05659254532209405, std=0.05372003685615904, value=None), FeatureWeight(feature='bathrooms_log', weight=0.04778906194999856, std=0.02808939436798996, value=None), FeatureWeight(feature='price_extra_fees_sqrt', weight=0.03031430847215936, std=0.053061184894218905, value=None), FeatureWeight(feature='room_type_Shared room', weight=0.018333074072741234, std=0.014388713526867353, value=None), FeatureWeight(feature='calculated_host_listings_count', weight=0.01680300485795362, std=0.013051279319133682, value=None), FeatureWeight(feature='property_type_Boutique hotel', weight=0.011388075965976767, std=0.010074998527806897, value=None), FeatureWeight(feature='room_type_Hotel room', weight=0.00986877950777283, std=0.009782912363882294, value=None), FeatureWeight(feature='accommodates_per_bed', weight=0.009573429331093076, std=0.013120752008619513, value=None), FeatureWeight(feature='latitude', weight=0.00871732273581402, std=0.007140512477220172, value=None), FeatureWeight(feature='minimum_nights_log', weight=0.008214861307854027, std=0.0065933703806019695, value=None), FeatureWeight(feature='am_elevator', weight=0.0072034539970656335, std=0.008066598076751827, value=None), FeatureWeight(feature='longitude', weight=0.0025525167202695292, std=0.0029258881488528955, value=None), FeatureWeight(feature='am_private_entrance', weight=0.0025427811541464214, std=0.010648754907616204, value=None), FeatureWeight(feature='text_len_sqrt', weight=0.0025396929842750457, std=0.002969749042917084, value=None), FeatureWeight(feature='price_extra_people', weight=0.0023967643974235494, std=0.003474060273549355, value=None), FeatureWeight(feature='occupancy_class', weight=0.0015907112075712602, std=0.002358178331543673, value=None), FeatureWeight(feature='am_child_friendly', weight=0.0013693319132114063, std=0.009814880118952924, value=None), FeatureWeight(feature='am_balcony', weight=0.001213747170572608, std=0.0033864752258759728, value=None), FeatureWeight(feature='am_smoking_allowed', weight=0.0011478592444375245, std=0.005511810512923389, value=None), FeatureWeight(feature='host_is_superhost', weight=0.0011459137643414854, std=0.003634818406648297, value=None), FeatureWeight(feature='maximum_nights', weight=0.0009852585493261097, std=0.0013163125545792144, value=None), FeatureWeight(feature='wk_mth_discount', weight=0.0008106772616764621, std=0.0015517354844690938, value=None), FeatureWeight(feature='review_scores_rating_sqrt', weight=0.0007613830441489764, std=0.0012715602172675361, value=None), FeatureWeight(feature='am_breakfast', weight=0.0002967775067210692, std=0.0007395232230942638, value=None), FeatureWeight(feature='am_white_goods', weight=0.00014441893273399122, std=0.0005556295112683501, value=None), FeatureWeight(feature='cancellation_policy_strict', weight=0.0001357731284386657, std=0.0005878519008552023, value=None), FeatureWeight(feature='instant_bookable', weight=0.00010924700335557807, std=0.0004969993460803003, value=None), FeatureWeight(feature='am_essentials', weight=0.00010201819310141775, std=0.0005894848800918577, value=None), FeatureWeight(feature='am_pets_allowed', weight=7.400293445557156e-05, std=0.0004184982913817667, value=None), FeatureWeight(feature='property_type_Bed and breakfast', weight=4.0672404051541804e-05, std=0.00021277549169222614, value=None), FeatureWeight(feature='cancellation_policy_moderate', weight=1.281657189240346e-05, std=0.00013101876373472962, value=None), FeatureWeight(feature='property_type_House', weight=9.591602826138476e-06, std=0.00010013927341661835, value=None), FeatureWeight(feature='property_type_Secondary unit', weight=0.0, std=0.0, value=None), FeatureWeight(feature='cancellation_policy_super_strict', weight=0.0, std=0.0, value=None), FeatureWeight(feature='am_nature_and_views', weight=0.0, std=0.0, value=None), FeatureWeight(feature='property_type_Unique space', weight=0.0, std=0.0, value=None)], remaining=0), decision_tree=None, highlight_spaces=None, transition_features=None, image=None)"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.explain_weights(pipeline_rf_reg_fi.named_steps['rf_reg'], top=50, feature_names=features_prep_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid_rf_reg = [\n",
    "    {'rf_reg__n_estimators': [30, 50, 70], 'rf_reg__max_features': [8, 10, 15, 25]},\n",
    "    {'rf_reg__bootstrap': [False], 'rf_reg__n_estimators': [30, 50, 70], 'rf_reg__max_features': [8, 10, 15, 25]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   50.7s finished\n"
     ]
    }
   ],
   "source": [
    "# Create and fit GridSearchCV, save \"best_model\"\n",
    "grid_rf_reg = GridSearchCV(pipeline_rf_reg, param_grid_rf_reg, cv=5, scoring=scoring, \n",
    "                           return_train_score=True, verbose=4, n_jobs=-1, random_state=random_state)\n",
    "\n",
    "grid_rf_reg.fit(X_train, y_train)\n",
    "best_model_rf_reg = grid_rf_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "-0.19\n",
      "Best parameters:\n",
      "{'rf_reg__max_features': 25, 'rf_reg__n_estimators': 70}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(grid_rf_reg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_rf_reg.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Get and print feature_importances\n",
    "#feature_importances = grid_rf_reg.best_estimator_.feature_importances_\n",
    "#feature_importances\n",
    "#cat_encoder = preprocessor.named_transformers_[\"cat\"]\n",
    "#cat_one_hot_features = list(cat_encoder.categories_[0])\n",
    "#attributes = num_features + cat_one_hot_features\n",
    "#sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed evaluation with training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-529-14fa9d71d116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predict target with \"best model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_train_pred_rf_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model_rf_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_prep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;31m# TODO: also call _check_n_features(reset=False) in 0.24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_feature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_transform_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X, y, func, fitted)\u001b[0m\n\u001b[1;32m    465\u001b[0m                     message=self._log_message(name, idx, len(transformers)))\n\u001b[1;32m    466\u001b[0m                 for idx, (name, trans, column, weight) in enumerate(\n\u001b[0;32m--> 467\u001b[0;31m                         self._iter(fitted=fitted, replace_strings=True), 1))\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"Expected 2D array, got 1D array instead\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    817\u001b[0m                 \u001b[0mbig_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0mislice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ColumnTransformer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     message=self._log_message(name, idx, len(transformers)))\n\u001b[0;32m--> 466\u001b[0;31m                 for idx, (name, trans, column, weight) in enumerate(\n\u001b[0m\u001b[1;32m    467\u001b[0m                         self._iter(fitted=fitted, replace_strings=True), 1))\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindices_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         raise ValueError(\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;34m\"Specifying the columns using strings is only supported for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;34m\"pandas DataFrames\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_train_pred_rf_reg = best_model_rf_reg.predict(X_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "print(\"MSE: {:.2f}\".format(mean_squared_error(y_test, y_train_pred_rf_reg))),\n",
    "print(\"RMSE: {:.2f}\".format(mean_squared_error(y_test, y_train_pred_rf_reg, squared=False))),\n",
    "print(\"MAE: {:.2f}\".format(mean_absolute_error(y_test, y_train_pred_rf_reg))),\n",
    "print(\"R2: {:.2f}\".format(r2_score(y_test, y_train_pred_rf_reg))),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display confidence interval (scipy stats)\n",
    "confidence = 0.95\n",
    "squared_errors = (y_train_pred_rf_reg - y_test) ** 2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "                         loc=squared_errors.mean(), \n",
    "                          scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation with testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Transform X_test for final evaluation\n",
    "#X_test_prep = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "#y_pred_rf_reg = best_model_rf_reg.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "#print(\"MSE: {:.2f}\".format(mean_squared_error(y_test, y_pred_rf_reg))),\n",
    "#print(\"RMSE: {:.2f}\".format(mean_squared_error(y_test, y_pred_rf_reg, squared=False))),\n",
    "#print(\"MAE: {:.2f}\".format(mean_absolute_error(y_test, y_pred_rf_reg))),\n",
    "#print(\"R2: {:.2f}\".format(r2_score(y_test, y_pred_rf_reg))),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display confidence interval (scipy stats)\n",
    "#confidence = 0.95\n",
    "#squared_errors = (y_pred_rf_reg - y_test) ** 2\n",
    "#np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "#                         loc=squared_errors.mean(), \n",
    "#                         scale=stats.sem(squared_errors)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
