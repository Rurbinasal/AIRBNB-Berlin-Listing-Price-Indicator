{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains predictive modeling for various distinct target options, namely:\n",
    "\n",
    "3 Modeling: Binary Classification (**PRICE_BINARY**)\n",
    "- Binary split is set as a USD Price amount in the Dashboard\n",
    "\n",
    "4 Modeling: Multi-Class Classification (**PRICE_CLASS**)\n",
    "- TBD\n",
    "\n",
    "5 Modeling: Regression (**PRICE_LOG**)\n",
    "- TBD\n",
    "\n",
    "**As a consequence, the respective targets need to be selected via the Dashboard BEFORE running the corresponding section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline  # Same, but with the latter it is not necessary to name estimator and transformer\n",
    "#from imblearn.pipeline import Pipeline as Imb_Pipe\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, GenericUnivariateSelect, mutual_info_classif\n",
    "import eli5\n",
    "\n",
    "# Predictive Modeling (Models)\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_predict, cross_val_score, cross_validate, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, NuSVC, SVR\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, PassiveAggressiveRegressor, ElasticNet, SGDRegressor, RANSACRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor, VotingClassifier, RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from scipy.stats import randint\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer, fbeta_score, accuracy_score, confusion_matrix, f1_score, precision_recall_curve, recall_score, precision_score, roc_auc_score\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Neural Networks\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import data_engineered\n",
    "data = pd.read_pickle(f\"saves/{dataset_loc}_{dataset_date}/data_engineered.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "\n",
    "#... by removing certain features\n",
    "all_features = [\n",
    "    el for el in data.columns if el not in [\n",
    "        'occupancy_rate', 'occupancy_class', 'listing_no', 'price_log', 'price_class',\n",
    "        'price_binary', \"review_scores_class_new\", \"review_scores_class\",\n",
    "        \"review_scores_calc\", \"neighbourhood_cleansed\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "#... by only considering certain features\n",
    "key_features = [\n",
    "    \"accommodates\", \"accommodates_per_bed\", \"am_balcony\", \"am_breakfast\",\n",
    "    \"am_child_friendly\", \"am_elevator\", \"am_essentials\", \"am_pets_allowed\",\n",
    "    \"am_private_entrance\", \"am_smoking_allowed\", \"am_tv\", \"availability_90\", \"bathrooms_log\", \"bedrooms\",\n",
    "    \"calc_host_lst_count_sqrt_log\", \"cancellation_policy\", \"host_is_superhost\", \"instant_bookable\",\n",
    "    \"maximum_nights\", \"minimum_nights_log\", \"property_type\", \"room_type\", \"wk_mth_discount\",\n",
    "    \"zipcode\"\n",
    "]\n",
    "\n",
    "#Display columns:\n",
    "#all_features\n",
    "#key_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Dashboard (Global Variables)\n",
    "dataset_loc = \"berlin\"              # \"berlin\", \"paris\", \"amsterdam\"\n",
    "dataset_date = \"2020-03-17\"         # \"2019-12-11\", \"2020-01-10\", \"2020-02-18\", \"2020-03-17\", \"2020-05-14\"\n",
    "target = 'price_log'             # for regression: 'occupancy_rate', 'price_log' | for classification: 'price_class', 'occupancy_class'\n",
    "binary_split = 50                   # price at which \"price_binary\" will be split\n",
    "pred_features = key_features        # select from cell \"feature selection\" above: [all_features, key_features]\n",
    "scoring = 'neg_median_absolute_error'  # for regression: 'neg_mean_squared_error', 'r2', 'neg_mean_poisson_deviance', 'neg_median_absolute_error' | for classification: \"f1(_micro, _macro, _weighted for multiclass)\", \"recall\", \"precision\", \"accuracy\", \"roc_auc\"\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using \u001b[1mPRICE_LOG\u001b[0m as the target and \u001b[1mneg_median_absolute_error\u001b[0m for scoring to predict prices for \u001b[1mberlin\u001b[0m on \u001b[1m2020-03-17\u001b[0m\n",
      "\n",
      "You are currently using these features for its prediction:\n",
      "\u001b[1m['accommodates', 'accommodates_per_bed', 'am_balcony', 'am_breakfast', 'am_child_friendly', 'am_elevator', 'am_essentials', 'am_pets_allowed', 'am_private_entrance', 'am_smoking_allowed', 'am_tv', 'availability_90', 'bathrooms_log', 'bedrooms', 'calc_host_lst_count_sqrt_log', 'cancellation_policy', 'host_is_superhost', 'instant_bookable', 'maximum_nights', 'minimum_nights_log', 'property_type', 'room_type', 'wk_mth_discount', 'zipcode']\u001b[0m\n",
      "\n",
      "No issues with your selection of pred_features have been detected. Please make sure to manually check for correctness nevertheless.\n"
     ]
    }
   ],
   "source": [
    "# Display target and used features\n",
    "# Print current setting for TARGET\n",
    "target_upper = target.upper()\n",
    "print(f\"You are currently using \\033[1m{target_upper}\\033[0m as the target and \\033[1m{scoring}\\033[0m for scoring to predict prices for \\033[1m{dataset_loc}\\033[0m on \\033[1m{dataset_date}\\033[0m\\n\")\n",
    "print(f\"You are currently using these features for its prediction:\\n\\033[1m{pred_features}\\033[0m\\n\")\n",
    "\n",
    "if target in pred_features:\n",
    "    print(f\"WARNING: You are using \\033[1m{target_upper}\\033[0m as predictor. Please remove before proceeding.\\n\")\n",
    "\n",
    "if target == 'price_binary':\n",
    "    if 'price_class' in pred_features:\n",
    "        print(f\"WARNING: Please remove \\033[1m'price_class'\\033[0m from features before proceeding with target \\033[1m{target_upper}\\033[0m\")\n",
    "    if 'price_log' in pred_features:\n",
    "        print(f\"WARNING: Please remove \\033[1m'price_log'\\033[0m from features before proceeding with target \\033[1m{target_upper}\\033[0m\")\n",
    "\n",
    "if target == 'price_class':\n",
    "    if 'price_binary' in pred_features:\n",
    "        print(f\"WARNING: Please remove \\033[1m'price_binary'\\033[0m from features before proceeding with target \\033[1m{target_upper}\\033[0m\")\n",
    "    if 'price_log' in pred_features:\n",
    "        print(f\"WARNING: Please remove \\033[1m'price_log'\\033[0m from features before proceeding with target \\033[1m{target_upper}\\033[0m\")\n",
    "\n",
    "if target == 'price_log':\n",
    "    if 'price_class' in pred_features:\n",
    "        print(f\"WARNING: Please remove \\033[1m'price_class'\\033[0m from features before proceeding with target \\033[1m{target_upper}\\033[0m\")\n",
    "    if 'price_binary' in pred_features:\n",
    "        print(f\"WARNING: Please remove \\033[1m'price_binary'\\033[0m from features before proceeding with target \\033[1m{target_upper}\\033[0m\")\n",
    "        \n",
    "if \"occupancy_class\" in pred_features and \"occupancy_rate\" in pred_features:\n",
    "    print(f\"WARNING: Please remove \\033[1m'ocupancy_class'\\033[0m or \\033[1m'ocupancy_rate'\\033[0m from features before proceeding with target \\033[1m{target_upper}\\033[0m\")\n",
    "\n",
    "else:\n",
    "    print(\"No issues with your selection of pred_features have been detected. Please make sure to manually check for correctness nevertheless.\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Functions and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create \"price_binary\" at given split\n",
    "price_binary = []\n",
    "for price in data.price_log:\n",
    "    if math.exp(price) <= binary_split:\n",
    "        price_binary.append(1)\n",
    "    else:\n",
    "        price_binary.append(2)\n",
    "data[\"price_binary\"] = price_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \"mean_absolute_percentage_error\": Function for mean absolute percentage error (MAPE)\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \"median_absolute_percentage_error\": Function for median absolute percentage error (MAPE median)\n",
    "def median_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.median(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \"print_target_setting\": Function for printing current setting for TARGET and the corresponding features\n",
    "def print_target_setting():\n",
    "    target_upper = target.upper()\n",
    "    y_upper = y_train.name.upper()\n",
    "    print(f\"You are currently using \\033[1m{target_upper}\\033[0m as the target and \\033[1m{scoring}\\033[0m for scoring to predict prices for \\033[1m{dataset_loc}\\033[0m on \\033[1m{dataset_date}\\033[0m\\n\")\n",
    "    print(f\"The target variable y is set to \\033[1m{y_upper}\\033[0m\\n\")\n",
    "    print(f\"You are currently using these features for its prediction:\\n\\033[1m{pred_features}\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \"get_feat_importances\": Function for retrieving feature importances\n",
    "def get_feat_importances(model, column_names=column_names):\n",
    "    model=model\n",
    "    feat_importances = pd.DataFrame(model.feature_importances_,\n",
    "                 columns=['weight'],\n",
    "                 index=column_names)\n",
    "    feat_importances.sort_values('weight', inplace=True, ascending=False)\n",
    "    return feat_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \"model_eval\": Function for final evaluation of \"best model\"\n",
    "def model_eval(y, y_pred, model=\"reg\"):\n",
    "    \"\"\"\n",
    "    Please always specify the type of model:\n",
    "    Regression: model=\"reg\"\n",
    "    Binary Classification: model=\"bclf\"\n",
    "    Multiclass Classification: model=\"clf\"\n",
    "    \"\"\"\n",
    "    if model==\"reg\":\n",
    "        print(\"MSE: {:.2f}\".format(mean_squared_error(y, y_pred)))\n",
    "        print(\"RMSE: {:.2f}\".format(\n",
    "        mean_squared_error(y, y_pred, squared=False)))\n",
    "        print(\"MAE: {:.2f}\".format(mean_absolute_error(y, y_pred)))\n",
    "        print(\"R2: {:.2f}\".format(r2_score(y, y_pred)))\n",
    "        print(\"MAPE: {:.2f}\".format(mean_absolute_percentage_error(y, y_pred)))\n",
    "        print(\"MAPE median: {:.2f}\".format(median_absolute_percentage_error(y, y_pred)))\n",
    "\n",
    "    elif model==\"bclf\":\n",
    "        print(\"Accuracy: {:.2f}\".format(accuracy_score(y, y_pred)))\n",
    "        print(\"Recall: {:.2f}\".format(recall_score(y, y_pred)))\n",
    "        print(\"Precision: {:.2f}\".format(precision_score(y, y_pred)))\n",
    "        print(\"F1 Score: {:.2f}\".format(f1_score(y, y_pred)))\n",
    "        print(\"ROC/AUC: {:.2f}\".format(roc_auc_score(y, y_pred)))\n",
    "        print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y, y_pred)))\n",
    "\n",
    "    elif model==\"clf\":\n",
    "        print(\"Accuracy: {:.2f}\".format(accuracy_score(y, y_pred)))\n",
    "        print(\"Recall: {:.2f}\".format(recall_score(y, y_pred, average='weighted')))\n",
    "        print(\"Precision: {:.2f}\".format(precision_score(y, y_pred, average='weighted')))\n",
    "        print(\"F1 Score: {:.2f}\".format(f1_score(y, y_pred, average='weighted')))\n",
    "        print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y, y_pred)))\n",
    "    \n",
    "    else:\n",
    "        print(\"Please revise your parameters (e.g. provide a valid model).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \"save_model\": Function for saving model (pickle or joblib)\n",
    "def save_model(model, title=\"unknown\", save=\"joblib\"):\n",
    "    if save==\"joblib\":\n",
    "        joblib.dump(model, f\"saves/{dataset_loc}_{dataset_date}/{title}.pkl\")\n",
    "    elif save==\"pickle\":\n",
    "        model.to_pickle(f\"saves/{dataset_loc}_{dataset_date}/{title}.pkl\")\n",
    "#        save also evaluation (from y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \"load_model\": Function for loading model (pickle or joblib)\n",
    "def load_model(title=\"unknown\", dataset_loc=dataset_loc, dataset_date=dataset_date, load=\"joblib\"):\n",
    "    if load==\"joblib\":\n",
    "        return joblib.load(f\"saves/{dataset_loc}_{dataset_date}/{title}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \"clf_learning_curves\": Function to evaluate classification model based on learning curves\n",
    "def clf_learning_curves(model):\n",
    "# Fit model on training data\n",
    "    model = model\n",
    "    eval_set = [(X_train_prep, y_train), (X_test_prep, y_test)]\n",
    "    model.fit(X_train_prep, y_train, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\n",
    "\n",
    "    # Make predictions for test data\n",
    "    y_pred = model.predict(X_test_prep)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # Evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    \n",
    "    # Retrieve performance metrics\n",
    "    results = model.evals_result()\n",
    "    epochs = len(results['validation_0']['error'])\n",
    "    x_axis = range(0, epochs)\n",
    "    \n",
    "    # Plot log loss\n",
    "    fig, ax = pyplot.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "    ax.legend()\n",
    "    pyplot.ylabel('Log Loss')\n",
    "    pyplot.title('XGBoost Log Loss')\n",
    "    pyplot.show()\n",
    "    \n",
    "    # Plot classification error\n",
    "    fig, ax = pyplot.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "    ax.legend()\n",
    "    pyplot.ylabel('Classification Error')\n",
    "    pyplot.title('XGBoost Classification Error')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (Train/Test Split and Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "drop_columns = [el for el in data.columns if el not in pred_features]\n",
    "drop_columns.remove(target)\n",
    "data.drop(labels=drop_columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Drop rows (optional, just temporary)\n",
    "#data = data[data.number_of_reviews_ltm_log>1.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cancellation_policy', 'property_type', 'room_type', 'zipcode']"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list for categorical predictors/features (used in \"Scaling with Preprocessing Pipeline\")\n",
    "cat_features = list(data.columns[data.dtypes == object])\n",
    "#cat_features.remove(\"neighbourhood\")\n",
    "#cat_features.remove(\"zipcode\")\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accommodates',\n",
       " 'accommodates_per_bed',\n",
       " 'am_balcony',\n",
       " 'am_breakfast',\n",
       " 'am_child_friendly',\n",
       " 'am_elevator',\n",
       " 'am_essentials',\n",
       " 'am_pets_allowed',\n",
       " 'am_private_entrance',\n",
       " 'am_smoking_allowed',\n",
       " 'am_tv',\n",
       " 'availability_90',\n",
       " 'bathrooms_log',\n",
       " 'bedrooms',\n",
       " 'calc_host_lst_count_sqrt_log',\n",
       " 'host_is_superhost',\n",
       " 'instant_bookable',\n",
       " 'maximum_nights',\n",
       " 'minimum_nights_log',\n",
       " 'wk_mth_discount']"
      ]
     },
     "execution_count": 849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list for numerical predictors/features (removing target column, used in \"Scaling with Preprocessing Pipeline\")\n",
    "num_features = list(data.columns[data.dtypes != object])\n",
    "num_features.remove(target)\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Build preprocessor pipeline\n",
    "# Pipeline for numerical features\n",
    "num_pipeline = Pipeline([('imputer_num', SimpleImputer(strategy='median')),\n",
    "                         ('std_scaler', StandardScaler())])\n",
    "\n",
    "# Pipeline for categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('1hot', OneHotEncoder(drop='first', handle_unknown='error'))\n",
    "])\n",
    "\n",
    "# Complete pipeline\n",
    "preprocessor = ColumnTransformer([('num', num_pipeline, num_features),\n",
    "                                  ('cat', cat_pipeline, cat_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define predictors and target variable\n",
    "X = data.drop([target], axis=1)\n",
    "y = data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=random_state,\n",
    "                                                    shuffle=True)\n",
    "#                                                   stratify=y) # Use stratify=y if labels are inbalanced (e.g. most wines are 5 or 6; check with value_counts()!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving preprocessed X_train and X_test\n",
    "X_train_prep_preprocessor = preprocessor.fit(X_train)\n",
    "\n",
    "X_train_prep = X_train_prep_preprocessor.transform(X_train)\n",
    "X_train_num_prep = num_pipeline.fit_transform(X_train[num_features])\n",
    "X_test_prep = X_train_prep_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get feature names from pipeline after one-hot encoding as \"column_names\"\n",
    "onehot_columns = list(preprocessor.named_transformers_['cat']['1hot'].get_feature_names(cat_features))\n",
    "column_names = num_features + onehot_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "train_outl = num_pipeline.fit_transform(X_train[num_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Fit DBSCAN model\n",
    "outl_model = DBSCAN(eps=3.0, min_samples=10).fit(train_outl)\n",
    "outl_labels = outl_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8456\n",
       "1      34\n",
       "dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display results (# of outliers)\n",
    "pd.Series(outl_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Illustrate results\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "unique_labels = set(outl_labels)\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "for color,label in zip(colors, unique_labels):\n",
    "    sample_mask = [True if l == label else False for l in outl_labels]\n",
    "    plt.plot(train_outl[:,0][sample_mask], train_outl[:, 1][sample_mask], 'o', color=color);\n",
    "plt.xlabel('accommodates_per_bed');\n",
    "plt.ylabel('accommodates_per_room');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the results:\n",
    "\n",
    "- https://www.kaggle.com/kevinarvai/outlier-detection-practice-uni-multivariate\n",
    "- https://datascience.stackexchange.com/questions/46092/how-do-we-interpret-the-outputs-of-dbscan-clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Feature Selection (add most useful to modeling pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Set X_fs to desired variable\n",
    "X_fs = X_train[\n",
    "    num_features]  # X_train_prep, X_train_num_prep, X_train[num_features]\n",
    "#X_fs = pd.DataFrame(X_fs, columns = X_train_prep_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GenericUnivariateSelect** (Classification and Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Apply GenericUnivariateSelect\n",
    "trans_GUS = GenericUnivariateSelect(score_func=lambda X, y: X.mean(axis=0),\n",
    "                                    mode='k_best',\n",
    "                                    param=15)  #mode='percentile', 'k_best'\n",
    "X_train_GUS = trans_GUS.fit_transform(X_fs, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mutual_info_classif** (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Fit mutual_info_classif\n",
    "#X_train_mic = mutual_info_classif(X_fs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "#plt.subplots(1, figsize=(26, 1))\n",
    "#sns.heatmap(X_train_mic[:, np.newaxis].T, cmap='Blues', cbar=False, linewidths=1, annot=True)\n",
    "#plt.yticks([], [])\n",
    "#plt.gca().set_xticklabels(X_fs.columns, rotation=45, ha='right', fontsize=12)\n",
    "#plt.suptitle(\"Feature Importance (mutual_info_classif)\", fontsize=18, y=1.2)\n",
    "#plt.gcf().subplots_adjust(wspace=0.2)\n",
    "#pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Apply GenericUnivariateSelect to reduce features (optional)\n",
    "#trans_mic = GenericUnivariateSelect(score_func=mutual_info_classif, mode='k_best', param=15) #mode='percentile', 'k_best',\n",
    "#X_train_mic_GUS = trans_mic.fit_transform(X_fs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Print kept features\n",
    "#print(\"We started with {0} features but retained only {1} of them!\".format(\n",
    "#    X_fs.shape[1] - 1, X_train_mic_GUS.shape[1]))\n",
    "\n",
    "#columns_retained_Select = X_fs.columns[trans_mic.get_support()].values\n",
    "#pd.DataFrame(X_train_mic_GUS, columns=columns_retained_Select).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chi2** (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mutual_info_regression** (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: Binary Classification (\"price_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using \u001b[1mPRICE_BINARY\u001b[0m as the target and \u001b[1mroc_auc\u001b[0m for scoring to predict prices for \u001b[1mberlin\u001b[0m on \u001b[1m2020-03-17\u001b[0m\n",
      "\n",
      "The target variable y is set to \u001b[1mPRICE_BINARY\u001b[0m\n",
      "\n",
      "You are currently using these features for its prediction:\n",
      "\u001b[1m['accommodates', 'bathrooms_log', 'host_is_superhost', 'property_type', 'room_type', 'zipcode']\u001b[0m\n",
      "\n",
      "Binary Split:  \u001b[1mUSD 50\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Print current setting for TARGET\n",
    "print_target_setting()\n",
    "print(f\"Binary Split:  \\033[1mUSD {binary_split}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Select models for comparison\n",
    "bclfmodels = {\n",
    "    'Baseline':\n",
    "    DummyClassifier(strategy='most_frequent'),\n",
    "    'LogReg':\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    'KNN':\n",
    "    KNeighborsClassifier(),\n",
    "    'SVC':\n",
    "    SVC(kernel='rbf', C=1E6),\n",
    "    'Decision Tree':\n",
    "    DecisionTreeClassifier(criterion=\"gini\",\n",
    "                           max_depth=3,\n",
    "                           random_state=random_state),\n",
    "    'Random Forest':\n",
    "    RandomForestClassifier(random_state=random_state,\n",
    "                           max_features='sqrt',\n",
    "                           n_jobs=-1),\n",
    "    'Gradient Boost':\n",
    "    GradientBoostingClassifier(random_state=random_state),\n",
    "    'XGBoost':\n",
    "    XGBClassifier(),\n",
    "    'AdaBoost':\n",
    "    AdaBoostClassifier(random_state=random_state)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.6s remaining:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.6s finished\n",
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Baseline: \n",
      "[[   0 3993]\n",
      " [   0 4497]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix LogReg: \n",
      "[[2941 1052]\n",
      " [ 793 3704]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.3s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix KNN: \n",
      "[[2837 1156]\n",
      " [ 962 3535]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.3min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix SVC: \n",
      "[[2926 1067]\n",
      " [ 938 3559]]\n",
      "Confusion Matrix Decision Tree: \n",
      "[[2775 1218]\n",
      " [ 693 3804]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.8s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Random Forest: \n",
      "[[2906 1087]\n",
      " [ 867 3630]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.8s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Gradient Boost: \n",
      "[[2871 1122]\n",
      " [ 666 3831]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.8s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix XGBoost: \n",
      "[[2860 1133]\n",
      " [ 667 3830]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.5s remaining:    0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix AdaBoost: \n",
      "[[2904 1089]\n",
      " [ 792 3705]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC/AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.529682</td>\n",
       "      <td>0.529682</td>\n",
       "      <td>0.280563</td>\n",
       "      <td>0.366825</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.782686</td>\n",
       "      <td>0.782686</td>\n",
       "      <td>0.782955</td>\n",
       "      <td>0.782085</td>\n",
       "      <td>0.780100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.750530</td>\n",
       "      <td>0.750530</td>\n",
       "      <td>0.750375</td>\n",
       "      <td>0.750058</td>\n",
       "      <td>0.748286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.763840</td>\n",
       "      <td>0.763840</td>\n",
       "      <td>0.763656</td>\n",
       "      <td>0.763571</td>\n",
       "      <td>0.762099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.774912</td>\n",
       "      <td>0.774912</td>\n",
       "      <td>0.777552</td>\n",
       "      <td>0.773200</td>\n",
       "      <td>0.770432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.769847</td>\n",
       "      <td>0.769847</td>\n",
       "      <td>0.769864</td>\n",
       "      <td>0.769335</td>\n",
       "      <td>0.767489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boost</td>\n",
       "      <td>0.789399</td>\n",
       "      <td>0.789399</td>\n",
       "      <td>0.791453</td>\n",
       "      <td>0.788104</td>\n",
       "      <td>0.785455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.787986</td>\n",
       "      <td>0.787986</td>\n",
       "      <td>0.790136</td>\n",
       "      <td>0.786639</td>\n",
       "      <td>0.783966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>0.778896</td>\n",
       "      <td>0.777707</td>\n",
       "      <td>0.775578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Accuracy    Recall  Precision  F1 Score   ROC/AUC\n",
       "0        Baseline  0.529682  0.529682   0.280563  0.366825  0.500000\n",
       "1          LogReg  0.782686  0.782686   0.782955  0.782085  0.780100\n",
       "2             KNN  0.750530  0.750530   0.750375  0.750058  0.748286\n",
       "3             SVC  0.763840  0.763840   0.763656  0.763571  0.762099\n",
       "4   Decision Tree  0.774912  0.774912   0.777552  0.773200  0.770432\n",
       "5   Random Forest  0.769847  0.769847   0.769864  0.769335  0.767489\n",
       "6  Gradient Boost  0.789399  0.789399   0.791453  0.788104  0.785455\n",
       "7         XGBoost  0.787986  0.787986   0.790136  0.786639  0.783966\n",
       "8        AdaBoost  0.778445  0.778445   0.778896  0.777707  0.775578"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and display results\n",
    "results = pd.DataFrame(columns=['Model', 'Accuracy', 'Recall', 'Precision', 'F1 Score', 'ROC/AUC'])\n",
    "i = 0\n",
    "for m in bclfmodels.items():\n",
    "    # Building a full pipeline with our preprocessor and a Classifier\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), (m[0], m[1])])\n",
    "    # Making predictions on the training set using cross validation as well as calculating the probabilities\n",
    "    y_train_pred = cross_val_predict(pipe,\n",
    "                                     X_train,\n",
    "                                     y_train.values.ravel(),\n",
    "                                     cv=5,\n",
    "                                     verbose=4,\n",
    "                                     n_jobs=-1)\n",
    "    # Calculating metrices\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            'Model': m[0],\n",
    "            'Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'Recall': recall_score(y_train, y_train_pred, average=\"weighted\"),\n",
    "            'Precision': precision_score(\n",
    "                y_train, y_train_pred, average=\"weighted\"),\n",
    "            'F1 Score': f1_score(y_train, y_train_pred, average=\"weighted\"),\n",
    "            'ROC/AUC': roc_auc_score(y_train, y_train_pred)\n",
    "        },\n",
    "        index=[i])\n",
    "    print(f\"Confusion Matrix {m[0]}: \\n\" +\n",
    "          str(confusion_matrix(y_train, y_train_pred)))\n",
    "    i += 1\n",
    "    results = pd.concat([results, temp])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clf Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create pipeline to use in RandomizedSearchCV and GridSearchCV\n",
    "pipeline_lr_bclf = Pipeline([('preprocessor', preprocessor),\n",
    "                            ('lr_bclf',\n",
    "                             LogisticRegression(penalty='l2',\n",
    "                                                max_iter=100,\n",
    "                                                C=0.9,\n",
    "                                                random_state=random_state,\n",
    "                                                l1_ratio=0.5,\n",
    "                                                n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Pre-Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display possible hyperparameters for LogisticRegression\n",
    "test_lr_bclf = LogisticRegression()\n",
    "test_lr_bclf.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default values for LogisticRegression** (as base for hyperparameter search):\n",
    "\n",
    "penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter distribution\n",
    "param_distribs_lr_bclf = {\n",
    "    'lr_bclf__penalty': ['l1', 'l2'],\n",
    "    'lr_bclf__max_iter': randint(low=10, high=100),\n",
    "    'lr_bclf__C': [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "    'lr_bclf__l1_ratio': [None, 0.1, 0.2, 0.3, 0.5, 0.9, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.5s finished\n",
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  \"(penalty={})\".format(self.penalty))\n"
     ]
    }
   ],
   "source": [
    "# Create and fit RandomizedSearchCV, save \"best_model\"\n",
    "rnd_lr_bclf = RandomizedSearchCV(pipeline_lr_bclf,\n",
    "                                 param_distribs_lr_bclf,\n",
    "                                 cv=5,\n",
    "                                 scoring=scoring,\n",
    "                                 n_iter=50,\n",
    "                                 return_train_score=True,\n",
    "                                 verbose=4,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "best_model_rnd_lr_bclf = rnd_lr_bclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.85\n",
      "Best parameters:\n",
      "{'lr_bclf__C': 20, 'lr_bclf__l1_ratio': 0.1, 'lr_bclf__max_iter': 83, 'lr_bclf__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(rnd_lr_bclf.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(rnd_lr_bclf.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid_lr_bclf = {\n",
    "    'lr_bclf__penalty': ['l1', 'l2'],\n",
    "    'lr_bclf__max_iter': [100, 125, 150, 200],\n",
    "    'lr_bclf__C': [10, 20, 30],\n",
    "    'lr_bclf__l1_ratio': [None, 0.05, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   18.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Create and fit GridSearchCV, save \"best_model\"\n",
    "grid_lr_bclf = GridSearchCV(pipeline_lr_bclf,\n",
    "                            param_grid_lr_bclf,\n",
    "                            cv=5,\n",
    "                            scoring=scoring,\n",
    "                            return_train_score=True,\n",
    "                            verbose=4,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "grid_lr_bclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_lr_bclf = grid_lr_bclf.best_estimator_['lr_bclf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.85\n",
      "Best parameters:\n",
      "{'lr_bclf__C': 20, 'lr_bclf__l1_ratio': None, 'lr_bclf__max_iter': 150, 'lr_bclf__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(grid_lr_bclf.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_lr_bclf.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get and print feature importances\n",
    "#grid_lr_bclf_fi = feat_importances(grid_lr_bclf, cv_model=True, named_steps='lr_bclf', column_names=column_names)\n",
    "#grid_lr_bclf_fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_train_pred_lr_bclf = best_model_lr_bclf.predict(X_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Final evaluation with \"best model\"\n",
    "model_eval(y_train, y_train_pred_lr_bclf, model=\"bclf\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-567-ddc2260eef86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_pred_lr_bclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model_grid_lr_bclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_prep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr_bclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr_bclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr_bclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 91\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy, recall and precision for the test set with the best model\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_train, y_pred_lr_bclf)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_train, y_pred_lr_bclf)))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_train, y_pred_lr_bclf)))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_train, y_pred_lr_bclf)))\n",
    "print(\"ROC/AUC: {:.2f}\".format(roc_auc_score(y_train, y_pred_lr_bclf)))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_train, y_pred_lr_bclf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_test_pred_lr_bclf = best_model_lr_bclf.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.16\n",
      "RMSE: 0.40\n",
      "MAE: 0.31\n",
      "R2: 0.50\n",
      "MAPE: 7.97\n",
      "MAPE median: 6.10\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_test, y_test_pred_lr_bclf, model=\"bclf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!! NN Model 1: TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Evaluation with Testing Set and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bclf_best_model = best_model_grid_lr_bclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best model using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best model at given binary_split\n",
    "bclf_best_model.to_pickle(f\"saves/{dataset_loc}_{dataset_date}/{target}_{binary_split}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: Multi-Class Classification (\"price_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using \u001b[1mPRICE_CLASS\u001b[0m as the target and \u001b[1mf1\u001b[0m for scoring to predict prices for \u001b[1mberlin\u001b[0m on \u001b[1m2020-03-17\u001b[0m\n",
      "\n",
      "The target variable y is set to \u001b[1mPRICE_CLASS\u001b[0m\n",
      "\n",
      "You are currently using these features for its prediction:\n",
      "\u001b[1m['accommodates', 'accommodates_per_bed', 'am_balcony', 'am_breakfast', 'am_child_friendly', 'am_elevator', 'am_essentials', 'am_nature_and_views', 'am_pets_allowed', 'am_private_entrance', 'am_smoking_allowed', 'am_tv', 'am_white_goods', 'availability_90', 'bathrooms_log', 'bedrooms', 'calc_host_lst_count_sqrt_log', 'cancellation_policy', 'first_review_days_sqrt', 'host_acceptance_rate', 'host_is_superhost', 'host_response_rate', 'host_response_time', 'instant_bookable', 'last_review_days_sqrt', 'latitude', 'listing_no', 'longitude', 'maximum_nights', 'minimum_nights_log', 'neighbourhood_cleansed', 'number_of_reviews_ltm_log', 'price_extra_fees_sqrt', 'price_extra_people', 'property_type', 'review_scores_location', 'review_scores_rating_sqrt', 'room_type', 'text_len_sqrt', 'wk_mth_discount', 'zipcode']\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print current setting for TARGET\n",
    "print_target_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Select models for comparison\n",
    "clfmodels = {\n",
    "    'Baseline':\n",
    "    DummyClassifier(strategy='most_frequent'),\n",
    "    'LogReg':\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    'KNN':\n",
    "    KNeighborsClassifier(),\n",
    "    'SVC':\n",
    "    SVC(kernel='rbf', C=1E6),\n",
    "    'Decision Tree':\n",
    "    DecisionTreeClassifier(criterion=\"gini\",\n",
    "                           max_depth=3,\n",
    "                           random_state=random_state),\n",
    "    'Random Forest':\n",
    "    RandomForestClassifier(random_state=random_state,\n",
    "                           max_features='sqrt',\n",
    "                           n_jobs=-1),\n",
    "    'Gradient Boost':\n",
    "    GradientBoostingClassifier(random_state=random_state),\n",
    "    'XGBoost':\n",
    "    XGBClassifier(),\n",
    "    'AdaBoost':\n",
    "    AdaBoostClassifier(random_state=random_state)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.2s finished\n",
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Baseline: \n",
      "[[   0    0  307    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1057    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1353    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1276    0    0    0    0    0    0    0    0]\n",
      " [   0    0  926    0    0    0    0    0    0    0    0]\n",
      " [   0    0  817    0    0    0    0    0    0    0    0]\n",
      " [   0    0  665    0    0    0    0    0    0    0    0]\n",
      " [   0    0  449    0    0    0    0    0    0    0    0]\n",
      " [   0    0  457    0    0    0    0    0    0    0    0]\n",
      " [   0    0  786    0    0    0    0    0    0    0    0]\n",
      " [   0    0  397    0    0    0    0    0    0    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.7s remaining:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix LogReg: \n",
      "[[ 55 125  78  21   4   4   3   0   1  11   5]\n",
      " [ 27 447 404 121  25   9   3   0   0  18   3]\n",
      " [ 12 347 497 301  81  61  22  10   1  18   3]\n",
      " [ 15 167 401 297 150 125  39  12  14  47   9]\n",
      " [  3  54 177 217 167 130  61  21  26  61   9]\n",
      " [  1  30  86 196 129 143  94  23  22  80  13]\n",
      " [  1  15  53 105  99 139  71  17  25 122  18]\n",
      " [  1   8  25  51  63  79  47  21  19 116  19]\n",
      " [  1   5  15  44  44  74  48  20  22 159  25]\n",
      " [  3   6  15  42  43  73  65  32  49 352 106]\n",
      " [  0   1   2   7  17   7   4   8   8 176 167]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.9s remaining:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix KNN: \n",
      "[[ 80  96  73  35   9   4   6   0   1   3   0]\n",
      " [ 83 370 337 158  44  33  12   3   9   7   1]\n",
      " [ 64 390 441 233  71  72  36  13  10  19   4]\n",
      " [ 55 290 331 278 127  88  47  13  13  30   4]\n",
      " [ 48 140 203 178 146  96  39  15  13  44   4]\n",
      " [ 23 124 194 147  91  94  52  21  20  41  10]\n",
      " [ 19  87 147 106  75  78  65  19  14  49   6]\n",
      " [ 12  41  74  75  46  49  29  44  25  50   4]\n",
      " [ 16  50  68  65  44  37  41  18  40  64  14]\n",
      " [ 13  56  85  77  70  59  60  33  58 221  54]\n",
      " [  7  15  31  15  25  12  11  18  22 109 132]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   21.5s remaining:   32.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   31.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix SVC: \n",
      "[[ 88 101  59  33   9   6   5   0   2   3   1]\n",
      " [ 58 428 338 157  34  18  11   3   3   6   1]\n",
      " [ 38 359 446 275  99  65  25  19  10  13   4]\n",
      " [ 25 195 310 356 152 110  47  29  15  31   6]\n",
      " [  9  66 158 199 204 125  72  22  33  30   8]\n",
      " [  7  30  93 145 142 171  90  46  34  54   5]\n",
      " [  5  20  65  88  94 109 120  43  43  66  12]\n",
      " [  0   9  34  56  55  70  45  66  35  73   6]\n",
      " [  2  10  22  43  45  68  50  39  62  96  20]\n",
      " [  0   9  17  52  47  55  74  58  94 284  96]\n",
      " [  0   3   4   8  10  11  16  15  18 124 188]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Decision Tree: \n",
      "[[  8  76 137   0   0  31   0   0   0  51   4]\n",
      " [  2 177 730   0   0 102   0   0   0  45   1]\n",
      " [  2 159 825   0   0 314   0   0   0  53   0]\n",
      " [  4  64 584   0   0 524   0   0   0  97   3]\n",
      " [  1  20 280   0   1 488   0   0   0 129   7]\n",
      " [  3  16 138   0   0 549   0   0   0 101  10]\n",
      " [  4   6 105   0   0 400   0   0   0 136  14]\n",
      " [  1   3  38   0   0 269   0   0   0 123  15]\n",
      " [  4   3  47   0   0 231   0   0   0 149  23]\n",
      " [ 14   3  50   0   0 301   0   0   0 336  82]\n",
      " [  9   3  18   0   0  54   0   0   0 176 137]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   11.9s remaining:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   15.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Random Forest: \n",
      "[[ 65 130  82  17   3   5   2   0   0   2   1]\n",
      " [ 22 435 456  97  17   7   4   0   3  14   2]\n",
      " [  4 350 607 250  45  53  11   5   2  24   2]\n",
      " [  4 159 441 363 119  92  26   2   7  60   3]\n",
      " [  1  52 201 240 201 108  40   6   9  61   7]\n",
      " [  1  30 139 188 126 162  56  13  12  81   9]\n",
      " [  2  21  70 136  92 113  70  14  10 124  13]\n",
      " [  0   3  39  74  55  66  38  44  13 111   6]\n",
      " [  1   6  30  68  54  58  28  14  30 146  22]\n",
      " [  0   4  32  60  35  59  53  16  38 423  66]\n",
      " [  0   1   3  11  13  12   9   3  11 173 161]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.0min remaining:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Gradient Boost: \n",
      "[[ 73 124  80  12   4   4   3   1   0   5   1]\n",
      " [ 39 448 412  96  12  18   5   6   5  14   2]\n",
      " [ 18 329 576 254  39  83  23   2   5  21   3]\n",
      " [ 12 132 438 365  88 121  34  15  16  50   5]\n",
      " [  5  52 195 239 120 131  57  25  25  67  10]\n",
      " [  0  30 101 198  92 177  69  21  30  84  15]\n",
      " [  3  14  68 126  76 127  69  24  36  99  23]\n",
      " [  0   5  29  71  47  80  38  32  25 107  15]\n",
      " [  1   7  17  58  38  64  30  20  29 164  29]\n",
      " [  2   4  14  55  53  67  49  20  45 384  93]\n",
      " [  0   1   4   9  10  11   4   7   5 160 186]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   35.8s remaining:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   52.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix XGBoost: \n",
      "[[ 66 134  76  14   3   6   0   0   0   6   2]\n",
      " [ 31 444 458  62  16  17   3   2   0  21   3]\n",
      " [  9 361 611 222  28  80   9   3   2  25   3]\n",
      " [  7 144 476 356  78 111  20   5   4  71   4]\n",
      " [  3  55 223 255 104 152  27   8  13  80   6]\n",
      " [  0  24 117 214  78 208  36  10  20  98  12]\n",
      " [  0  20  68 156  57 139  51  11  19 125  19]\n",
      " [  0   7  25  83  34  86  23  22  11 142  16]\n",
      " [  1   9  23  63  26  75  18   8  16 189  29]\n",
      " [  0   4  17  72  40  72  29   8  20 421 103]\n",
      " [  0   1   4  11   7  10   4   3   5 157 195]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.0s remaining:    3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix AdaBoost: \n",
      "[[ 55  99 103  22   2   4   1   0   0  14   7]\n",
      " [130 314 443 108  11  14   1   1   0  25  10]\n",
      " [ 98 258 571 249  37  65  18   3   4  34  16]\n",
      " [ 55 139 406 313  78 138  24   8  11  91  13]\n",
      " [ 26  63 182 250  85 155  20   5   6 107  27]\n",
      " [ 15  33 112 214  64 179  37  10  15 108  30]\n",
      " [  6  23  80 137  37 123  35   6   5 170  43]\n",
      " [  3  10  31  74  32  97  26   6   9 136  25]\n",
      " [  5  15  25  54  27  74  19   3   9 180  46]\n",
      " [  7  30  13  57  25 100  30  10  14 348 152]\n",
      " [  0   5   8  10   3  12  10   2   7 171 169]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.159364</td>\n",
       "      <td>0.159364</td>\n",
       "      <td>0.025397</td>\n",
       "      <td>0.043812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.263722</td>\n",
       "      <td>0.263722</td>\n",
       "      <td>0.252461</td>\n",
       "      <td>0.251175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.225088</td>\n",
       "      <td>0.225088</td>\n",
       "      <td>0.229258</td>\n",
       "      <td>0.218664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.284217</td>\n",
       "      <td>0.284217</td>\n",
       "      <td>0.282050</td>\n",
       "      <td>0.281744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.239458</td>\n",
       "      <td>0.239458</td>\n",
       "      <td>0.260865</td>\n",
       "      <td>0.163596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.301649</td>\n",
       "      <td>0.301649</td>\n",
       "      <td>0.307399</td>\n",
       "      <td>0.286645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boost</td>\n",
       "      <td>0.289635</td>\n",
       "      <td>0.289635</td>\n",
       "      <td>0.276851</td>\n",
       "      <td>0.274505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.293757</td>\n",
       "      <td>0.293757</td>\n",
       "      <td>0.286248</td>\n",
       "      <td>0.270193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.245465</td>\n",
       "      <td>0.245465</td>\n",
       "      <td>0.225560</td>\n",
       "      <td>0.221782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Accuracy    Recall  Precision  F1 Score\n",
       "0        Baseline  0.159364  0.159364   0.025397  0.043812\n",
       "1          LogReg  0.263722  0.263722   0.252461  0.251175\n",
       "2             KNN  0.225088  0.225088   0.229258  0.218664\n",
       "3             SVC  0.284217  0.284217   0.282050  0.281744\n",
       "4   Decision Tree  0.239458  0.239458   0.260865  0.163596\n",
       "5   Random Forest  0.301649  0.301649   0.307399  0.286645\n",
       "6  Gradient Boost  0.289635  0.289635   0.276851  0.274505\n",
       "7         XGBoost  0.293757  0.293757   0.286248  0.270193\n",
       "8        AdaBoost  0.245465  0.245465   0.225560  0.221782"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and display results\n",
    "results = pd.DataFrame(columns=['Model', 'Accuracy', 'Recall', 'Precision', 'F1 Score'])\n",
    "i = 0\n",
    "for m in clfmodels.items():\n",
    "    # Building a full pipeline with our preprocessor and a Classifier\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), (m[0], m[1])])\n",
    "    # Making predictions on the training set using cross validation as well as calculating the probabilities\n",
    "    y_train_pred = cross_val_predict(pipe,\n",
    "                                     X_train,\n",
    "                                     y_train.values.ravel(),\n",
    "                                     cv=5,\n",
    "                                     verbose=4,\n",
    "                                     n_jobs=-1)\n",
    "    # Calculating metrices\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            'Model': m[0],\n",
    "            'Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'Recall': recall_score(y_train, y_train_pred, average=\"weighted\"),\n",
    "            'Precision': precision_score(\n",
    "                y_train, y_train_pred, average=\"weighted\"),\n",
    "            'F1 Score': f1_score(y_train, y_train_pred, average=\"weighted\"),\n",
    "        },\n",
    "        index=[i])\n",
    "    print(f\"Confusion Matrix {m[0]}: \\n\" +\n",
    "          str(confusion_matrix(y_train, y_train_pred)))\n",
    "    i += 1\n",
    "    results = pd.concat([results, temp])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.24546525323910484\n",
      "Recall:  0.24546525323910484\n",
      "Precision:  0.22555966733807414\n",
      "F1 Score:  0.22178198870408516\n"
     ]
    }
   ],
   "source": [
    "# Apply LogReg with OVR\n",
    "lr_ovr_model = LogisticRegression(multi_class='ovr', max_iter=200)\n",
    "# fit model\n",
    "lr_ovr_model.fit(X_train_prep, y_train)\n",
    "# make predictions\n",
    "y_pred_lr_ovr = lr_ovr_model.predict(X_train_prep)\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_train, y_train_pred))\n",
    "print('Recall: ', recall_score(y_train, y_train_pred, average=\"weighted\"))\n",
    "print('Precision: ', precision_score(y_train, y_train_pred, average=\"weighted\"))\n",
    "print('F1 Score: ', f1_score(y_train, y_train_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clf Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create pipeline to use in RandomizedSearchCV and GridSearchCV\n",
    "pipeline_lr_clf = Pipeline([('preprocessor', preprocessor),\n",
    "                            ('lr_clf',\n",
    "                             LogisticRegression(penalty='l2',\n",
    "                                                max_iter=100,\n",
    "                                                C=0.9,\n",
    "#                                                multi_class='multinomial',\n",
    "                                                random_state=random_state,\n",
    "                                                l1_ratio=0.5,\n",
    "                                                n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Pre-Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display possible hyperparameters for LogisticRegression\n",
    "test_lr_clf = LogisticRegression()\n",
    "test_lr_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default values for LogisticRegression** (as base for hyperparameter search):\n",
    "\n",
    "penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter distribution\n",
    "param_distribs_lr_clf = {\n",
    "    'lr_clf__penalty': ['l1', 'l2'],\n",
    "    'lr_clf__max_iter': randint(low=10, high=100),\n",
    "    'lr_clf__C': [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "    'lr_clf__l1_ratio': [None, 0.1, 0.2, 0.3, 0.5, 0.9, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:   35.5s finished\n",
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  \"(penalty={})\".format(self.penalty))\n"
     ]
    }
   ],
   "source": [
    "# Create and fit RandomizedSearchCV, save \"best_model\"\n",
    "rnd_lr_clf = RandomizedSearchCV(pipeline_lr_clf,\n",
    "                                 param_distribs_lr_clf,\n",
    "                                 cv=5,\n",
    "                                 scoring=scoring,\n",
    "                                 n_iter=50,\n",
    "                                 return_train_score=True,\n",
    "                                 verbose=4,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "best_model_rnd_lr_clf = rnd_lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.25\n",
      "Best parameters:\n",
      "{'lr_clf__C': 1, 'lr_clf__l1_ratio': 0.9, 'lr_clf__max_iter': 96, 'lr_clf__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(rnd_lr_clf.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(rnd_lr_clf.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid_lr_clf = {\n",
    "#    'lr_clf__penalty': ['l1', 'l2'],\n",
    "    'lr_clf__max_iter': [85, 100, 125],\n",
    "    'lr_clf__C': [0.1, 0.5, 1, 2],\n",
    "    'lr_clf__l1_ratio': [0.8, 0.9, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  1.2min finished\n",
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  \"(penalty={})\".format(self.penalty))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         Pipeline(steps=[('imputer_num',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('std_scaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['accommodates',\n",
       "                                                                          'accommodates_per_bed',\n",
       "                                                                          'am_balcony',\n",
       "                                                                          'am_breakfast',\n",
       "                                                                          'am_child_friendly',\n",
       "                                                                          'am_elevator',\n",
       "                                                                          'am_essentials',\n",
       "                                                                          'am_nature_and_views',\n",
       "                                                                          'am_pets_allo...\n",
       "                                                                         ['cancellation_policy',\n",
       "                                                                          'host_response_time',\n",
       "                                                                          'neighbourhood_cleansed',\n",
       "                                                                          'property_type',\n",
       "                                                                          'room_type',\n",
       "                                                                          'zipcode'])])),\n",
       "                                       ('lr_clf',\n",
       "                                        LogisticRegression(C=0.9, l1_ratio=0.5,\n",
       "                                                           n_jobs=-1,\n",
       "                                                           random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'lr_clf__C': [0.1, 0.5, 1, 2],\n",
       "                         'lr_clf__l1_ratio': [0.8, 0.9, 1],\n",
       "                         'lr_clf__max_iter': [85, 100, 125]},\n",
       "             return_train_score=True, scoring='f1_weighted', verbose=4)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit GridSearchCV, save \"best_model\"\n",
    "grid_lr_clf = GridSearchCV(pipeline_lr_clf,\n",
    "                            param_grid_lr_clf,\n",
    "                            cv=5,\n",
    "                            scoring=scoring,\n",
    "                            return_train_score=True,\n",
    "                            verbose=4,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "grid_lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_lr_clf = grid_lr_clf.best_estimator_['lr_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.25\n",
      "Best parameters:\n",
      "{'lr_clf__C': 1, 'lr_clf__l1_ratio': 0.8, 'lr_clf__max_iter': 100}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(grid_lr_clf.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_lr_clf.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_train_pred_lr_clf = best_model_lr_clf.predict(X_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_train, y_train_pred_lr_clf, model=\"clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35\n",
      "Recall: 0.35\n",
      "Precision: 0.35\n",
      "F1 Score: 0.34\n",
      "Confusion Matrix: \n",
      "[[ 74 120  73  19   4   3   3   0   0   7   4]\n",
      " [ 20 543 350  94  20   7   3   1   1  18   0]\n",
      " [ 12 299 648 225  63  60  19   5   1  18   3]\n",
      " [ 11 144 369 412 112 119  32  11   9  48   9]\n",
      " [  6  45 178 211 209 127  52  17  13  62   6]\n",
      " [  0  27  75 182 112 228  67  20  15  82   9]\n",
      " [  1  10  58  97  84 125 126  15  21 111  17]\n",
      " [  0   7  23  51  54  70  48  42  18 118  18]\n",
      " [  1   8  15  47  42  64  39  13  56 149  23]\n",
      " [  2   2  15  35  38  68  49  23  33 450  71]\n",
      " [  0   1   3   6  14   9   8   6   7 139 204]]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy, recall and precision for the test set with the best model\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_train, y_pred_lr_clf)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_train, y_pred_lr_clf, average='weighted')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_train, y_pred_lr_clf, average='weighted')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_train, y_pred_lr_clf, average='weighted')))\n",
    "#print(\"ROC/AUC: {:.2f}\".format(roc_auc_score(y_train, y_pred_lr_clf, multi_class='ovr')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_train, y_pred_lr_clf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_test_pred_lr_clf = best_model_lr_clf.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.16\n",
      "RMSE: 0.40\n",
      "MAE: 0.31\n",
      "R2: 0.50\n",
      "MAPE: 7.97\n",
      "MAPE median: 6.10\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_test, y_test_pred_lr_clf, model=\"clf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clf Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create pipeline to use in RandomizedSearchCV and GridSearchCV\n",
    "pipeline_rf_clf = Pipeline([('preprocessor', preprocessor),\n",
    "                            ('rf_clf',\n",
    "                             RandomForestClassifier(n_estimators=110,\n",
    "                                                    random_state=random_state,\n",
    "                                                    max_depth=5,\n",
    "                                                    max_features=20,\n",
    "                                                    n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Pre-Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display possible hyperparameters for RandomForestClassifier\n",
    "test_rf_clf = RandomForestClassifier()\n",
    "test_rf_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default values for LogisticRegression** (as base for hyperparameter search):\n",
    "\n",
    "penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter distribution\n",
    "param_distribs_rf_clf = {\n",
    "    'rf_clf__n_estimators': randint(low=10, high=500),\n",
    "    'rf_clf__max_depth': randint(low=1, high=30),\n",
    "    'rf_clf__max_features': randint(low=1, high=100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  9.0min finished\n"
     ]
    }
   ],
   "source": [
    "# Create and fit RandomizedSearchCV, save \"best_model\"\n",
    "rnd_rf_clf = RandomizedSearchCV(pipeline_rf_clf,\n",
    "                           param_distribs_rf_clf,\n",
    "                           cv=5,\n",
    "                           scoring=scoring,\n",
    "                           return_train_score=True,\n",
    "                           verbose=4,\n",
    "                           n_jobs=-1,\n",
    "                         random_state=random_state)\n",
    "\n",
    "rnd_rf_clf.fit(X_train, y_train)\n",
    "best_model_rnd_rf_clf = rnd_rf_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.30\n",
      "Best parameters:\n",
      "{'rf_clf__max_depth': 21, 'rf_clf__max_features': 33, 'rf_clf__n_estimators': 469}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(rnd_rf_clf.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(rnd_rf_clf.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid_rf_clf = {\n",
    "    'rf_clf__n_estimators': [450, 500],\n",
    "    'rf_clf__max_depth': [20, 25],\n",
    "    'rf_clf__max_features': [30, 35],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed: 10.1min finished\n"
     ]
    }
   ],
   "source": [
    "# Create and fit GridSearchCV, save \"best_model\"\n",
    "grid_rf_clf = GridSearchCV(pipeline_rf_clf,\n",
    "                            param_grid_rf_clf,\n",
    "                            cv=4,\n",
    "                            scoring=scoring,\n",
    "                            return_train_score=True,\n",
    "                            verbose=4,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "grid_rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_rf_clf = grid_rf_clf.best_estimator_['rf_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.30\n",
      "Best parameters:\n",
      "{'rf_clf__max_depth': 25, 'rf_clf__max_features': 35, 'rf_clf__n_estimators': 450}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(grid_rf_clf.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_rf_clf.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>0.053475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>0.052189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len_sqrt</th>\n",
       "      <td>0.049861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_no</th>\n",
       "      <td>0.046906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_review_days_sqrt</th>\n",
       "      <td>0.045590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_review_days_sqrt</th>\n",
       "      <td>0.045221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>availability_90</th>\n",
       "      <td>0.043774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_extra_fees_sqrt</th>\n",
       "      <td>0.041142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_reviews_ltm_log</th>\n",
       "      <td>0.039622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <td>0.034078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_rating_sqrt</th>\n",
       "      <td>0.032940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_extra_people</th>\n",
       "      <td>0.031769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Private room</th>\n",
       "      <td>0.031012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maximum_nights</th>\n",
       "      <td>0.028774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimum_nights_log</th>\n",
       "      <td>0.028319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates</th>\n",
       "      <td>0.028069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calc_host_lst_count_sqrt_log</th>\n",
       "      <td>0.020518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates_per_bed</th>\n",
       "      <td>0.017959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>0.017844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_rate</th>\n",
       "      <td>0.016925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms_log</th>\n",
       "      <td>0.010814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_location</th>\n",
       "      <td>0.009840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_tv</th>\n",
       "      <td>0.009255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_strict</th>\n",
       "      <td>0.008948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_balcony</th>\n",
       "      <td>0.008566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_elevator</th>\n",
       "      <td>0.008566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_moderate</th>\n",
       "      <td>0.008442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instant_bookable</th>\n",
       "      <td>0.008298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_is_superhost</th>\n",
       "      <td>0.007610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wk_mth_discount</th>\n",
       "      <td>0.007536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_time_within an hour</th>\n",
       "      <td>0.007456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_child_friendly</th>\n",
       "      <td>0.007280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_private_entrance</th>\n",
       "      <td>0.007224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_white_goods</th>\n",
       "      <td>0.006866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_pets_allowed</th>\n",
       "      <td>0.006845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_time_within a few hours</th>\n",
       "      <td>0.006434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_smoking_allowed</th>\n",
       "      <td>0.005714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_time_within a day</th>\n",
       "      <td>0.005443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_nb_other</th>\n",
       "      <td>0.004374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Alexanderplatz</th>\n",
       "      <td>0.004197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_other</th>\n",
       "      <td>0.004160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_time_unknown</th>\n",
       "      <td>0.004015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Boutique hotel</th>\n",
       "      <td>0.003912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Brunnenstr. Süd</th>\n",
       "      <td>0.003231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_breakfast</th>\n",
       "      <td>0.003216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Frankfurter Allee Süd FK</th>\n",
       "      <td>0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Tempelhofer Vorstadt</th>\n",
       "      <td>0.002710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10119</th>\n",
       "      <td>0.002596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10245</th>\n",
       "      <td>0.002543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_essentials</th>\n",
       "      <td>0.002475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_nature_and_views</th>\n",
       "      <td>0.002362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Prenzlauer Berg Südwest</th>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Reuterstraße</th>\n",
       "      <td>0.002128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Shared room</th>\n",
       "      <td>0.002116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10405</th>\n",
       "      <td>0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10247</th>\n",
       "      <td>0.002015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Prenzlauer Berg Süd</th>\n",
       "      <td>0.001905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_südliche Luisenstadt</th>\n",
       "      <td>0.001856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10999</th>\n",
       "      <td>0.001827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10439</th>\n",
       "      <td>0.001758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10437</th>\n",
       "      <td>0.001726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Prenzlauer Berg Nordwest</th>\n",
       "      <td>0.001707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_House</th>\n",
       "      <td>0.001691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Frankfurter Allee Nord</th>\n",
       "      <td>0.001615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Rixdorf</th>\n",
       "      <td>0.001610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Schöneberg-Nord</th>\n",
       "      <td>0.001566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10117</th>\n",
       "      <td>0.001520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Neuköllner Mitte/Zentrum</th>\n",
       "      <td>0.001512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10997</th>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10435</th>\n",
       "      <td>0.001472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Schillerpromenade</th>\n",
       "      <td>0.001430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10243</th>\n",
       "      <td>0.001430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10179</th>\n",
       "      <td>0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10249</th>\n",
       "      <td>0.001328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Prenzlauer Berg Nord</th>\n",
       "      <td>0.001321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_nördliche Luisenstadt</th>\n",
       "      <td>0.001290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10967</th>\n",
       "      <td>0.001282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Moabit West</th>\n",
       "      <td>0.001275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Helmholtzplatz</th>\n",
       "      <td>0.001251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_nan</th>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10407</th>\n",
       "      <td>0.001238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12047</th>\n",
       "      <td>0.001230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10178</th>\n",
       "      <td>0.001208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Schöneberg-Süd</th>\n",
       "      <td>0.001189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12049</th>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Wedding Zentrum</th>\n",
       "      <td>0.001136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Moabit Ost</th>\n",
       "      <td>0.001133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10965</th>\n",
       "      <td>0.001128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Südliche Friedrichstadt</th>\n",
       "      <td>0.001111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12051</th>\n",
       "      <td>0.001110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12045</th>\n",
       "      <td>0.001086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10961</th>\n",
       "      <td>0.001076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Regierungsviertel</th>\n",
       "      <td>0.001068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Karl-Marx-Allee-Süd</th>\n",
       "      <td>0.001022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Osloer Straße</th>\n",
       "      <td>0.000965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_super_strict</th>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Karl-Marx-Allee-Nord</th>\n",
       "      <td>0.000943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Tiergarten Süd</th>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13353</th>\n",
       "      <td>0.000923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Hotel room</th>\n",
       "      <td>0.000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10777</th>\n",
       "      <td>0.000893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13359</th>\n",
       "      <td>0.000887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12053</th>\n",
       "      <td>0.000870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12059</th>\n",
       "      <td>0.000853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Düsseldorfer Straße</th>\n",
       "      <td>0.000844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10785</th>\n",
       "      <td>0.000843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12043</th>\n",
       "      <td>0.000819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10963</th>\n",
       "      <td>0.000807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Prenzlauer Berg Ost</th>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Parkviertel</th>\n",
       "      <td>0.000767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12055</th>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Otto-Suhr-Allee</th>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Friedenau</th>\n",
       "      <td>0.000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10969</th>\n",
       "      <td>0.000710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Brunnenstr. Nord</th>\n",
       "      <td>0.000703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Tempelhof</th>\n",
       "      <td>0.000697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Kurfürstendamm</th>\n",
       "      <td>0.000691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Volkspark Wilmersdorf</th>\n",
       "      <td>0.000682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12435</th>\n",
       "      <td>0.000642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13357</th>\n",
       "      <td>0.000626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10559</th>\n",
       "      <td>0.000603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Weißensee</th>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10317</th>\n",
       "      <td>0.000592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13347</th>\n",
       "      <td>0.000590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Secondary unit</th>\n",
       "      <td>0.000588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Neu Lichtenberg</th>\n",
       "      <td>0.000570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10829</th>\n",
       "      <td>0.000570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10551</th>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Alt  Treptow</th>\n",
       "      <td>0.000539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Kantstraße</th>\n",
       "      <td>0.000520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Pankow Süd</th>\n",
       "      <td>0.000519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Alt-Lichtenberg</th>\n",
       "      <td>0.000509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10719</th>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10827</th>\n",
       "      <td>0.000498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13187</th>\n",
       "      <td>0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10555</th>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10409</th>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Pankow Zentrum</th>\n",
       "      <td>0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Neue Kantstraße</th>\n",
       "      <td>0.000444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13189</th>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10557</th>\n",
       "      <td>0.000415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10585</th>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10781</th>\n",
       "      <td>0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13086</th>\n",
       "      <td>0.000408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Schloß Charlottenburg</th>\n",
       "      <td>0.000401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Westend</th>\n",
       "      <td>0.000396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10365</th>\n",
       "      <td>0.000395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Britz</th>\n",
       "      <td>0.000377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10553</th>\n",
       "      <td>0.000372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13355</th>\n",
       "      <td>0.000368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14057</th>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13088</th>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Mierendorffplatz</th>\n",
       "      <td>0.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14059</th>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10707</th>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10787</th>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10715</th>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12157</th>\n",
       "      <td>0.000318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10711</th>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10717</th>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10589</th>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10783</th>\n",
       "      <td>0.000307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10623</th>\n",
       "      <td>0.000304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Halensee</th>\n",
       "      <td>0.000304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10629</th>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Grunewald</th>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Zehlendorf  Nord</th>\n",
       "      <td>0.000287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12161</th>\n",
       "      <td>0.000287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12437</th>\n",
       "      <td>0.000281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13351</th>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10625</th>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Drakestr.</th>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12099</th>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Ost 2</th>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12163</th>\n",
       "      <td>0.000257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10627</th>\n",
       "      <td>0.000252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12347</th>\n",
       "      <td>0.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13407</th>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12101</th>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10587</th>\n",
       "      <td>0.000209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Plänterwald</th>\n",
       "      <td>0.000208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10315</th>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Baumschulenweg</th>\n",
       "      <td>0.000204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13156</th>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10318</th>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Karlshorst</th>\n",
       "      <td>0.000199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10367</th>\n",
       "      <td>0.000199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13409</th>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10823</th>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12103</th>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Bed and breakfast</th>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Blankenfelde/Niederschönhausen</th>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14197</th>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13349</th>\n",
       "      <td>0.000169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Unique space</th>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Barstraße</th>\n",
       "      <td>0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10713</th>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      weight\n",
       "latitude                                            0.053475\n",
       "longitude                                           0.052189\n",
       "text_len_sqrt                                       0.049861\n",
       "listing_no                                          0.046906\n",
       "first_review_days_sqrt                              0.045590\n",
       "last_review_days_sqrt                               0.045221\n",
       "availability_90                                     0.043774\n",
       "price_extra_fees_sqrt                               0.041142\n",
       "number_of_reviews_ltm_log                           0.039622\n",
       "host_acceptance_rate                                0.034078\n",
       "review_scores_rating_sqrt                           0.032940\n",
       "price_extra_people                                  0.031769\n",
       "room_type_Private room                              0.031012\n",
       "maximum_nights                                      0.028774\n",
       "minimum_nights_log                                  0.028319\n",
       "accommodates                                        0.028069\n",
       "calc_host_lst_count_sqrt_log                        0.020518\n",
       "accommodates_per_bed                                0.017959\n",
       "bedrooms                                            0.017844\n",
       "host_response_rate                                  0.016925\n",
       "bathrooms_log                                       0.010814\n",
       "review_scores_location                              0.009840\n",
       "am_tv                                               0.009255\n",
       "cancellation_policy_strict                          0.008948\n",
       "am_balcony                                          0.008566\n",
       "am_elevator                                         0.008566\n",
       "cancellation_policy_moderate                        0.008442\n",
       "instant_bookable                                    0.008298\n",
       "host_is_superhost                                   0.007610\n",
       "wk_mth_discount                                     0.007536\n",
       "host_response_time_within an hour                   0.007456\n",
       "am_child_friendly                                   0.007280\n",
       "am_private_entrance                                 0.007224\n",
       "am_white_goods                                      0.006866\n",
       "am_pets_allowed                                     0.006845\n",
       "host_response_time_within a few hours               0.006434\n",
       "am_smoking_allowed                                  0.005714\n",
       "host_response_time_within a day                     0.005443\n",
       "neighbourhood_cleansed_nb_other                     0.004374\n",
       "neighbourhood_cleansed_Alexanderplatz               0.004197\n",
       "zipcode_zip_other                                   0.004160\n",
       "host_response_time_unknown                          0.004015\n",
       "property_type_Boutique hotel                        0.003912\n",
       "neighbourhood_cleansed_Brunnenstr. Süd              0.003231\n",
       "am_breakfast                                        0.003216\n",
       "neighbourhood_cleansed_Frankfurter Allee Süd FK     0.002968\n",
       "neighbourhood_cleansed_Tempelhofer Vorstadt         0.002710\n",
       "zipcode_zip_10119                                   0.002596\n",
       "zipcode_zip_10245                                   0.002543\n",
       "am_essentials                                       0.002475\n",
       "am_nature_and_views                                 0.002362\n",
       "neighbourhood_cleansed_Prenzlauer Berg Südwest      0.002200\n",
       "neighbourhood_cleansed_Reuterstraße                 0.002128\n",
       "room_type_Shared room                               0.002116\n",
       "zipcode_zip_10405                                   0.002054\n",
       "zipcode_zip_10247                                   0.002015\n",
       "neighbourhood_cleansed_Prenzlauer Berg Süd          0.001905\n",
       "neighbourhood_cleansed_südliche Luisenstadt         0.001856\n",
       "zipcode_zip_10999                                   0.001827\n",
       "zipcode_zip_10439                                   0.001758\n",
       "zipcode_zip_10437                                   0.001726\n",
       "neighbourhood_cleansed_Prenzlauer Berg Nordwest     0.001707\n",
       "property_type_House                                 0.001691\n",
       "neighbourhood_cleansed_Frankfurter Allee Nord       0.001615\n",
       "neighbourhood_cleansed_Rixdorf                      0.001610\n",
       "neighbourhood_cleansed_Schöneberg-Nord              0.001566\n",
       "zipcode_zip_10117                                   0.001520\n",
       "neighbourhood_cleansed_Neuköllner Mitte/Zentrum     0.001512\n",
       "zipcode_zip_10997                                   0.001478\n",
       "zipcode_zip_10435                                   0.001472\n",
       "neighbourhood_cleansed_Schillerpromenade            0.001430\n",
       "zipcode_zip_10243                                   0.001430\n",
       "zipcode_zip_10179                                   0.001409\n",
       "zipcode_zip_10249                                   0.001328\n",
       "neighbourhood_cleansed_Prenzlauer Berg Nord         0.001321\n",
       "neighbourhood_cleansed_nördliche Luisenstadt        0.001290\n",
       "zipcode_zip_10967                                   0.001282\n",
       "neighbourhood_cleansed_Moabit West                  0.001275\n",
       "neighbourhood_cleansed_Helmholtzplatz               0.001251\n",
       "zipcode_zip_nan                                     0.001244\n",
       "zipcode_zip_10407                                   0.001238\n",
       "zipcode_zip_12047                                   0.001230\n",
       "zipcode_zip_10178                                   0.001208\n",
       "neighbourhood_cleansed_Schöneberg-Süd               0.001189\n",
       "zipcode_zip_12049                                   0.001149\n",
       "neighbourhood_cleansed_Wedding Zentrum              0.001136\n",
       "neighbourhood_cleansed_Moabit Ost                   0.001133\n",
       "zipcode_zip_10965                                   0.001128\n",
       "neighbourhood_cleansed_Südliche Friedrichstadt      0.001111\n",
       "zipcode_zip_12051                                   0.001110\n",
       "zipcode_zip_12045                                   0.001086\n",
       "zipcode_zip_10961                                   0.001076\n",
       "neighbourhood_cleansed_Regierungsviertel            0.001068\n",
       "neighbourhood_cleansed_Karl-Marx-Allee-Süd          0.001022\n",
       "neighbourhood_cleansed_Osloer Straße                0.000965\n",
       "cancellation_policy_super_strict                    0.000956\n",
       "neighbourhood_cleansed_Karl-Marx-Allee-Nord         0.000943\n",
       "neighbourhood_cleansed_Tiergarten Süd               0.000930\n",
       "zipcode_zip_13353                                   0.000923\n",
       "room_type_Hotel room                                0.000914\n",
       "zipcode_zip_10777                                   0.000893\n",
       "zipcode_zip_13359                                   0.000887\n",
       "zipcode_zip_12053                                   0.000870\n",
       "zipcode_zip_12059                                   0.000853\n",
       "neighbourhood_cleansed_Düsseldorfer Straße          0.000844\n",
       "zipcode_zip_10785                                   0.000843\n",
       "zipcode_zip_12043                                   0.000819\n",
       "zipcode_zip_10963                                   0.000807\n",
       "neighbourhood_cleansed_Prenzlauer Berg Ost          0.000798\n",
       "neighbourhood_cleansed_Parkviertel                  0.000767\n",
       "zipcode_zip_12055                                   0.000745\n",
       "neighbourhood_cleansed_Otto-Suhr-Allee              0.000730\n",
       "neighbourhood_cleansed_Friedenau                    0.000724\n",
       "zipcode_zip_10969                                   0.000710\n",
       "neighbourhood_cleansed_Brunnenstr. Nord             0.000703\n",
       "neighbourhood_cleansed_Tempelhof                    0.000697\n",
       "neighbourhood_cleansed_Kurfürstendamm               0.000691\n",
       "neighbourhood_cleansed_Volkspark Wilmersdorf        0.000682\n",
       "zipcode_zip_12435                                   0.000642\n",
       "zipcode_zip_13357                                   0.000626\n",
       "zipcode_zip_10559                                   0.000603\n",
       "neighbourhood_cleansed_Weißensee                    0.000600\n",
       "zipcode_zip_10317                                   0.000592\n",
       "zipcode_zip_13347                                   0.000590\n",
       "property_type_Secondary unit                        0.000588\n",
       "neighbourhood_cleansed_Neu Lichtenberg              0.000570\n",
       "zipcode_zip_10829                                   0.000570\n",
       "zipcode_zip_10551                                   0.000560\n",
       "neighbourhood_cleansed_Alt  Treptow                 0.000539\n",
       "neighbourhood_cleansed_Kantstraße                   0.000520\n",
       "neighbourhood_cleansed_Pankow Süd                   0.000519\n",
       "neighbourhood_cleansed_Alt-Lichtenberg              0.000509\n",
       "zipcode_zip_10719                                   0.000501\n",
       "zipcode_zip_10827                                   0.000498\n",
       "zipcode_zip_13187                                   0.000473\n",
       "zipcode_zip_10555                                   0.000469\n",
       "zipcode_zip_10409                                   0.000453\n",
       "neighbourhood_cleansed_Pankow Zentrum               0.000445\n",
       "neighbourhood_cleansed_Neue Kantstraße              0.000444\n",
       "zipcode_zip_13189                                   0.000441\n",
       "zipcode_zip_10557                                   0.000415\n",
       "zipcode_zip_10585                                   0.000414\n",
       "zipcode_zip_10781                                   0.000410\n",
       "zipcode_zip_13086                                   0.000408\n",
       "neighbourhood_cleansed_Schloß Charlottenburg        0.000401\n",
       "neighbourhood_cleansed_Westend                      0.000396\n",
       "zipcode_zip_10365                                   0.000395\n",
       "neighbourhood_cleansed_Britz                        0.000377\n",
       "zipcode_zip_10553                                   0.000372\n",
       "zipcode_zip_13355                                   0.000368\n",
       "zipcode_zip_14057                                   0.000356\n",
       "zipcode_zip_13088                                   0.000349\n",
       "neighbourhood_cleansed_Mierendorffplatz             0.000348\n",
       "zipcode_zip_14059                                   0.000337\n",
       "zipcode_zip_10707                                   0.000332\n",
       "zipcode_zip_10787                                   0.000332\n",
       "zipcode_zip_10715                                   0.000326\n",
       "zipcode_zip_12157                                   0.000318\n",
       "zipcode_zip_10711                                   0.000314\n",
       "zipcode_zip_10717                                   0.000314\n",
       "zipcode_zip_10589                                   0.000312\n",
       "zipcode_zip_10783                                   0.000307\n",
       "zipcode_zip_10623                                   0.000304\n",
       "neighbourhood_cleansed_Halensee                     0.000304\n",
       "zipcode_zip_10629                                   0.000303\n",
       "neighbourhood_cleansed_Grunewald                    0.000299\n",
       "neighbourhood_cleansed_Zehlendorf  Nord             0.000287\n",
       "zipcode_zip_12161                                   0.000287\n",
       "zipcode_zip_12437                                   0.000281\n",
       "zipcode_zip_13351                                   0.000270\n",
       "zipcode_zip_10625                                   0.000268\n",
       "neighbourhood_cleansed_Drakestr.                    0.000267\n",
       "zipcode_zip_12099                                   0.000262\n",
       "neighbourhood_cleansed_Ost 2                        0.000260\n",
       "zipcode_zip_12163                                   0.000257\n",
       "zipcode_zip_10627                                   0.000252\n",
       "zipcode_zip_12347                                   0.000249\n",
       "zipcode_zip_13407                                   0.000244\n",
       "zipcode_zip_12101                                   0.000233\n",
       "zipcode_zip_10587                                   0.000209\n",
       "neighbourhood_cleansed_Plänterwald                  0.000208\n",
       "zipcode_zip_10315                                   0.000206\n",
       "neighbourhood_cleansed_Baumschulenweg               0.000204\n",
       "zipcode_zip_13156                                   0.000202\n",
       "zipcode_zip_10318                                   0.000202\n",
       "neighbourhood_cleansed_Karlshorst                   0.000199\n",
       "zipcode_zip_10367                                   0.000199\n",
       "zipcode_zip_13409                                   0.000197\n",
       "zipcode_zip_10823                                   0.000195\n",
       "zipcode_zip_12103                                   0.000194\n",
       "property_type_Bed and breakfast                     0.000193\n",
       "neighbourhood_cleansed_Blankenfelde/Niederschön...  0.000175\n",
       "zipcode_zip_14197                                   0.000174\n",
       "zipcode_zip_13349                                   0.000169\n",
       "property_type_Unique space                          0.000107\n",
       "neighbourhood_cleansed_Barstraße                    0.000106\n",
       "zipcode_zip_10713                                   0.000099"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get and print feature importances\n",
    "grid_rf_clf_fi = feat_importances(grid_rf_clf, cv_model=True, named_steps='rf_clf', column_names=column_names)\n",
    "grid_rf_clf_fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_train_pred_rf_clf = best_model_rf_clf.predict(X_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Recall: 1.00\n",
      "Precision: 1.00\n",
      "F1 Score: 1.00\n",
      "Confusion Matrix: \n",
      "[[ 307    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0 1057    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1353    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0 1276    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0  926    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0  817    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0  665    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0  449    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  457    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  786    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0  397]]\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation with \"best model\"\n",
    "model_eval(y_train, y_train_pred_rf_clf, model=\"clf\")\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_train, y_pred_rf_clf)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_train, y_pred_rf_clf, average='weighted')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_train, y_pred_rf_clf, average='weighted')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_train, y_pred_rf_clf, average='weighted')))\n",
    "#print(\"ROC/AUC: {:.2f}\".format(roc_auc_score(y_train, y_pred_lr_clf, multi_class='ovr')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_train, y_pred_rf_clf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_test_pred_rf_clf = best_model_rf_clf.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.16\n",
      "RMSE: 0.40\n",
      "MAE: 0.31\n",
      "R2: 0.50\n",
      "MAPE: 7.97\n",
      "MAPE median: 6.10\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_test, y_test_pred_rf_clf, model=\"clf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clf Model 3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create pipeline to use in RandomizedSearchCV and GridSearchCV\n",
    "pipeline_xgb_clf = Pipeline([('preprocessor', preprocessor),\n",
    "                             ('xgb_clf',\n",
    "                              XGBClassifier(n_estimators=110,\n",
    "                                            random_state=random_state,\n",
    "                                            max_depth=5,\n",
    "                                            max_features=20,\n",
    "                                            scoring=scoring,\n",
    "                                            n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Pre-Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_score', 'booster', 'colsample_bylevel', 'colsample_bynode', 'colsample_bytree', 'gamma', 'learning_rate', 'max_delta_step', 'max_depth', 'min_child_weight', 'missing', 'n_estimators', 'n_jobs', 'nthread', 'objective', 'random_state', 'reg_alpha', 'reg_lambda', 'scale_pos_weight', 'seed', 'silent', 'subsample', 'verbosity'])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display possible hyperparameters for LogisticRegression\n",
    "test_xgb_clf = XGBClassifier()\n",
    "test_xgb_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default values for LogisticRegression** (as base for hyperparameter search):\n",
    "\n",
    "penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter distribution\n",
    "param_distribs_xgb_clf = {\n",
    "    'xgb_clf__n_estimators': randint(low=10, high=200),\n",
    "    'xgb_clf__max_depth': randint(low=1, high=10),\n",
    "    'xgb_clf__learning_rate': [0.05, 0.1, 0.2, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 25.0min finished\n"
     ]
    }
   ],
   "source": [
    "# Create and fit RandomizedSearchCV, save \"best_model\"\n",
    "rnd_xgb_clf = RandomizedSearchCV(pipeline_xgb_clf,\n",
    "                                 param_distribs_xgb_clf,\n",
    "                                 cv=5,\n",
    "                                 n_iter=20,\n",
    "                                 scoring=scoring,\n",
    "                                 return_train_score=True,\n",
    "                                 verbose=4,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "best_model_rnd_xgb_clf = rnd_xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.30\n",
      "Best parameters:\n",
      "{'xgb_clf__learning_rate': 0.2, 'xgb_clf__max_depth': 8, 'xgb_clf__n_estimators': 126}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(rnd_xgb_clf.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(rnd_xgb_clf.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid_xgb_clf = {\n",
    "    'xgb_clf__n_estimators': [120, 130],\n",
    "    'xgb_clf__max_depth': [8, 10],\n",
    "    'xgb_clf__learning_rate': [0.2, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  7.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  7.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         Pipeline(steps=[('imputer_num',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('std_scaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['accommodates',\n",
       "                                                                          'accommodates_per_bed',\n",
       "                                                                          'am_balcony',\n",
       "                                                                          'am_breakfast',\n",
       "                                                                          'am_child_friendly',\n",
       "                                                                          'am_elevator',\n",
       "                                                                          'am_essentials',\n",
       "                                                                          'am_nature_and_views',\n",
       "                                                                          'am_pets_allo...\n",
       "                                                                          'neighbourhood_cleansed',\n",
       "                                                                          'property_type',\n",
       "                                                                          'room_type',\n",
       "                                                                          'zipcode'])])),\n",
       "                                       ('xgb_clf',\n",
       "                                        XGBClassifier(max_depth=5,\n",
       "                                                      max_features=20,\n",
       "                                                      n_estimators=110,\n",
       "                                                      n_jobs=-1,\n",
       "                                                      random_state=42,\n",
       "                                                      scoring='f1_weighted'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'xgb_clf__learning_rate': [0.2, 0.3],\n",
       "                         'xgb_clf__max_depth': [8, 10],\n",
       "                         'xgb_clf__n_estimators': [120, 130]},\n",
       "             return_train_score=True, scoring='f1_weighted', verbose=4)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit GridSearchCV, save \"best_model\"\n",
    "grid_xgb_clf = GridSearchCV(pipeline_xgb_clf,\n",
    "                            param_grid_xgb_clf,\n",
    "                            cv=3,\n",
    "                            scoring=scoring,\n",
    "                            return_train_score=True,\n",
    "                            verbose=4,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "grid_xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_xgb_clf = grid_xgb_clf.best_estimator_['xgb_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.29\n",
      "Best parameters:\n",
      "{'xgb_clf__learning_rate': 0.2, 'xgb_clf__max_depth': 10, 'xgb_clf__n_estimators': 130}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(grid_xgb_clf.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_xgb_clf.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>room_type_Private room</th>\n",
       "      <td>0.054966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Shared room</th>\n",
       "      <td>0.018998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Boutique hotel</th>\n",
       "      <td>0.017493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13407</th>\n",
       "      <td>0.013383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>0.013143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Ost 2</th>\n",
       "      <td>0.012749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Alt-Lichtenberg</th>\n",
       "      <td>0.012494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Hotel room</th>\n",
       "      <td>0.011030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13187</th>\n",
       "      <td>0.010751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14057</th>\n",
       "      <td>0.010312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates</th>\n",
       "      <td>0.009451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Friedenau</th>\n",
       "      <td>0.008156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13359</th>\n",
       "      <td>0.007994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_nb_other</th>\n",
       "      <td>0.007631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Zehlendorf  Nord</th>\n",
       "      <td>0.007563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10553</th>\n",
       "      <td>0.007483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Wedding Zentrum</th>\n",
       "      <td>0.007422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10243</th>\n",
       "      <td>0.007294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Secondary unit</th>\n",
       "      <td>0.007289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10707</th>\n",
       "      <td>0.007269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_super_strict</th>\n",
       "      <td>0.007202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms_log</th>\n",
       "      <td>0.007115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_südliche Luisenstadt</th>\n",
       "      <td>0.006945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10717</th>\n",
       "      <td>0.006924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10119</th>\n",
       "      <td>0.006911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10783</th>\n",
       "      <td>0.006894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12103</th>\n",
       "      <td>0.006793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10587</th>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13353</th>\n",
       "      <td>0.006660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Tempelhof</th>\n",
       "      <td>0.006578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13088</th>\n",
       "      <td>0.006563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10623</th>\n",
       "      <td>0.006499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10555</th>\n",
       "      <td>0.006387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10589</th>\n",
       "      <td>0.006326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Schöneberg-Nord</th>\n",
       "      <td>0.006293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Alexanderplatz</th>\n",
       "      <td>0.006273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10178</th>\n",
       "      <td>0.006212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10435</th>\n",
       "      <td>0.006151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10965</th>\n",
       "      <td>0.006052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Neu Lichtenberg</th>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10629</th>\n",
       "      <td>0.005938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calc_host_lst_count_sqrt_log</th>\n",
       "      <td>0.005938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14059</th>\n",
       "      <td>0.005891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Prenzlauer Berg Südwest</th>\n",
       "      <td>0.005885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Moabit Ost</th>\n",
       "      <td>0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Otto-Suhr-Allee</th>\n",
       "      <td>0.005796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_essentials</th>\n",
       "      <td>0.005793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10551</th>\n",
       "      <td>0.005791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Osloer Straße</th>\n",
       "      <td>0.005789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Parkviertel</th>\n",
       "      <td>0.005724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Helmholtzplatz</th>\n",
       "      <td>0.005640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10405</th>\n",
       "      <td>0.005610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10247</th>\n",
       "      <td>0.005561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Rixdorf</th>\n",
       "      <td>0.005527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10409</th>\n",
       "      <td>0.005518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12049</th>\n",
       "      <td>0.005502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_nördliche Luisenstadt</th>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_elevator</th>\n",
       "      <td>0.005457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10249</th>\n",
       "      <td>0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Brunnenstr. Süd</th>\n",
       "      <td>0.005409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12045</th>\n",
       "      <td>0.005394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10245</th>\n",
       "      <td>0.005228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12053</th>\n",
       "      <td>0.005208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10439</th>\n",
       "      <td>0.005203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Prenzlauer Berg Nordwest</th>\n",
       "      <td>0.005187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10407</th>\n",
       "      <td>0.005177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Schillerpromenade</th>\n",
       "      <td>0.005116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13189</th>\n",
       "      <td>0.005064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Frankfurter Allee Süd FK</th>\n",
       "      <td>0.005019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_location</th>\n",
       "      <td>0.004995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_other</th>\n",
       "      <td>0.004989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13086</th>\n",
       "      <td>0.004960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12157</th>\n",
       "      <td>0.004957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13351</th>\n",
       "      <td>0.004941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Düsseldorfer Straße</th>\n",
       "      <td>0.004876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Prenzlauer Berg Nord</th>\n",
       "      <td>0.004858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12043</th>\n",
       "      <td>0.004839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10179</th>\n",
       "      <td>0.004794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Karl-Marx-Allee-Süd</th>\n",
       "      <td>0.004779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_breakfast</th>\n",
       "      <td>0.004775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Kurfürstendamm</th>\n",
       "      <td>0.004750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wk_mth_discount</th>\n",
       "      <td>0.004747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_nature_and_views</th>\n",
       "      <td>0.004687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimum_nights_log</th>\n",
       "      <td>0.004670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12051</th>\n",
       "      <td>0.004667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Westend</th>\n",
       "      <td>0.004666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10785</th>\n",
       "      <td>0.004639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Mierendorffplatz</th>\n",
       "      <td>0.004625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_time_unknown</th>\n",
       "      <td>0.004614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12059</th>\n",
       "      <td>0.004593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_extra_people</th>\n",
       "      <td>0.004534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12055</th>\n",
       "      <td>0.004458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10117</th>\n",
       "      <td>0.004456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Pankow Süd</th>\n",
       "      <td>0.004432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_House</th>\n",
       "      <td>0.004413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Tempelhofer Vorstadt</th>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Moabit West</th>\n",
       "      <td>0.004383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10777</th>\n",
       "      <td>0.004377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Kantstraße</th>\n",
       "      <td>0.004302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Prenzlauer Berg Süd</th>\n",
       "      <td>0.004275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10557</th>\n",
       "      <td>0.004265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10437</th>\n",
       "      <td>0.004263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Neuköllner Mitte/Zentrum</th>\n",
       "      <td>0.004224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Südliche Friedrichstadt</th>\n",
       "      <td>0.004195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_nan</th>\n",
       "      <td>0.004192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Weißensee</th>\n",
       "      <td>0.004143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_time_within a few hours</th>\n",
       "      <td>0.004107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12163</th>\n",
       "      <td>0.004087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_private_entrance</th>\n",
       "      <td>0.004058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates_per_bed</th>\n",
       "      <td>0.004043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_white_goods</th>\n",
       "      <td>0.004037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_rate</th>\n",
       "      <td>0.004012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13347</th>\n",
       "      <td>0.004004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10787</th>\n",
       "      <td>0.003997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_tv</th>\n",
       "      <td>0.003970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>0.003969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12435</th>\n",
       "      <td>0.003965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10585</th>\n",
       "      <td>0.003965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Karl-Marx-Allee-Nord</th>\n",
       "      <td>0.003935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12047</th>\n",
       "      <td>0.003920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Brunnenstr. Nord</th>\n",
       "      <td>0.003901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10967</th>\n",
       "      <td>0.003895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10999</th>\n",
       "      <td>0.003894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10719</th>\n",
       "      <td>0.003889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_no</th>\n",
       "      <td>0.003883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_rating_sqrt</th>\n",
       "      <td>0.003872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>availability_90</th>\n",
       "      <td>0.003867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Plänterwald</th>\n",
       "      <td>0.003850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Regierungsviertel</th>\n",
       "      <td>0.003842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Reuterstraße</th>\n",
       "      <td>0.003815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10559</th>\n",
       "      <td>0.003788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Britz</th>\n",
       "      <td>0.003767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12161</th>\n",
       "      <td>0.003764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_extra_fees_sqrt</th>\n",
       "      <td>0.003748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Prenzlauer Berg Ost</th>\n",
       "      <td>0.003730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Volkspark Wilmersdorf</th>\n",
       "      <td>0.003716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Alt  Treptow</th>\n",
       "      <td>0.003683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_review_days_sqrt</th>\n",
       "      <td>0.003681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10715</th>\n",
       "      <td>0.003669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12347</th>\n",
       "      <td>0.003664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10997</th>\n",
       "      <td>0.003663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_balcony</th>\n",
       "      <td>0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_moderate</th>\n",
       "      <td>0.003643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_strict</th>\n",
       "      <td>0.003635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_time_within a day</th>\n",
       "      <td>0.003629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10627</th>\n",
       "      <td>0.003623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_review_days_sqrt</th>\n",
       "      <td>0.003619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_pets_allowed</th>\n",
       "      <td>0.003606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_time_within an hour</th>\n",
       "      <td>0.003597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len_sqrt</th>\n",
       "      <td>0.003577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_child_friendly</th>\n",
       "      <td>0.003555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_is_superhost</th>\n",
       "      <td>0.003531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maximum_nights</th>\n",
       "      <td>0.003530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <td>0.003523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Schöneberg-Süd</th>\n",
       "      <td>0.003509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_reviews_ltm_log</th>\n",
       "      <td>0.003498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Drakestr.</th>\n",
       "      <td>0.003492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10781</th>\n",
       "      <td>0.003480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Grunewald</th>\n",
       "      <td>0.003445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10969</th>\n",
       "      <td>0.003388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10365</th>\n",
       "      <td>0.003382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10317</th>\n",
       "      <td>0.003353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_smoking_allowed</th>\n",
       "      <td>0.003310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Tiergarten Süd</th>\n",
       "      <td>0.003217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14197</th>\n",
       "      <td>0.003181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Halensee</th>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instant_bookable</th>\n",
       "      <td>0.003075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13357</th>\n",
       "      <td>0.003074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10963</th>\n",
       "      <td>0.003046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13355</th>\n",
       "      <td>0.002945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Frankfurter Allee Nord</th>\n",
       "      <td>0.002911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Baumschulenweg</th>\n",
       "      <td>0.002856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10961</th>\n",
       "      <td>0.002832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12099</th>\n",
       "      <td>0.002758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Schloß Charlottenburg</th>\n",
       "      <td>0.002734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10625</th>\n",
       "      <td>0.002596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12101</th>\n",
       "      <td>0.002596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Neue Kantstraße</th>\n",
       "      <td>0.002455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10367</th>\n",
       "      <td>0.002427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10829</th>\n",
       "      <td>0.002406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Pankow Zentrum</th>\n",
       "      <td>0.002389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10823</th>\n",
       "      <td>0.001947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10711</th>\n",
       "      <td>0.001921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10827</th>\n",
       "      <td>0.001878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Bed and breakfast</th>\n",
       "      <td>0.001812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10318</th>\n",
       "      <td>0.001785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10315</th>\n",
       "      <td>0.001756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Blankenfelde/Niederschönhausen</th>\n",
       "      <td>0.001691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Barstraße</th>\n",
       "      <td>0.001523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed_Karlshorst</th>\n",
       "      <td>0.001491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13349</th>\n",
       "      <td>0.001258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12437</th>\n",
       "      <td>0.000839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13156</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13409</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10713</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Unique space</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      weight\n",
       "room_type_Private room                              0.054966\n",
       "room_type_Shared room                               0.018998\n",
       "property_type_Boutique hotel                        0.017493\n",
       "zipcode_zip_13407                                   0.013383\n",
       "bedrooms                                            0.013143\n",
       "neighbourhood_cleansed_Ost 2                        0.012749\n",
       "neighbourhood_cleansed_Alt-Lichtenberg              0.012494\n",
       "room_type_Hotel room                                0.011030\n",
       "zipcode_zip_13187                                   0.010751\n",
       "zipcode_zip_14057                                   0.010312\n",
       "accommodates                                        0.009451\n",
       "neighbourhood_cleansed_Friedenau                    0.008156\n",
       "zipcode_zip_13359                                   0.007994\n",
       "neighbourhood_cleansed_nb_other                     0.007631\n",
       "neighbourhood_cleansed_Zehlendorf  Nord             0.007563\n",
       "zipcode_zip_10553                                   0.007483\n",
       "neighbourhood_cleansed_Wedding Zentrum              0.007422\n",
       "zipcode_zip_10243                                   0.007294\n",
       "property_type_Secondary unit                        0.007289\n",
       "zipcode_zip_10707                                   0.007269\n",
       "cancellation_policy_super_strict                    0.007202\n",
       "bathrooms_log                                       0.007115\n",
       "neighbourhood_cleansed_südliche Luisenstadt         0.006945\n",
       "zipcode_zip_10717                                   0.006924\n",
       "zipcode_zip_10119                                   0.006911\n",
       "zipcode_zip_10783                                   0.006894\n",
       "zipcode_zip_12103                                   0.006793\n",
       "zipcode_zip_10587                                   0.006700\n",
       "zipcode_zip_13353                                   0.006660\n",
       "neighbourhood_cleansed_Tempelhof                    0.006578\n",
       "zipcode_zip_13088                                   0.006563\n",
       "zipcode_zip_10623                                   0.006499\n",
       "zipcode_zip_10555                                   0.006387\n",
       "zipcode_zip_10589                                   0.006326\n",
       "neighbourhood_cleansed_Schöneberg-Nord              0.006293\n",
       "neighbourhood_cleansed_Alexanderplatz               0.006273\n",
       "zipcode_zip_10178                                   0.006212\n",
       "zipcode_zip_10435                                   0.006151\n",
       "zipcode_zip_10965                                   0.006052\n",
       "neighbourhood_cleansed_Neu Lichtenberg              0.006048\n",
       "zipcode_zip_10629                                   0.005938\n",
       "calc_host_lst_count_sqrt_log                        0.005938\n",
       "zipcode_zip_14059                                   0.005891\n",
       "neighbourhood_cleansed_Prenzlauer Berg Südwest      0.005885\n",
       "neighbourhood_cleansed_Moabit Ost                   0.005859\n",
       "neighbourhood_cleansed_Otto-Suhr-Allee              0.005796\n",
       "am_essentials                                       0.005793\n",
       "zipcode_zip_10551                                   0.005791\n",
       "neighbourhood_cleansed_Osloer Straße                0.005789\n",
       "neighbourhood_cleansed_Parkviertel                  0.005724\n",
       "neighbourhood_cleansed_Helmholtzplatz               0.005640\n",
       "zipcode_zip_10405                                   0.005610\n",
       "zipcode_zip_10247                                   0.005561\n",
       "neighbourhood_cleansed_Rixdorf                      0.005527\n",
       "zipcode_zip_10409                                   0.005518\n",
       "zipcode_zip_12049                                   0.005502\n",
       "neighbourhood_cleansed_nördliche Luisenstadt        0.005500\n",
       "am_elevator                                         0.005457\n",
       "zipcode_zip_10249                                   0.005435\n",
       "neighbourhood_cleansed_Brunnenstr. Süd              0.005409\n",
       "zipcode_zip_12045                                   0.005394\n",
       "zipcode_zip_10245                                   0.005228\n",
       "zipcode_zip_12053                                   0.005208\n",
       "zipcode_zip_10439                                   0.005203\n",
       "neighbourhood_cleansed_Prenzlauer Berg Nordwest     0.005187\n",
       "zipcode_zip_10407                                   0.005177\n",
       "neighbourhood_cleansed_Schillerpromenade            0.005116\n",
       "zipcode_zip_13189                                   0.005064\n",
       "neighbourhood_cleansed_Frankfurter Allee Süd FK     0.005019\n",
       "review_scores_location                              0.004995\n",
       "zipcode_zip_other                                   0.004989\n",
       "zipcode_zip_13086                                   0.004960\n",
       "zipcode_zip_12157                                   0.004957\n",
       "zipcode_zip_13351                                   0.004941\n",
       "neighbourhood_cleansed_Düsseldorfer Straße          0.004876\n",
       "neighbourhood_cleansed_Prenzlauer Berg Nord         0.004858\n",
       "zipcode_zip_12043                                   0.004839\n",
       "zipcode_zip_10179                                   0.004794\n",
       "neighbourhood_cleansed_Karl-Marx-Allee-Süd          0.004779\n",
       "am_breakfast                                        0.004775\n",
       "neighbourhood_cleansed_Kurfürstendamm               0.004750\n",
       "wk_mth_discount                                     0.004747\n",
       "am_nature_and_views                                 0.004687\n",
       "minimum_nights_log                                  0.004670\n",
       "zipcode_zip_12051                                   0.004667\n",
       "neighbourhood_cleansed_Westend                      0.004666\n",
       "zipcode_zip_10785                                   0.004639\n",
       "neighbourhood_cleansed_Mierendorffplatz             0.004625\n",
       "host_response_time_unknown                          0.004614\n",
       "zipcode_zip_12059                                   0.004593\n",
       "price_extra_people                                  0.004534\n",
       "zipcode_zip_12055                                   0.004458\n",
       "zipcode_zip_10117                                   0.004456\n",
       "neighbourhood_cleansed_Pankow Süd                   0.004432\n",
       "property_type_House                                 0.004413\n",
       "neighbourhood_cleansed_Tempelhofer Vorstadt         0.004400\n",
       "neighbourhood_cleansed_Moabit West                  0.004383\n",
       "zipcode_zip_10777                                   0.004377\n",
       "neighbourhood_cleansed_Kantstraße                   0.004302\n",
       "neighbourhood_cleansed_Prenzlauer Berg Süd          0.004275\n",
       "zipcode_zip_10557                                   0.004265\n",
       "zipcode_zip_10437                                   0.004263\n",
       "neighbourhood_cleansed_Neuköllner Mitte/Zentrum     0.004224\n",
       "neighbourhood_cleansed_Südliche Friedrichstadt      0.004195\n",
       "zipcode_zip_nan                                     0.004192\n",
       "neighbourhood_cleansed_Weißensee                    0.004143\n",
       "host_response_time_within a few hours               0.004107\n",
       "zipcode_zip_12163                                   0.004087\n",
       "am_private_entrance                                 0.004058\n",
       "accommodates_per_bed                                0.004043\n",
       "am_white_goods                                      0.004037\n",
       "host_response_rate                                  0.004012\n",
       "zipcode_zip_13347                                   0.004004\n",
       "zipcode_zip_10787                                   0.003997\n",
       "am_tv                                               0.003970\n",
       "latitude                                            0.003969\n",
       "zipcode_zip_12435                                   0.003965\n",
       "zipcode_zip_10585                                   0.003965\n",
       "neighbourhood_cleansed_Karl-Marx-Allee-Nord         0.003935\n",
       "zipcode_zip_12047                                   0.003920\n",
       "longitude                                           0.003906\n",
       "neighbourhood_cleansed_Brunnenstr. Nord             0.003901\n",
       "zipcode_zip_10967                                   0.003895\n",
       "zipcode_zip_10999                                   0.003894\n",
       "zipcode_zip_10719                                   0.003889\n",
       "listing_no                                          0.003883\n",
       "review_scores_rating_sqrt                           0.003872\n",
       "availability_90                                     0.003867\n",
       "neighbourhood_cleansed_Plänterwald                  0.003850\n",
       "neighbourhood_cleansed_Regierungsviertel            0.003842\n",
       "neighbourhood_cleansed_Reuterstraße                 0.003815\n",
       "zipcode_zip_10559                                   0.003788\n",
       "neighbourhood_cleansed_Britz                        0.003767\n",
       "zipcode_zip_12161                                   0.003764\n",
       "price_extra_fees_sqrt                               0.003748\n",
       "neighbourhood_cleansed_Prenzlauer Berg Ost          0.003730\n",
       "neighbourhood_cleansed_Volkspark Wilmersdorf        0.003716\n",
       "neighbourhood_cleansed_Alt  Treptow                 0.003683\n",
       "first_review_days_sqrt                              0.003681\n",
       "zipcode_zip_10715                                   0.003669\n",
       "zipcode_zip_12347                                   0.003664\n",
       "zipcode_zip_10997                                   0.003663\n",
       "am_balcony                                          0.003648\n",
       "cancellation_policy_moderate                        0.003643\n",
       "cancellation_policy_strict                          0.003635\n",
       "host_response_time_within a day                     0.003629\n",
       "zipcode_zip_10627                                   0.003623\n",
       "last_review_days_sqrt                               0.003619\n",
       "am_pets_allowed                                     0.003606\n",
       "host_response_time_within an hour                   0.003597\n",
       "text_len_sqrt                                       0.003577\n",
       "am_child_friendly                                   0.003555\n",
       "host_is_superhost                                   0.003531\n",
       "maximum_nights                                      0.003530\n",
       "host_acceptance_rate                                0.003523\n",
       "neighbourhood_cleansed_Schöneberg-Süd               0.003509\n",
       "number_of_reviews_ltm_log                           0.003498\n",
       "neighbourhood_cleansed_Drakestr.                    0.003492\n",
       "zipcode_zip_10781                                   0.003480\n",
       "neighbourhood_cleansed_Grunewald                    0.003445\n",
       "zipcode_zip_10969                                   0.003388\n",
       "zipcode_zip_10365                                   0.003382\n",
       "zipcode_zip_10317                                   0.003353\n",
       "am_smoking_allowed                                  0.003310\n",
       "neighbourhood_cleansed_Tiergarten Süd               0.003217\n",
       "zipcode_zip_14197                                   0.003181\n",
       "neighbourhood_cleansed_Halensee                     0.003176\n",
       "instant_bookable                                    0.003075\n",
       "zipcode_zip_13357                                   0.003074\n",
       "zipcode_zip_10963                                   0.003046\n",
       "zipcode_zip_13355                                   0.002945\n",
       "neighbourhood_cleansed_Frankfurter Allee Nord       0.002911\n",
       "neighbourhood_cleansed_Baumschulenweg               0.002856\n",
       "zipcode_zip_10961                                   0.002832\n",
       "zipcode_zip_12099                                   0.002758\n",
       "neighbourhood_cleansed_Schloß Charlottenburg        0.002734\n",
       "zipcode_zip_10625                                   0.002596\n",
       "zipcode_zip_12101                                   0.002596\n",
       "neighbourhood_cleansed_Neue Kantstraße              0.002455\n",
       "zipcode_zip_10367                                   0.002427\n",
       "zipcode_zip_10829                                   0.002406\n",
       "neighbourhood_cleansed_Pankow Zentrum               0.002389\n",
       "zipcode_zip_10823                                   0.001947\n",
       "zipcode_zip_10711                                   0.001921\n",
       "zipcode_zip_10827                                   0.001878\n",
       "property_type_Bed and breakfast                     0.001812\n",
       "zipcode_zip_10318                                   0.001785\n",
       "zipcode_zip_10315                                   0.001756\n",
       "neighbourhood_cleansed_Blankenfelde/Niederschön...  0.001691\n",
       "neighbourhood_cleansed_Barstraße                    0.001523\n",
       "neighbourhood_cleansed_Karlshorst                   0.001491\n",
       "zipcode_zip_13349                                   0.001258\n",
       "zipcode_zip_12437                                   0.000839\n",
       "zipcode_zip_13156                                   0.000000\n",
       "zipcode_zip_13409                                   0.000000\n",
       "zipcode_zip_10713                                   0.000000\n",
       "property_type_Unique space                          0.000000"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get and print feature importances\n",
    "grid_xgb_clf_fi = feat_importances(grid_xgb_clf, cv_model=True, named_steps='xgb_clf', column_names=column_names)\n",
    "grid_xgb_clf_fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_train_pred_xgb_clf = best_model_xgb_clf.predict(X_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Recall: 1.00\n",
      "Precision: 1.00\n",
      "F1 Score: 1.00\n",
      "Confusion Matrix: \n",
      "[[ 307    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0 1057    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1353    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0 1276    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0  926    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0  817    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0  665    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0  449    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  457    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  786    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0  397]]\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation with \"best model\"\n",
    "model_eval(y_train, y_train_pred_xgb_clf, model=\"clf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_test_pred_xgb_clf = best_model_xgb_clf.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.16\n",
      "RMSE: 0.40\n",
      "MAE: 0.31\n",
      "R2: 0.50\n",
      "MAPE: 7.97\n",
      "MAPE median: 6.10\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_test, y_test_pred_xgb_clf, model=\"clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!! NN Model 1: TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: Regression (\"price_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using \u001b[1mPRICE_LOG\u001b[0m as the target and \u001b[1mneg_median_absolute_error\u001b[0m for scoring to predict prices for \u001b[1mberlin\u001b[0m on \u001b[1m2020-03-17\u001b[0m\n",
      "\n",
      "The target variable y is set to \u001b[1mPRICE_LOG\u001b[0m\n",
      "\n",
      "You are currently using these features for its prediction:\n",
      "\u001b[1m['accommodates', 'accommodates_per_bed', 'am_balcony', 'am_breakfast', 'am_child_friendly', 'am_elevator', 'am_essentials', 'am_pets_allowed', 'am_private_entrance', 'am_smoking_allowed', 'am_tv', 'availability_90', 'bathrooms_log', 'bedrooms', 'calc_host_lst_count_sqrt_log', 'cancellation_policy', 'host_is_superhost', 'instant_bookable', 'maximum_nights', 'minimum_nights_log', 'property_type', 'room_type', 'wk_mth_discount', 'zipcode']\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print current setting for TARGET\n",
    "print_target_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Select models for comparison\n",
    "regmodels = {\n",
    "    'Baseline':\n",
    "    DummyRegressor(strategy='mean'),\n",
    "    'LinReg':\n",
    "    LinearRegression(),\n",
    "    'Passive Aggressive':\n",
    "    PassiveAggressiveRegressor(),\n",
    "    #        'RANSAC' : RANSACRegressor(),\n",
    "    'ElasticNet':\n",
    "    ElasticNet(),\n",
    "    'Stochastic Gradient Descent':\n",
    "    SGDRegressor(max_iter=1000, tol=1e-3),\n",
    "    'Decision Tree':\n",
    "    DecisionTreeRegressor(criterion=\"mse\",\n",
    "                          max_depth=3,\n",
    "                          random_state=random_state),\n",
    "    'Random Forest':\n",
    "    RandomForestRegressor(random_state=random_state,\n",
    "                          max_features='sqrt',\n",
    "                          n_jobs=-1),\n",
    "    'Gradient Boost':\n",
    "    GradientBoostingRegressor(random_state=random_state),\n",
    "    'XGBoost':\n",
    "    XGBRegressor(),\n",
    "    'AdaBoost':\n",
    "    AdaBoostRegressor(random_state=random_state),\n",
    "    'SVR':\n",
    "    SVR(),\n",
    "    'CatBoost':\n",
    "    CatBoostRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.2s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.6s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   10.1s remaining:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   12.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.9s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.5s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.6s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    7.1s remaining:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   11.3s remaining:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MAPE median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>11.766269</td>\n",
       "      <td>9.480414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7.671335</td>\n",
       "      <td>6.054153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Passive Aggressive</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>10.144652</td>\n",
       "      <td>8.144422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>11.766269</td>\n",
       "      <td>9.480414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>13.034482</td>\n",
       "      <td>10.043096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>8.776042</td>\n",
       "      <td>6.868136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.410750</td>\n",
       "      <td>5.773472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gradient Boost</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7.535995</td>\n",
       "      <td>5.968549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7.545994</td>\n",
       "      <td>5.967451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.34</td>\n",
       "      <td>8.725021</td>\n",
       "      <td>6.942201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SVR</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.424019</td>\n",
       "      <td>5.725685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.28</td>\n",
       "      <td>7.160986</td>\n",
       "      <td>5.542889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model   MSE  RMSE     R2   MAE       MAPE  \\\n",
       "0                      Baseline  0.33  0.57  -0.00  0.46  11.766269   \n",
       "1                        LinReg  0.15  0.39   0.53  0.30   7.671335   \n",
       "2            Passive Aggressive  0.27  0.52   0.19  0.40  10.144652   \n",
       "3                    ElasticNet  0.33  0.57  -0.00  0.46  11.766269   \n",
       "4   Stochastic Gradient Descent  0.49  0.70  -0.48  0.52  13.034482   \n",
       "5                 Decision Tree  0.20  0.45   0.39  0.34   8.776042   \n",
       "6                 Random Forest  0.14  0.38   0.56  0.29   7.410750   \n",
       "7                Gradient Boost  0.15  0.38   0.55  0.30   7.535995   \n",
       "8                       XGBoost  0.15  0.38   0.55  0.30   7.545994   \n",
       "9                      AdaBoost  0.19  0.44   0.41  0.34   8.725021   \n",
       "10                          SVR  0.15  0.38   0.55  0.29   7.424019   \n",
       "11                     CatBoost  0.14  0.37   0.59  0.28   7.160986   \n",
       "\n",
       "    MAPE median  \n",
       "0      9.480414  \n",
       "1      6.054153  \n",
       "2      8.144422  \n",
       "3      9.480414  \n",
       "4     10.043096  \n",
       "5      6.868136  \n",
       "6      5.773472  \n",
       "7      5.968549  \n",
       "8      5.967451  \n",
       "9      6.942201  \n",
       "10     5.725685  \n",
       "11     5.542889  "
      ]
     },
     "execution_count": 1018,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and display results\n",
    "results = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'R2', 'MAE', 'MAPE', 'MAPE median'])\n",
    "i = 0\n",
    "for m in regmodels.items():\n",
    "    # Building a full pipeline with our preprocessor and a Classifier\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), (m[0], m[1])])\n",
    "    # Making predictions on the training set using cross validation as well as calculating the probabilities\n",
    "    y_train_pred = cross_val_predict(pipe,\n",
    "                                     X_train,\n",
    "                                     y_train.values.ravel(),\n",
    "                                     cv=5,\n",
    "                                     verbose=4,\n",
    "                                     n_jobs=-1)\n",
    "    # Calculating metrices\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            'Model':\n",
    "            m[0],\n",
    "            'MSE':\n",
    "            \"{:.2f}\".format(mean_squared_error(y_train, y_train_pred)),\n",
    "            'RMSE':\n",
    "            \"{:.2f}\".format(\n",
    "                mean_squared_error(y_train, y_train_pred, squared=False)),\n",
    "            'R2':\n",
    "            \"{:.2f}\".format(r2_score(y_train, y_train_pred)),\n",
    "            'MAE':\n",
    "            \"{:.2f}\".format(mean_absolute_error(y_train, y_train_pred)),\n",
    "            'MAPE': mean_absolute_percentage_error(y_train, y_train_pred),\n",
    "            'MAPE median': median_absolute_percentage_error(y_train, y_train_pred)\n",
    "        },\n",
    "        index=[i])\n",
    "    i += 1\n",
    "    results = pd.concat([results, temp])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg Model 1: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create pipeline to use in RandomizedSearchCV and GridSearchCV\n",
    "pipeline_xgb_reg = Pipeline([('preprocessor', preprocessor),\n",
    "                             ('xgb_reg',\n",
    "                              XGBRegressor(n_estimators=182,\n",
    "                                           learning_rate=0.45,\n",
    "                                           random_state=random_state,\n",
    "                                           max_depth=5,\n",
    "                                           gamma=0.3,\n",
    "                                           bootstrap=True,\n",
    "                                           max_features=21,\n",
    "                                           scoring=scoring,\n",
    "                                           n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Pre-Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_score', 'booster', 'colsample_bylevel', 'colsample_bynode', 'colsample_bytree', 'gamma', 'importance_type', 'learning_rate', 'max_delta_step', 'max_depth', 'min_child_weight', 'missing', 'n_estimators', 'n_jobs', 'nthread', 'objective', 'random_state', 'reg_alpha', 'reg_lambda', 'scale_pos_weight', 'seed', 'silent', 'subsample', 'verbosity'])"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display possible hyperparameters for XGBoost Regressor\n",
    "test_xgb_reg = XGBRegressor()\n",
    "test_xgb_reg.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default values for XGBRegressor** (as base for hyperparameter search):\n",
    "\n",
    "max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1449,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter distribution\n",
    "param_distribs_xgb_reg = {\n",
    "    'xgb_reg__n_estimators': randint(low=80, high=300),\n",
    "    'xgb_reg__bootstrap': [True, False],\n",
    "    'xgb_reg__gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'xgb_reg__max_depth': randint(low=1, high=6),\n",
    "    'xgb_reg__max_features': randint(low=1, high=40),\n",
    "    'xgb_reg__learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1450,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   33.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:15:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# Create and fit RandomizedSearchCV, save \"best_model\"\n",
    "rnd_xgb_reg = RandomizedSearchCV(pipeline_xgb_reg,\n",
    "                                 param_distribs_xgb_reg,\n",
    "                                 cv=5,\n",
    "                                 scoring=scoring,\n",
    "                                 n_iter=10,\n",
    "                                 return_train_score=True,\n",
    "                                 verbose=4,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "best_model_rnd_xgb_reg = rnd_xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "-0.23\n",
      "Best parameters:\n",
      "{'xgb_reg__bootstrap': True, 'xgb_reg__gamma': 0.3, 'xgb_reg__learning_rate': 0.45, 'xgb_reg__max_depth': 5, 'xgb_reg__max_features': 21, 'xgb_reg__n_estimators': 182}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(rnd_xgb_reg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(rnd_xgb_reg.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(rnd_xgb_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid_xgb_reg = {\n",
    "#    'xgb_reg__bootstrap': [True, False],\n",
    "#    'xgb_reg__n_estimators': [190, 230, 290],\n",
    "#    'xgb_reg__max_features': [40, 45],\n",
    "    'xgb_reg__max_depth': [4, 5],\n",
    "    'xgb_reg__learning_rate': [0.42, 0.45, 0.48]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1454,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   28.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:16:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# Create and fit GridSearchCV, save \"best_model\"\n",
    "grid_xgb_reg = GridSearchCV(pipeline_xgb_reg,\n",
    "                            param_grid_xgb_reg,\n",
    "                            cv=5,\n",
    "                            scoring=scoring,\n",
    "                            return_train_score=True,\n",
    "                            verbose=4,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "grid_xgb_reg = grid_xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1455,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_xgb_reg = grid_xgb_reg.best_estimator_['xgb_reg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1456,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "-0.23\n",
      "Best parameters:\n",
      "{'xgb_reg__learning_rate': 0.48, 'xgb_reg__max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(grid_xgb_reg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_xgb_reg.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_xgb_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1457,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>room_type_Private room</th>\n",
       "      <td>0.393896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>0.047220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Shared room</th>\n",
       "      <td>0.045039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Hotel room</th>\n",
       "      <td>0.035768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_elevator</th>\n",
       "      <td>0.029572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10117</th>\n",
       "      <td>0.025624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10119</th>\n",
       "      <td>0.020617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Boutique hotel</th>\n",
       "      <td>0.017275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calc_host_lst_count_sqrt_log</th>\n",
       "      <td>0.015102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates</th>\n",
       "      <td>0.014723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms_log</th>\n",
       "      <td>0.013985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10435</th>\n",
       "      <td>0.009736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_other</th>\n",
       "      <td>0.009508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_tv</th>\n",
       "      <td>0.009331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10405</th>\n",
       "      <td>0.008985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_super_strict</th>\n",
       "      <td>0.008548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10997</th>\n",
       "      <td>0.008253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10553</th>\n",
       "      <td>0.007570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13407</th>\n",
       "      <td>0.007345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10245</th>\n",
       "      <td>0.007024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12099</th>\n",
       "      <td>0.006712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wk_mth_discount</th>\n",
       "      <td>0.005927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimum_nights_log</th>\n",
       "      <td>0.005838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>availability_90</th>\n",
       "      <td>0.005367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13409</th>\n",
       "      <td>0.005184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_balcony</th>\n",
       "      <td>0.005152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12347</th>\n",
       "      <td>0.005148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10999</th>\n",
       "      <td>0.005018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12043</th>\n",
       "      <td>0.004988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13347</th>\n",
       "      <td>0.004924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13189</th>\n",
       "      <td>0.004727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13359</th>\n",
       "      <td>0.004726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12055</th>\n",
       "      <td>0.004724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_essentials</th>\n",
       "      <td>0.004585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13086</th>\n",
       "      <td>0.004498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10559</th>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10365</th>\n",
       "      <td>0.004378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10178</th>\n",
       "      <td>0.004334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13353</th>\n",
       "      <td>0.004257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12157</th>\n",
       "      <td>0.004244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maximum_nights</th>\n",
       "      <td>0.004154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10587</th>\n",
       "      <td>0.004055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10315</th>\n",
       "      <td>0.004053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13187</th>\n",
       "      <td>0.003970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_strict</th>\n",
       "      <td>0.003836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10965</th>\n",
       "      <td>0.003827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10179</th>\n",
       "      <td>0.003784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_breakfast</th>\n",
       "      <td>0.003713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10967</th>\n",
       "      <td>0.003701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates_per_bed</th>\n",
       "      <td>0.003689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10719</th>\n",
       "      <td>0.003675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10437</th>\n",
       "      <td>0.003671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_is_superhost</th>\n",
       "      <td>0.003525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_smoking_allowed</th>\n",
       "      <td>0.003519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12103</th>\n",
       "      <td>0.003492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_private_entrance</th>\n",
       "      <td>0.003483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12047</th>\n",
       "      <td>0.003467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10589</th>\n",
       "      <td>0.003255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10963</th>\n",
       "      <td>0.003210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_pets_allowed</th>\n",
       "      <td>0.003207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13355</th>\n",
       "      <td>0.003136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13357</th>\n",
       "      <td>0.003116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13349</th>\n",
       "      <td>0.003105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10317</th>\n",
       "      <td>0.003088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10439</th>\n",
       "      <td>0.002992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13351</th>\n",
       "      <td>0.002990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10557</th>\n",
       "      <td>0.002886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10243</th>\n",
       "      <td>0.002865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_House</th>\n",
       "      <td>0.002852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12049</th>\n",
       "      <td>0.002694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12053</th>\n",
       "      <td>0.002682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12437</th>\n",
       "      <td>0.002612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10318</th>\n",
       "      <td>0.002572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Secondary unit</th>\n",
       "      <td>0.002402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10367</th>\n",
       "      <td>0.002326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14057</th>\n",
       "      <td>0.002310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instant_bookable</th>\n",
       "      <td>0.002201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12059</th>\n",
       "      <td>0.002197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10551</th>\n",
       "      <td>0.002177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12163</th>\n",
       "      <td>0.002164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12051</th>\n",
       "      <td>0.002116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Bed and breakfast</th>\n",
       "      <td>0.002068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13156</th>\n",
       "      <td>0.002052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10715</th>\n",
       "      <td>0.001866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10713</th>\n",
       "      <td>0.001846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10249</th>\n",
       "      <td>0.001817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10961</th>\n",
       "      <td>0.001809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10409</th>\n",
       "      <td>0.001795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_moderate</th>\n",
       "      <td>0.001778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10629</th>\n",
       "      <td>0.001724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10969</th>\n",
       "      <td>0.001677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10785</th>\n",
       "      <td>0.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10707</th>\n",
       "      <td>0.001666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10827</th>\n",
       "      <td>0.001654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12435</th>\n",
       "      <td>0.001624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_child_friendly</th>\n",
       "      <td>0.001574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_nan</th>\n",
       "      <td>0.001472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12101</th>\n",
       "      <td>0.001460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12161</th>\n",
       "      <td>0.001379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10717</th>\n",
       "      <td>0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14059</th>\n",
       "      <td>0.001240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10627</th>\n",
       "      <td>0.001221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13088</th>\n",
       "      <td>0.001160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10781</th>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10585</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Unique space</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14197</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10247</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10407</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10555</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12045</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10623</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10711</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10777</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10783</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10787</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10823</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10829</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10625</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    weight\n",
       "room_type_Private room            0.393896\n",
       "bedrooms                          0.047220\n",
       "room_type_Shared room             0.045039\n",
       "room_type_Hotel room              0.035768\n",
       "am_elevator                       0.029572\n",
       "zipcode_zip_10117                 0.025624\n",
       "zipcode_zip_10119                 0.020617\n",
       "property_type_Boutique hotel      0.017275\n",
       "calc_host_lst_count_sqrt_log      0.015102\n",
       "accommodates                      0.014723\n",
       "bathrooms_log                     0.013985\n",
       "zipcode_zip_10435                 0.009736\n",
       "zipcode_zip_other                 0.009508\n",
       "am_tv                             0.009331\n",
       "zipcode_zip_10405                 0.008985\n",
       "cancellation_policy_super_strict  0.008548\n",
       "zipcode_zip_10997                 0.008253\n",
       "zipcode_zip_10553                 0.007570\n",
       "zipcode_zip_13407                 0.007345\n",
       "zipcode_zip_10245                 0.007024\n",
       "zipcode_zip_12099                 0.006712\n",
       "wk_mth_discount                   0.005927\n",
       "minimum_nights_log                0.005838\n",
       "availability_90                   0.005367\n",
       "zipcode_zip_13409                 0.005184\n",
       "am_balcony                        0.005152\n",
       "zipcode_zip_12347                 0.005148\n",
       "zipcode_zip_10999                 0.005018\n",
       "zipcode_zip_12043                 0.004988\n",
       "zipcode_zip_13347                 0.004924\n",
       "zipcode_zip_13189                 0.004727\n",
       "zipcode_zip_13359                 0.004726\n",
       "zipcode_zip_12055                 0.004724\n",
       "am_essentials                     0.004585\n",
       "zipcode_zip_13086                 0.004498\n",
       "zipcode_zip_10559                 0.004400\n",
       "zipcode_zip_10365                 0.004378\n",
       "zipcode_zip_10178                 0.004334\n",
       "zipcode_zip_13353                 0.004257\n",
       "zipcode_zip_12157                 0.004244\n",
       "maximum_nights                    0.004154\n",
       "zipcode_zip_10587                 0.004055\n",
       "zipcode_zip_10315                 0.004053\n",
       "zipcode_zip_13187                 0.003970\n",
       "cancellation_policy_strict        0.003836\n",
       "zipcode_zip_10965                 0.003827\n",
       "zipcode_zip_10179                 0.003784\n",
       "am_breakfast                      0.003713\n",
       "zipcode_zip_10967                 0.003701\n",
       "accommodates_per_bed              0.003689\n",
       "zipcode_zip_10719                 0.003675\n",
       "zipcode_zip_10437                 0.003671\n",
       "host_is_superhost                 0.003525\n",
       "am_smoking_allowed                0.003519\n",
       "zipcode_zip_12103                 0.003492\n",
       "am_private_entrance               0.003483\n",
       "zipcode_zip_12047                 0.003467\n",
       "zipcode_zip_10589                 0.003255\n",
       "zipcode_zip_10963                 0.003210\n",
       "am_pets_allowed                   0.003207\n",
       "zipcode_zip_13355                 0.003136\n",
       "zipcode_zip_13357                 0.003116\n",
       "zipcode_zip_13349                 0.003105\n",
       "zipcode_zip_10317                 0.003088\n",
       "zipcode_zip_10439                 0.002992\n",
       "zipcode_zip_13351                 0.002990\n",
       "zipcode_zip_10557                 0.002886\n",
       "zipcode_zip_10243                 0.002865\n",
       "property_type_House               0.002852\n",
       "zipcode_zip_12049                 0.002694\n",
       "zipcode_zip_12053                 0.002682\n",
       "zipcode_zip_12437                 0.002612\n",
       "zipcode_zip_10318                 0.002572\n",
       "property_type_Secondary unit      0.002402\n",
       "zipcode_zip_10367                 0.002326\n",
       "zipcode_zip_14057                 0.002310\n",
       "instant_bookable                  0.002201\n",
       "zipcode_zip_12059                 0.002197\n",
       "zipcode_zip_10551                 0.002177\n",
       "zipcode_zip_12163                 0.002164\n",
       "zipcode_zip_12051                 0.002116\n",
       "property_type_Bed and breakfast   0.002068\n",
       "zipcode_zip_13156                 0.002052\n",
       "zipcode_zip_10715                 0.001866\n",
       "zipcode_zip_10713                 0.001846\n",
       "zipcode_zip_10249                 0.001817\n",
       "zipcode_zip_10961                 0.001809\n",
       "zipcode_zip_10409                 0.001795\n",
       "cancellation_policy_moderate      0.001778\n",
       "zipcode_zip_10629                 0.001724\n",
       "zipcode_zip_10969                 0.001677\n",
       "zipcode_zip_10785                 0.001673\n",
       "zipcode_zip_10707                 0.001666\n",
       "zipcode_zip_10827                 0.001654\n",
       "zipcode_zip_12435                 0.001624\n",
       "am_child_friendly                 0.001574\n",
       "zipcode_zip_nan                   0.001472\n",
       "zipcode_zip_12101                 0.001460\n",
       "zipcode_zip_12161                 0.001379\n",
       "zipcode_zip_10717                 0.001286\n",
       "zipcode_zip_14059                 0.001240\n",
       "zipcode_zip_10627                 0.001221\n",
       "zipcode_zip_13088                 0.001160\n",
       "zipcode_zip_10781                 0.001135\n",
       "zipcode_zip_10585                 0.000000\n",
       "property_type_Unique space        0.000000\n",
       "zipcode_zip_14197                 0.000000\n",
       "zipcode_zip_10247                 0.000000\n",
       "zipcode_zip_10407                 0.000000\n",
       "zipcode_zip_10555                 0.000000\n",
       "zipcode_zip_12045                 0.000000\n",
       "zipcode_zip_10623                 0.000000\n",
       "zipcode_zip_10711                 0.000000\n",
       "zipcode_zip_10777                 0.000000\n",
       "zipcode_zip_10783                 0.000000\n",
       "zipcode_zip_10787                 0.000000\n",
       "zipcode_zip_10823                 0.000000\n",
       "zipcode_zip_10829                 0.000000\n",
       "zipcode_zip_10625                 0.000000"
      ]
     },
     "execution_count": 1457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display feature importances\n",
    "fi_xgb_reg = get_feat_importances(best_model_xgb_reg)\n",
    "fi_xgb_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1458,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load existing model\n",
    "#load_best_model = load_model(title=\"best_model_xgb_reg\", dataset_loc=dataset_loc, dataset_date=dataset_date)\n",
    "#load_best_cv = load_model(title=\"best_cv_xgb_reg\", dataset_loc=dataset_loc, dataset_date=dataset_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Curves (Overfitting)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1459,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_train_pred_xgb_reg = best_model_xgb_reg.predict(X_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1460,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.11\n",
      "RMSE: 0.33\n",
      "MAE: 0.25\n",
      "R2: 0.67\n",
      "MAPE: 6.41\n",
      "MAPE median: 5.03\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_train, y_train_pred_xgb_reg, model=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1461,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_test_pred_xgb_reg = best_model_xgb_reg.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1462,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.14\n",
      "RMSE: 0.38\n",
      "MAE: 0.29\n",
      "R2: 0.56\n",
      "MAPE: 7.37\n",
      "MAPE median: 5.53\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_test, y_test_pred_xgb_reg, model=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1463,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36090395, 0.39091746])"
      ]
     },
     "execution_count": 1463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display confidence interval (scipy stats)\n",
    "confidence = 0.95\n",
    "squared_errors = (y_test_pred_xgb_reg - y_test)**2\n",
    "np.sqrt(\n",
    "    stats.t.interval(confidence,\n",
    "                     len(squared_errors) - 1,\n",
    "                     loc=squared_errors.mean(),\n",
    "                     scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Median Price Intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1464,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save MAPE_median as variable\n",
    "MAPE_median_xgb_reg = (median_absolute_percentage_error(y_test, y_test_pred_xgb_reg))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1465,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51.87, 82.4),\n",
       " (61.47, 99.61),\n",
       " (55.63, 89.08),\n",
       " (31.68, 47.49),\n",
       " (39.53, 60.83),\n",
       " (27.36, 40.33),\n",
       " (54.61, 87.26),\n",
       " (32.05, 48.12),\n",
       " (52.94, 84.3),\n",
       " (43.97, 68.51),\n",
       " (25.99, 38.07),\n",
       " (36.13, 55.01),\n",
       " (75.34, 125.02),\n",
       " (23.46, 33.96),\n",
       " (37.2, 56.84),\n",
       " (78.13, 130.2),\n",
       " (74.33, 123.14),\n",
       " (65.05, 106.1),\n",
       " (52.57, 83.64),\n",
       " (49.36, 77.94),\n",
       " (90.31, 153.08),\n",
       " (48.75, 76.87),\n",
       " (40.17, 61.92),\n",
       " (82.97, 139.24),\n",
       " (24.74, 36.02),\n",
       " (55.15, 88.22),\n",
       " (82.73, 138.79),\n",
       " (33.45, 50.47),\n",
       " (24.42, 35.51),\n",
       " (42.86, 66.57),\n",
       " (26.27, 38.52),\n",
       " (66.55, 108.84),\n",
       " (49.83, 78.78),\n",
       " (28.4, 42.04),\n",
       " (41.82, 64.77),\n",
       " (57.06, 91.66),\n",
       " (37.24, 56.9),\n",
       " (54.21, 86.56),\n",
       " (33.49, 50.53),\n",
       " (38.09, 58.35),\n",
       " (30.22, 45.06),\n",
       " (20.82, 29.72),\n",
       " (40.43, 62.37),\n",
       " (27.77, 41.0),\n",
       " (29.32, 43.56),\n",
       " (41.89, 64.9),\n",
       " (47.0, 73.79),\n",
       " (72.09, 119.01),\n",
       " (66.56, 108.87),\n",
       " (91.05, 154.49),\n",
       " (36.89, 56.3),\n",
       " (66.12, 108.06),\n",
       " (52.33, 83.21),\n",
       " (51.11, 81.05),\n",
       " (34.33, 51.95),\n",
       " (45.05, 70.38),\n",
       " (54.61, 87.26),\n",
       " (36.07, 54.91),\n",
       " (66.46, 108.67),\n",
       " (32.78, 49.34),\n",
       " (56.26, 90.21),\n",
       " (53.77, 85.77),\n",
       " (63.9, 104.01),\n",
       " (46.57, 73.05),\n",
       " (53.23, 84.81),\n",
       " (49.06, 77.43),\n",
       " (25.84, 37.83),\n",
       " (29.91, 44.54),\n",
       " (47.98, 75.52),\n",
       " (36.24, 55.2),\n",
       " (46.42, 72.78),\n",
       " (80.55, 134.71),\n",
       " (35.68, 54.24),\n",
       " (55.03, 88.02),\n",
       " (211.93, 396.99),\n",
       " (49.43, 78.08),\n",
       " (15.31, 21.08),\n",
       " (59.42, 95.9),\n",
       " (32.76, 49.31),\n",
       " (28.5, 42.2),\n",
       " (24.95, 36.37),\n",
       " (20.35, 28.96),\n",
       " (36.33, 55.34),\n",
       " (24.94, 36.36),\n",
       " (54.7, 87.43),\n",
       " (61.79, 100.18),\n",
       " (38.8, 59.57),\n",
       " (69.98, 115.13),\n",
       " (42.16, 65.36),\n",
       " (63.85, 103.93),\n",
       " (71.84, 118.54),\n",
       " (45.31, 70.84),\n",
       " (39.28, 60.4),\n",
       " (92.78, 157.76),\n",
       " (58.96, 95.08),\n",
       " (41.44, 64.12),\n",
       " (115.3, 201.11),\n",
       " (53.35, 85.02),\n",
       " (30.91, 46.21),\n",
       " (16.24, 22.52),\n",
       " (30.91, 46.2),\n",
       " (51.64, 81.99),\n",
       " (27.67, 40.83),\n",
       " (42.6, 66.11),\n",
       " (28.02, 41.42),\n",
       " (57.97, 93.29),\n",
       " (23.81, 34.52),\n",
       " (27.81, 41.07),\n",
       " (38.02, 58.23),\n",
       " (52.24, 83.05),\n",
       " (37.57, 57.45),\n",
       " (21.02, 30.04),\n",
       " (89.98, 152.45),\n",
       " (24.55, 35.72),\n",
       " (84.37, 141.88),\n",
       " (65.46, 106.86),\n",
       " (40.35, 62.24),\n",
       " (76.18, 126.59),\n",
       " (47.03, 73.85),\n",
       " (34.34, 51.97),\n",
       " (28.96, 42.97),\n",
       " (47.93, 75.42),\n",
       " (32.54, 48.93),\n",
       " (44.64, 69.66),\n",
       " (31.11, 46.53),\n",
       " (68.5, 112.42),\n",
       " (35.98, 54.74),\n",
       " (53.24, 84.82),\n",
       " (37.72, 57.71),\n",
       " (25.8, 37.77),\n",
       " (91.32, 155.0),\n",
       " (21.19, 30.31),\n",
       " (30.98, 46.33),\n",
       " (25.21, 36.79),\n",
       " (39.41, 60.61),\n",
       " (33.26, 50.15),\n",
       " (27.76, 40.97),\n",
       " (46.33, 72.62),\n",
       " (130.15, 230.26),\n",
       " (107.35, 185.68),\n",
       " (31.72, 47.56),\n",
       " (43.27, 67.28),\n",
       " (45.61, 71.37),\n",
       " (26.28, 38.55),\n",
       " (39.84, 61.36),\n",
       " (86.65, 146.16),\n",
       " (44.75, 69.86),\n",
       " (35.56, 54.04),\n",
       " (58.15, 93.62),\n",
       " (72.72, 120.17),\n",
       " (72.61, 119.98),\n",
       " (55.54, 88.92),\n",
       " (63.76, 103.76),\n",
       " (23.63, 34.24),\n",
       " (85.4, 143.82),\n",
       " (91.05, 154.49),\n",
       " (22.35, 32.16),\n",
       " (46.12, 72.25),\n",
       " (32.22, 48.4),\n",
       " (85.4, 143.81),\n",
       " (26.6, 39.07),\n",
       " (31.38, 46.99),\n",
       " (46.78, 73.42),\n",
       " (34.65, 52.5),\n",
       " (41.96, 65.02),\n",
       " (90.72, 153.86),\n",
       " (45.52, 71.2),\n",
       " (29.02, 43.07),\n",
       " (27.21, 40.08),\n",
       " (24.81, 36.15),\n",
       " (43.99, 68.53),\n",
       " (127.5, 225.04),\n",
       " (42.05, 65.17),\n",
       " (40.02, 61.67),\n",
       " (30.88, 46.16),\n",
       " (67.72, 110.98),\n",
       " (26.6, 39.08),\n",
       " (50.87, 80.61),\n",
       " (29.46, 43.79),\n",
       " (38.47, 59.0),\n",
       " (51.85, 82.36),\n",
       " (58.9, 94.96),\n",
       " (60.3, 97.48),\n",
       " (61.81, 100.21),\n",
       " (29.2, 43.36),\n",
       " (52.35, 83.23),\n",
       " (43.23, 67.22),\n",
       " (67.61, 110.79),\n",
       " (57.75, 92.9),\n",
       " (44.88, 70.09),\n",
       " (63.4, 103.1),\n",
       " (26.25, 38.49),\n",
       " (79.48, 132.72),\n",
       " (74.76, 123.95),\n",
       " (60.03, 96.99),\n",
       " (62.79, 101.99),\n",
       " (73.41, 121.45),\n",
       " (30.79, 46.01),\n",
       " (60.11, 97.15),\n",
       " (25.38, 37.08),\n",
       " (60.69, 98.19),\n",
       " (32.6, 49.04),\n",
       " (118.36, 207.09),\n",
       " (42.11, 65.28),\n",
       " (37.68, 57.64),\n",
       " (41.91, 64.93),\n",
       " (62.94, 102.26),\n",
       " (89.87, 152.25),\n",
       " (54.59, 87.23),\n",
       " (33.89, 51.21),\n",
       " (143.68, 257.16),\n",
       " (29.67, 44.14),\n",
       " (32.52, 48.9),\n",
       " (71.94, 118.73),\n",
       " (27.27, 40.17),\n",
       " (47.16, 74.08),\n",
       " (28.88, 42.82),\n",
       " (48.19, 75.89),\n",
       " (48.8, 76.96),\n",
       " (63.07, 102.51),\n",
       " (87.92, 148.56),\n",
       " (23.88, 34.64),\n",
       " (55.23, 88.38),\n",
       " (28.5, 42.2),\n",
       " (73.07, 120.83),\n",
       " (28.13, 41.58),\n",
       " (16.7, 23.23),\n",
       " (26.21, 38.43),\n",
       " (65.51, 106.94),\n",
       " (67.23, 110.09),\n",
       " (57.97, 93.29),\n",
       " (67.17, 109.97),\n",
       " (42.04, 65.16),\n",
       " (36.66, 55.91),\n",
       " (41.28, 63.84),\n",
       " (45.63, 71.41),\n",
       " (49.43, 78.08),\n",
       " (81.93, 137.3),\n",
       " (48.77, 76.91),\n",
       " (40.11, 61.82),\n",
       " (60.49, 97.83),\n",
       " (58.91, 94.99),\n",
       " (51.5, 81.73),\n",
       " (101.46, 174.33),\n",
       " (35.88, 54.59),\n",
       " (28.74, 42.59),\n",
       " (30.7, 45.85),\n",
       " (48.51, 76.45),\n",
       " (49.38, 77.99),\n",
       " (40.27, 62.09),\n",
       " (41.38, 64.01),\n",
       " (54.29, 86.69),\n",
       " (61.6, 99.84),\n",
       " (77.84, 129.66),\n",
       " (51.52, 81.76),\n",
       " (51.48, 81.71),\n",
       " (56.94, 91.44),\n",
       " (42.57, 66.07),\n",
       " (32.88, 49.51),\n",
       " (32.01, 48.05),\n",
       " (28.12, 41.57),\n",
       " (31.05, 46.44),\n",
       " (50.27, 79.56),\n",
       " (31.91, 47.87),\n",
       " (30.94, 46.26),\n",
       " (28.95, 42.94),\n",
       " (50.19, 79.42),\n",
       " (34.56, 52.35),\n",
       " (66.2, 108.2),\n",
       " (35.39, 53.75),\n",
       " (22.13, 31.82),\n",
       " (42.53, 65.99),\n",
       " (43.83, 68.26),\n",
       " (61.9, 100.37),\n",
       " (97.02, 165.84),\n",
       " (79.1, 132.0),\n",
       " (95.48, 162.91),\n",
       " (42.39, 65.76),\n",
       " (64.7, 105.46),\n",
       " (51.08, 80.99),\n",
       " (25.72, 37.63),\n",
       " (58.06, 93.44),\n",
       " (41.2, 63.7),\n",
       " (89.2, 150.98),\n",
       " (52.59, 83.67),\n",
       " (39.9, 61.46),\n",
       " (32.81, 49.4),\n",
       " (24.85, 36.22),\n",
       " (55.37, 88.62),\n",
       " (49.05, 77.4),\n",
       " (30.07, 44.8),\n",
       " (54.9, 87.78),\n",
       " (115.35, 201.21),\n",
       " (43.93, 68.44),\n",
       " (72.09, 119.01),\n",
       " (26.08, 38.21),\n",
       " (35.38, 53.74),\n",
       " (47.93, 75.42),\n",
       " (26.4, 38.75),\n",
       " (58.69, 94.59),\n",
       " (28.0, 41.37),\n",
       " (49.45, 78.12),\n",
       " (31.79, 47.68),\n",
       " (45.61, 71.36),\n",
       " (60.97, 98.69),\n",
       " (34.5, 52.24),\n",
       " (60.18, 97.26),\n",
       " (55.32, 88.54),\n",
       " (53.54, 85.36),\n",
       " (66.72, 109.15),\n",
       " (86.17, 145.26),\n",
       " (44.74, 69.84),\n",
       " (89.11, 150.81),\n",
       " (63.55, 103.37),\n",
       " (85.09, 143.23),\n",
       " (31.36, 46.96),\n",
       " (27.29, 40.2),\n",
       " (25.72, 37.64),\n",
       " (48.53, 76.49),\n",
       " (68.07, 111.62),\n",
       " (37.58, 57.49),\n",
       " (65.01, 106.03),\n",
       " (61.84, 100.28),\n",
       " (31.88, 47.82),\n",
       " (103.66, 178.57),\n",
       " (39.51, 60.78),\n",
       " (47.18, 74.11),\n",
       " (52.02, 82.66),\n",
       " (25.91, 37.94),\n",
       " (56.21, 90.12),\n",
       " (71.91, 118.67),\n",
       " (45.15, 70.56),\n",
       " (24.65, 35.88),\n",
       " (67.02, 109.71),\n",
       " (60.91, 98.59),\n",
       " (30.77, 45.97),\n",
       " (25.93, 37.98),\n",
       " (44.15, 68.82),\n",
       " (31.11, 46.55),\n",
       " (42.48, 65.91),\n",
       " (40.17, 61.92),\n",
       " (125.92, 221.92),\n",
       " (27.05, 39.8),\n",
       " (27.32, 40.26),\n",
       " (51.25, 81.28),\n",
       " (31.93, 47.91),\n",
       " (26.29, 38.56),\n",
       " (48.67, 76.73),\n",
       " (55.47, 88.81),\n",
       " (77.82, 129.63),\n",
       " (67.44, 110.48),\n",
       " (47.93, 75.44),\n",
       " (27.11, 39.91),\n",
       " (83.31, 139.88),\n",
       " (22.31, 32.1),\n",
       " (24.32, 35.35),\n",
       " (44.89, 70.1),\n",
       " (56.28, 90.26),\n",
       " (82.11, 137.63),\n",
       " (29.56, 43.96),\n",
       " (32.09, 48.18),\n",
       " (31.09, 46.5),\n",
       " (33.7, 50.89),\n",
       " (32.34, 48.61),\n",
       " (53.17, 84.7),\n",
       " (40.76, 62.93),\n",
       " (57.01, 91.56),\n",
       " (30.92, 46.23),\n",
       " (53.89, 85.99),\n",
       " (70.63, 116.32),\n",
       " (32.93, 49.6),\n",
       " (25.64, 37.49),\n",
       " (45.15, 70.56),\n",
       " (68.21, 111.88),\n",
       " (22.29, 32.06),\n",
       " (43.42, 67.54),\n",
       " (56.54, 90.73),\n",
       " (30.64, 45.75),\n",
       " (39.89, 61.44),\n",
       " (26.41, 38.76),\n",
       " (62.9, 102.19),\n",
       " (63.21, 102.75),\n",
       " (102.08, 175.54),\n",
       " (35.31, 53.61),\n",
       " (29.01, 43.05),\n",
       " (47.14, 74.05),\n",
       " (57.04, 91.61),\n",
       " (28.99, 43.01),\n",
       " (25.54, 37.34),\n",
       " (54.83, 87.66),\n",
       " (88.34, 149.36),\n",
       " (55.54, 88.92),\n",
       " (30.07, 44.81),\n",
       " (27.96, 41.31),\n",
       " (32.69, 49.19),\n",
       " (109.0, 188.88),\n",
       " (46.64, 73.17),\n",
       " (61.14, 99.01),\n",
       " (55.23, 88.38),\n",
       " (26.92, 39.6),\n",
       " (29.14, 43.26),\n",
       " (112.48, 195.63),\n",
       " (18.51, 26.05),\n",
       " (57.73, 92.86),\n",
       " (43.69, 68.02),\n",
       " (31.39, 47.02),\n",
       " (47.6, 74.85),\n",
       " (36.61, 55.82),\n",
       " (28.0, 41.37),\n",
       " (34.86, 52.86),\n",
       " (30.7, 45.86),\n",
       " (61.38, 99.44),\n",
       " (73.42, 121.46),\n",
       " (76.06, 126.36),\n",
       " (25.36, 37.04),\n",
       " (51.09, 81.01),\n",
       " (18.46, 25.98),\n",
       " (57.14, 91.79),\n",
       " (48.16, 75.84),\n",
       " (92.53, 157.29),\n",
       " (29.71, 44.2),\n",
       " (18.76, 26.45),\n",
       " (26.93, 39.62),\n",
       " (62.72, 101.87),\n",
       " (39.82, 61.32),\n",
       " (39.23, 60.31),\n",
       " (38.28, 58.67),\n",
       " (79.73, 133.19),\n",
       " (29.42, 43.73),\n",
       " (43.36, 67.43),\n",
       " (45.81, 71.71),\n",
       " (54.83, 87.65),\n",
       " (45.02, 70.34),\n",
       " (43.22, 67.2),\n",
       " (53.03, 84.45),\n",
       " (27.36, 40.33),\n",
       " (25.2, 36.79),\n",
       " (42.45, 65.87),\n",
       " (28.15, 41.63),\n",
       " (73.42, 121.47),\n",
       " (54.56, 87.17),\n",
       " (48.12, 75.77),\n",
       " (28.82, 42.72),\n",
       " (66.96, 109.6),\n",
       " (41.68, 64.53),\n",
       " (18.24, 25.63),\n",
       " (85.47, 143.95),\n",
       " (23.32, 33.73),\n",
       " (26.79, 39.38),\n",
       " (46.63, 73.15),\n",
       " (60.96, 98.69),\n",
       " (45.26, 70.75),\n",
       " (44.02, 68.59),\n",
       " (36.3, 55.29),\n",
       " (30.91, 46.2),\n",
       " (118.52, 207.39),\n",
       " (93.05, 158.27),\n",
       " (61.78, 100.16),\n",
       " (45.68, 71.48),\n",
       " (47.91, 75.39),\n",
       " (29.08, 43.16),\n",
       " (34.69, 52.57),\n",
       " (50.64, 80.22),\n",
       " (72.67, 120.07),\n",
       " (58.52, 94.28),\n",
       " (20.7, 29.52),\n",
       " (33.43, 50.43),\n",
       " (77.04, 128.18),\n",
       " (23.94, 34.74),\n",
       " (20.73, 29.57),\n",
       " (31.54, 47.26),\n",
       " (52.21, 82.99),\n",
       " (76.03, 126.3),\n",
       " (72.96, 120.61),\n",
       " (39.7, 61.11),\n",
       " (28.38, 42.01),\n",
       " (113.31, 197.23),\n",
       " (25.23, 36.84),\n",
       " (35.72, 54.31),\n",
       " (18.25, 25.65),\n",
       " (38.66, 59.34),\n",
       " (52.68, 83.83),\n",
       " (75.3, 124.94),\n",
       " (34.66, 52.51),\n",
       " (28.07, 41.49),\n",
       " (45.62, 71.37),\n",
       " (24.88, 36.26),\n",
       " (31.89, 47.85),\n",
       " (117.31, 205.04),\n",
       " (51.93, 82.49),\n",
       " (34.95, 53.01),\n",
       " (31.43, 47.07),\n",
       " (26.6, 39.07),\n",
       " (128.54, 227.09),\n",
       " (30.01, 44.7),\n",
       " (29.72, 44.22),\n",
       " (46.11, 72.24),\n",
       " (39.37, 60.55),\n",
       " (53.63, 85.52),\n",
       " (65.87, 107.6),\n",
       " (47.06, 73.9),\n",
       " (85.56, 144.11),\n",
       " (42.47, 65.89),\n",
       " (27.15, 39.98),\n",
       " (23.36, 33.79),\n",
       " (30.14, 44.93),\n",
       " (66.68, 109.08),\n",
       " (49.38, 77.99),\n",
       " (38.65, 59.31),\n",
       " (118.39, 207.15),\n",
       " (32.13, 48.26),\n",
       " (56.82, 91.22),\n",
       " (33.99, 51.38),\n",
       " (58.27, 93.82),\n",
       " (29.02, 43.06),\n",
       " (38.54, 59.13),\n",
       " (73.12, 120.92),\n",
       " (23.98, 34.8),\n",
       " (27.85, 41.13),\n",
       " (31.95, 47.96),\n",
       " (72.82, 120.36),\n",
       " (25.37, 37.05),\n",
       " (51.72, 82.13),\n",
       " (59.51, 96.06),\n",
       " (29.39, 43.67),\n",
       " (61.34, 99.37),\n",
       " (24.39, 35.46),\n",
       " (59.32, 95.71),\n",
       " (68.75, 112.86),\n",
       " (70.57, 116.21),\n",
       " (23.84, 34.57),\n",
       " (48.26, 76.01),\n",
       " (30.89, 46.18),\n",
       " (114.05, 198.69),\n",
       " (25.4, 37.1),\n",
       " (29.46, 43.79),\n",
       " (54.72, 87.47),\n",
       " (48.17, 75.85),\n",
       " (27.81, 41.06),\n",
       " (43.72, 68.06),\n",
       " (61.25, 99.2),\n",
       " (50.3, 79.61),\n",
       " (91.05, 154.49),\n",
       " (67.32, 110.24),\n",
       " (21.21, 30.34),\n",
       " (39.73, 61.17),\n",
       " (48.56, 76.53),\n",
       " (41.32, 63.91),\n",
       " (44.9, 70.13),\n",
       " (43.2, 67.17),\n",
       " (28.34, 41.94),\n",
       " (27.4, 40.38),\n",
       " (41.97, 65.03),\n",
       " (38.6, 59.22),\n",
       " (32.55, 48.96),\n",
       " (154.01, 277.9),\n",
       " (43.7, 68.03),\n",
       " (52.88, 84.19),\n",
       " (48.01, 75.57),\n",
       " (25.63, 37.48),\n",
       " (44.73, 69.83),\n",
       " (38.36, 58.81),\n",
       " (23.34, 33.77),\n",
       " (79.43, 132.62),\n",
       " (75.05, 124.48),\n",
       " (35.96, 54.72),\n",
       " (48.88, 77.1),\n",
       " (49.19, 77.64),\n",
       " (45.51, 71.19),\n",
       " (36.9, 56.33),\n",
       " (56.74, 91.08),\n",
       " (47.94, 75.45),\n",
       " (35.33, 53.65),\n",
       " (41.75, 64.66),\n",
       " (57.27, 92.03),\n",
       " (61.41, 99.5),\n",
       " (18.49, 26.02),\n",
       " (26.05, 38.17),\n",
       " (38.81, 59.58),\n",
       " (33.59, 50.7),\n",
       " (71.21, 117.4),\n",
       " (41.06, 63.45),\n",
       " (56.8, 91.19),\n",
       " (39.48, 60.73),\n",
       " (63.39, 103.09),\n",
       " (44.75, 69.86),\n",
       " (87.77, 148.28),\n",
       " (61.15, 99.03),\n",
       " (48.11, 75.74),\n",
       " (54.38, 86.86),\n",
       " (37.37, 57.12),\n",
       " (57.73, 92.86),\n",
       " (41.12, 63.56),\n",
       " (25.2, 36.78),\n",
       " (54.77, 87.55),\n",
       " (54.73, 87.49),\n",
       " (28.39, 42.01),\n",
       " (44.89, 70.1),\n",
       " (27.09, 39.88),\n",
       " (29.14, 43.26),\n",
       " (52.01, 82.64),\n",
       " (38.82, 59.59),\n",
       " (31.45, 47.1),\n",
       " (50.81, 80.51),\n",
       " (32.65, 49.11),\n",
       " (58.46, 94.18),\n",
       " (39.15, 60.16),\n",
       " (22.7, 32.74),\n",
       " (41.38, 64.01),\n",
       " (57.97, 93.29),\n",
       " (37.11, 56.69),\n",
       " (54.23, 86.58),\n",
       " (55.0, 87.96),\n",
       " (61.62, 99.87),\n",
       " (46.06, 72.16),\n",
       " (29.04, 43.09),\n",
       " (21.41, 30.65),\n",
       " (54.77, 87.56),\n",
       " (59.27, 95.63),\n",
       " (18.91, 26.69),\n",
       " (60.18, 97.26),\n",
       " (35.16, 53.36),\n",
       " (60.41, 97.69),\n",
       " (29.83, 44.41),\n",
       " (33.37, 50.33),\n",
       " (43.67, 67.99),\n",
       " (39.95, 61.54),\n",
       " (28.52, 42.24),\n",
       " (22.64, 32.63),\n",
       " (31.03, 46.41),\n",
       " (64.75, 105.55),\n",
       " (39.56, 60.88),\n",
       " (39.3, 60.42),\n",
       " (28.87, 42.82),\n",
       " (42.56, 66.05),\n",
       " (67.9, 111.31),\n",
       " (92.66, 157.53),\n",
       " (50.0, 79.07),\n",
       " (37.69, 57.66),\n",
       " (33.67, 50.85),\n",
       " (27.49, 40.53),\n",
       " (48.49, 76.42),\n",
       " (46.48, 72.89),\n",
       " (35.98, 54.76),\n",
       " (27.38, 40.35),\n",
       " (49.9, 78.9),\n",
       " (41.87, 64.86),\n",
       " (59.49, 96.02),\n",
       " (40.11, 61.81),\n",
       " (31.74, 47.6),\n",
       " (28.8, 42.69),\n",
       " (48.44, 76.33),\n",
       " (29.53, 43.9),\n",
       " (46.7, 73.26),\n",
       " (64.86, 105.75),\n",
       " (69.29, 113.86),\n",
       " (23.31, 33.71),\n",
       " (21.35, 30.57),\n",
       " (33.45, 50.48),\n",
       " (52.69, 83.84),\n",
       " (24.78, 36.1),\n",
       " (50.31, 79.63),\n",
       " (24.63, 35.85),\n",
       " (43.22, 67.2),\n",
       " (35.84, 54.52),\n",
       " (68.19, 111.84),\n",
       " (31.67, 47.48),\n",
       " (51.37, 81.5),\n",
       " (24.5, 35.64),\n",
       " (28.31, 41.88),\n",
       " (48.28, 76.05),\n",
       " (52.64, 83.76),\n",
       " (78.28, 130.49),\n",
       " (135.9, 241.66),\n",
       " (35.95, 54.7),\n",
       " (43.69, 68.02),\n",
       " (30.87, 46.14),\n",
       " (34.83, 52.81),\n",
       " (107.96, 186.86),\n",
       " (28.24, 41.77),\n",
       " (55.55, 88.94),\n",
       " (60.26, 97.42),\n",
       " (37.29, 56.99),\n",
       " (28.48, 42.17),\n",
       " (26.86, 39.49),\n",
       " (30.45, 45.43),\n",
       " (21.3, 30.48),\n",
       " (60.45, 97.75),\n",
       " (40.22, 62.01),\n",
       " (38.0, 58.2),\n",
       " (36.08, 54.92),\n",
       " (21.62, 31.0),\n",
       " (43.72, 68.06),\n",
       " (36.5, 55.64),\n",
       " (26.76, 39.33),\n",
       " (53.43, 85.16),\n",
       " (34.53, 52.3),\n",
       " (41.2, 63.69),\n",
       " (32.93, 49.59),\n",
       " (22.26, 32.02),\n",
       " (36.9, 56.31),\n",
       " (45.33, 70.87),\n",
       " (42.94, 66.71),\n",
       " (57.38, 92.22),\n",
       " (55.51, 88.87),\n",
       " (29.46, 43.79),\n",
       " (30.08, 44.83),\n",
       " (55.36, 88.61),\n",
       " (46.55, 73.02),\n",
       " (36.8, 56.14),\n",
       " (34.57, 52.35),\n",
       " (55.63, 89.09),\n",
       " (91.62, 155.56),\n",
       " (66.35, 108.48),\n",
       " (34.39, 52.05),\n",
       " (50.04, 79.14),\n",
       " (22.29, 32.07),\n",
       " (24.26, 35.25),\n",
       " (34.99, 53.07),\n",
       " (29.05, 43.1),\n",
       " (54.3, 86.71),\n",
       " (67.81, 111.14),\n",
       " (106.9, 184.81),\n",
       " (59.9, 96.76),\n",
       " (45.55, 71.27),\n",
       " (50.15, 79.35),\n",
       " (58.3, 93.88),\n",
       " (46.96, 73.72),\n",
       " (29.13, 43.25),\n",
       " (36.15, 55.05),\n",
       " (119.2, 208.73),\n",
       " (37.21, 56.85),\n",
       " (30.42, 45.39),\n",
       " (63.75, 103.73),\n",
       " (14.61, 20.0),\n",
       " (30.27, 45.13),\n",
       " (75.45, 125.23),\n",
       " (71.44, 117.81),\n",
       " (60.11, 97.15),\n",
       " (14.24, 19.45),\n",
       " (35.56, 54.04),\n",
       " (19.08, 26.95),\n",
       " (29.39, 43.67),\n",
       " (79.22, 132.23),\n",
       " (108.42, 187.75),\n",
       " (21.24, 30.38),\n",
       " (31.95, 47.95),\n",
       " (33.71, 50.91),\n",
       " (41.77, 64.69),\n",
       " (37.26, 56.93),\n",
       " (71.86, 118.58),\n",
       " (46.31, 72.59),\n",
       " (30.91, 46.21),\n",
       " (31.11, 46.54),\n",
       " (134.15, 238.19),\n",
       " (94.34, 160.73),\n",
       " (59.0, 95.15),\n",
       " (62.12, 100.78),\n",
       " (24.37, 35.43),\n",
       " (37.89, 58.0),\n",
       " (39.06, 60.02),\n",
       " (38.32, 58.74),\n",
       " (49.38, 77.99),\n",
       " (30.81, 46.05),\n",
       " (56.54, 90.72),\n",
       " (48.71, 76.81),\n",
       " (52.01, 82.64),\n",
       " (45.4, 70.99),\n",
       " (46.09, 72.19),\n",
       " (25.55, 37.35),\n",
       " (58.23, 93.76),\n",
       " (36.84, 56.22),\n",
       " (41.36, 63.98),\n",
       " (34.49, 52.23),\n",
       " (29.77, 44.31),\n",
       " (30.69, 45.84),\n",
       " (76.22, 126.66),\n",
       " (20.71, 29.54),\n",
       " (31.08, 46.49),\n",
       " (29.78, 44.33),\n",
       " (47.14, 74.04),\n",
       " (32.4, 48.7),\n",
       " (22.35, 32.16),\n",
       " (89.68, 151.89),\n",
       " (42.03, 65.13),\n",
       " (76.61, 127.38),\n",
       " (45.64, 71.41),\n",
       " (36.19, 55.12),\n",
       " (45.22, 70.68),\n",
       " (59.7, 96.39),\n",
       " (25.29, 36.92),\n",
       " (62.71, 101.85),\n",
       " (51.41, 81.57),\n",
       " (55.71, 89.23),\n",
       " (26.29, 38.57),\n",
       " (46.3, 72.57),\n",
       " (26.29, 38.56),\n",
       " (33.69, 50.87),\n",
       " (30.71, 45.87),\n",
       " (68.51, 112.42),\n",
       " (49.56, 78.3),\n",
       " (20.94, 29.9),\n",
       " (22.44, 32.31),\n",
       " (39.85, 61.37),\n",
       " (27.83, 41.1),\n",
       " (39.66, 61.05),\n",
       " (23.77, 34.46),\n",
       " (49.4, 78.03),\n",
       " (44.72, 69.8),\n",
       " (29.95, 44.6),\n",
       " (45.88, 71.83),\n",
       " (79.13, 132.08),\n",
       " (47.76, 75.13),\n",
       " (27.49, 40.53),\n",
       " (43.34, 67.41),\n",
       " (33.06, 49.81),\n",
       " (43.7, 68.03),\n",
       " (52.88, 84.18),\n",
       " (31.32, 46.9),\n",
       " (45.32, 70.86),\n",
       " (67.17, 109.97),\n",
       " (21.33, 30.53),\n",
       " (23.37, 33.81),\n",
       " (53.65, 85.56),\n",
       " (87.99, 148.7),\n",
       " (73.08, 120.83),\n",
       " (50.91, 80.68),\n",
       " (50.61, 80.15),\n",
       " (52.33, 83.21),\n",
       " (61.19, 99.09),\n",
       " (35.26, 53.53),\n",
       " (50.62, 80.17),\n",
       " (45.08, 70.44),\n",
       " (38.71, 59.41),\n",
       " (31.63, 47.42),\n",
       " (54.32, 86.76),\n",
       " (106.53, 184.1),\n",
       " (39.77, 61.23),\n",
       " (29.25, 43.44),\n",
       " (58.45, 94.15),\n",
       " (37.09, 56.65),\n",
       " (23.31, 33.71),\n",
       " (34.01, 51.41),\n",
       " (51.55, 81.82),\n",
       " (27.59, 40.7),\n",
       " (49.98, 79.04),\n",
       " (21.48, 30.78),\n",
       " (60.93, 98.63),\n",
       " (49.55, 78.28),\n",
       " (55.61, 89.06),\n",
       " (65.42, 106.77),\n",
       " (47.59, 74.83),\n",
       " (53.6, 85.46),\n",
       " (27.27, 40.17),\n",
       " (29.46, 43.79),\n",
       " (34.31, 51.92),\n",
       " (76.24, 126.7),\n",
       " (32.98, 49.67),\n",
       " (76.13, 126.48),\n",
       " (34.43, 52.13),\n",
       " (62.45, 101.38),\n",
       " (46.06, 72.16),\n",
       " (46.27, 72.52),\n",
       " (22.02, 31.63),\n",
       " (57.45, 92.36),\n",
       " (55.72, 89.24),\n",
       " (28.44, 42.11),\n",
       " (21.03, 30.05),\n",
       " (26.02, 38.12),\n",
       " (41.9, 64.92),\n",
       " (94.91, 161.82),\n",
       " (56.85, 91.28),\n",
       " (74.82, 124.06),\n",
       " (67.77, 111.07),\n",
       " (26.58, 39.03),\n",
       " (26.57, 39.02),\n",
       " (70.26, 115.65),\n",
       " (32.46, 48.81),\n",
       " (31.25, 46.77),\n",
       " (16.78, 23.35),\n",
       " (57.25, 92.0),\n",
       " (29.46, 43.79),\n",
       " (69.66, 114.54),\n",
       " (45.92, 71.9),\n",
       " (119.07, 208.47),\n",
       " (38.15, 58.46),\n",
       " (58.63, 94.47),\n",
       " (51.67, 82.04),\n",
       " (29.41, 43.7),\n",
       " (21.3, 30.48),\n",
       " (60.17, 97.25),\n",
       " (73.89, 122.34),\n",
       " (27.11, 39.91),\n",
       " (41.22, 63.74),\n",
       " (40.17, 61.93),\n",
       " (24.17, 35.11),\n",
       " (45.52, 71.2),\n",
       " (49.29, 77.82),\n",
       " (29.7, 44.19),\n",
       " (46.06, 72.15),\n",
       " (98.8, 169.25),\n",
       " (29.61, 44.05),\n",
       " (22.95, 33.13),\n",
       " (28.69, 42.52),\n",
       " (57.55, 92.53),\n",
       " (63.12, 102.59),\n",
       " (32.19, 48.35),\n",
       " (77.91, 129.8),\n",
       " (62.1, 100.75),\n",
       " (20.97, 29.96),\n",
       " (28.73, 42.58),\n",
       " (37.02, 56.52),\n",
       " (65.88, 107.61),\n",
       " (116.96, 204.36),\n",
       " (47.45, 74.59),\n",
       " (58.01, 93.36),\n",
       " (29.11, 43.21),\n",
       " (34.55, 52.32),\n",
       " (69.6, 114.43),\n",
       " (25.65, 37.52),\n",
       " (30.82, 46.05),\n",
       " (24.43, 35.52),\n",
       " (32.21, 48.39),\n",
       " (23.05, 33.29),\n",
       " (63.69, 103.63),\n",
       " (71.83, 118.53),\n",
       " (78.43, 130.76),\n",
       " (52.8, 84.05),\n",
       " (52.17, 82.92),\n",
       " (75.31, 124.96),\n",
       " (35.13, 53.31),\n",
       " (25.64, 37.49),\n",
       " (116.26, 202.99),\n",
       " (24.65, 35.88),\n",
       " (42.99, 66.8),\n",
       " (28.58, 42.33),\n",
       " (84.26, 141.68),\n",
       " (36.62, 55.83),\n",
       " (68.32, 112.07),\n",
       " (26.5, 38.91),\n",
       " (63.62, 103.5),\n",
       " (130.92, 231.78),\n",
       " (43.51, 67.71),\n",
       " (44.37, 69.2),\n",
       " (25.81, 37.77),\n",
       " (39.17, 60.2),\n",
       " (80.32, 134.29),\n",
       " (28.08, 41.5),\n",
       " (57.43, 92.32),\n",
       " (72.58, 119.92),\n",
       " (25.95, 38.0),\n",
       " (24.97, 36.4),\n",
       " (25.19, 36.76),\n",
       " (31.8, 47.69),\n",
       " (27.95, 41.3),\n",
       " (65.93, 107.71),\n",
       " (37.33, 57.05),\n",
       " (33.51, 50.57),\n",
       " (57.12, 91.77),\n",
       " (39.27, 60.37),\n",
       " (45.2, 70.64),\n",
       " (26.8, 39.4),\n",
       " (27.12, 39.93),\n",
       " (76.06, 126.35),\n",
       " (28.09, 41.53),\n",
       " (35.7, 54.28),\n",
       " (31.95, 47.96),\n",
       " (62.54, 101.54),\n",
       " (17.2, 24.0),\n",
       " (67.81, 111.14),\n",
       " (33.03, 49.77),\n",
       " (45.51, 71.19),\n",
       " (22.94, 33.11),\n",
       " (64.78, 105.62),\n",
       " (25.62, 37.47),\n",
       " (62.76, 101.93),\n",
       " (26.22, 38.44),\n",
       " (20.36, 28.99),\n",
       " (38.04, 58.26),\n",
       " (31.67, 47.48),\n",
       " (27.34, 40.29),\n",
       " (62.02, 100.6),\n",
       " (60.56, 97.96),\n",
       " (40.39, 62.3),\n",
       " (44.31, 69.09),\n",
       " (47.85, 75.28),\n",
       " (46.24, 72.47),\n",
       " (28.75, 42.61),\n",
       " (38.97, 59.86),\n",
       " (49.72, 78.58),\n",
       " (76.73, 127.61),\n",
       " (29.02, 43.07),\n",
       " (41.84, 64.8),\n",
       " (52.44, 83.4),\n",
       " (114.97, 200.46),\n",
       " (47.88, 75.34),\n",
       " (98.58, 168.82),\n",
       " (35.92, 54.65),\n",
       " (49.15, 77.59),\n",
       " (32.58, 49.01),\n",
       " ...)"
      ]
     },
     "execution_count": 1465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate price interval for MAPE median\n",
    "y_pred_interval_xgb_reg = tuple([(round(math.exp(el-el*MAPE_median_xgb_reg),2),round(math.exp(el+el*MAPE_median_xgb_reg),2)) for el in y_test_pred_xgb_reg])\n",
    "y_pred_interval_xgb_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model and Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1466,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model and cv\n",
    "save_model(best_model_xgb_reg, title=\"best_model_xgb_reg_01\", save=\"joblib\")\n",
    "save_model(grid_xgb_reg, title=\"best_cv_xgb_reg_01\", save=\"joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg Model 2: Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1473,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create pipeline to use in RandomizedSearchCV and GridSearchCV\n",
    "pipeline_svm_reg = Pipeline([('preprocessor', preprocessor),\n",
    "                             ('svm_reg',\n",
    "                              SVR(kernel='rbf',\n",
    "                                  C=10,\n",
    "                                  degree=1,\n",
    "                                  gamma=0.001,\n",
    "                                  epsilon=0.1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Pre-Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1472,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'cache_size', 'coef0', 'degree', 'epsilon', 'gamma', 'kernel', 'max_iter', 'shrinking', 'tol', 'verbose'])"
      ]
     },
     "execution_count": 1472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display possible hyperparameters for Support Vector Machine\n",
    "test_svr_reg = SVR()\n",
    "test_svr_reg.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default values for Support Vector Machine** (as base for hyperparameter search):\n",
    "\n",
    "kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1471,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter distribution\n",
    "param_distribs_svm_reg = {\n",
    "#    'svm_reg__kernel': ['linear', 'poly', 'rbf'],\n",
    "    'svm_reg__C': [0.1, 0.5, 0.8, 1, 1.5, 2, 3, 5, 10, 50, 100],        # initial: [0.1, 0.5, 1, 2, 5, 10, 50, 100, 500, 1000]\n",
    "    'svm_reg__gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1],   # initial: [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1]\n",
    "    'svm_reg__epsilon': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.9],            # initial: [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.9]\n",
    "    'svm_reg__degree': randint(low=1, high=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1474,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   54.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   54.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Create and fit RandomizedSearchCV, save \"best_model\"\n",
    "rnd_svm_reg = RandomizedSearchCV(pipeline_svm_reg,\n",
    "                                 param_distribs_svm_reg,\n",
    "                                 cv=5,\n",
    "                                 scoring=scoring,\n",
    "                                 n_iter=5,\n",
    "                                 return_train_score=True,\n",
    "                                 verbose=4,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "best_model_rnd_svm_reg = rnd_svm_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1470,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "-0.24\n",
      "Best parameters:\n",
      "{'svm_reg__C': 1, 'svm_reg__degree': 3, 'svm_reg__epsilon': 0.2, 'svm_reg__gamma': 0.001, 'svm_reg__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(rnd_svm_reg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(rnd_svm_reg.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(rnd_svm_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid_svm_reg = {\n",
    "#    'svm_reg__kernel': ['linear', 'poly', 'rbf'],\n",
    "    'svm_reg__gamma': [0.0015, 0.002, 0.003, 0.005],\n",
    "    'svm_reg__C': [10, 12, 14, 16],\n",
    "#    'svm_reg__degree': [1, 3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:  3.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Create and fit GridSearchCV, save \"best_model\"\n",
    "grid_svm_reg = GridSearchCV(pipeline_svm_reg,\n",
    "                            param_grid_svm_reg,\n",
    "                            cv=5,\n",
    "                            scoring=scoring,\n",
    "                            return_train_score=True,\n",
    "                            verbose=4,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "grid_svm_reg = grid_svm_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_svm_reg = grid_svm_reg.best_estimator_[\"svm_reg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "-0.23\n",
      "Best parameters:\n",
      "{'svm_reg__C': 10, 'svm_reg__gamma': 0.005}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(grid_svm_reg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_svm_reg.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_svm_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display feature importances\n",
    "#fi_svm_reg = get_feat_importances(best_model_svm_reg)\n",
    "#fi_svm_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load existing model\n",
    "#load_best_model = load_model(title=\"best_model_svm_reg_01\", dataset_loc=dataset_loc, dataset_date=dataset_date)\n",
    "#load_best_cv = load_model(title=\"best_cv_svm_reg_01\", dataset_loc=dataset_loc, dataset_date=dataset_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Curves (Overfitting)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_train_pred_svm_reg = best_model_svm_reg.predict(X_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.12\n",
      "RMSE: 0.35\n",
      "MAE: 0.26\n",
      "R2: 0.64\n",
      "MAPE: 6.57\n",
      "MAPE median: 4.71\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_train, y_train_pred_svm_reg, model=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_test_pred_svm_reg = best_model_svm_reg.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.14\n",
      "RMSE: 0.38\n",
      "MAE: 0.28\n",
      "R2: 0.56\n",
      "MAPE: 7.35\n",
      "MAPE median: 5.47\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_test, y_test_pred_svm_reg, model=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36155178, 0.39367148])"
      ]
     },
     "execution_count": 981,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display confidence interval (scipy stats)\n",
    "confidence = 0.95\n",
    "squared_errors = (y_test_pred_svm_reg - y_test)**2\n",
    "np.sqrt(\n",
    "    stats.t.interval(confidence,\n",
    "                     len(squared_errors) - 1,\n",
    "                     loc=squared_errors.mean(),\n",
    "                     scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Median Price Intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save MAPE_median as variable\n",
    "MAPE_median_svm_reg = (median_absolute_percentage_error(y_test, y_test_pred_svm_reg))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53.12, 84.14),\n",
       " (51.18, 80.72),\n",
       " (55.53, 88.41),\n",
       " (33.31, 50.0),\n",
       " (38.42, 58.62),\n",
       " (26.45, 38.64),\n",
       " (40.57, 62.29),\n",
       " (33.56, 50.4),\n",
       " (46.12, 71.88),\n",
       " (40.16, 61.58),\n",
       " (23.38, 33.68),\n",
       " (32.57, 48.75),\n",
       " (53.56, 84.93),\n",
       " (26.83, 39.27),\n",
       " (36.89, 56.02),\n",
       " (71.04, 116.39),\n",
       " (72.96, 119.9),\n",
       " (66.26, 107.68),\n",
       " (50.53, 79.57),\n",
       " (49.11, 77.09),\n",
       " (107.08, 183.96),\n",
       " (50.51, 79.55),\n",
       " (37.65, 57.3),\n",
       " (71.65, 117.49),\n",
       " (27.14, 39.78),\n",
       " (58.15, 93.08),\n",
       " (83.24, 138.9),\n",
       " (36.45, 55.27),\n",
       " (25.58, 37.24),\n",
       " (46.86, 73.16),\n",
       " (24.72, 35.84),\n",
       " (68.19, 111.18),\n",
       " (51.71, 81.65),\n",
       " (27.01, 39.56),\n",
       " (41.13, 63.25),\n",
       " (52.75, 83.48),\n",
       " (36.01, 54.53),\n",
       " (55.4, 88.18),\n",
       " (30.14, 44.7),\n",
       " (27.82, 40.9),\n",
       " (29.37, 43.44),\n",
       " (25.01, 36.3),\n",
       " (44.76, 69.51),\n",
       " (31.98, 47.77),\n",
       " (30.1, 44.64),\n",
       " (42.12, 64.94),\n",
       " (46.45, 72.44),\n",
       " (82.71, 137.9),\n",
       " (64.54, 104.56),\n",
       " (101.83, 173.92),\n",
       " (33.48, 50.27),\n",
       " (81.78, 136.18),\n",
       " (44.58, 69.2),\n",
       " (44.79, 69.56),\n",
       " (37.16, 56.48),\n",
       " (44.88, 69.71),\n",
       " (54.8, 87.13),\n",
       " (34.37, 51.76),\n",
       " (65.97, 107.15),\n",
       " (34.5, 51.99),\n",
       " (56.64, 90.39),\n",
       " (55.47, 88.3),\n",
       " (60.55, 97.38),\n",
       " (47.13, 73.62),\n",
       " (46.42, 72.4),\n",
       " (52.66, 83.33),\n",
       " (22.65, 32.51),\n",
       " (29.37, 43.44),\n",
       " (44.51, 69.07),\n",
       " (45.06, 70.03),\n",
       " (40.98, 63.0),\n",
       " (65.65, 106.58),\n",
       " (34.65, 52.24),\n",
       " (59.45, 95.41),\n",
       " (141.05, 250.16),\n",
       " (38.45, 58.68),\n",
       " (16.67, 23.09),\n",
       " (52.91, 83.77),\n",
       " (35.43, 53.55),\n",
       " (30.62, 45.51),\n",
       " (30.13, 44.69),\n",
       " (20.23, 28.66),\n",
       " (34.92, 52.7),\n",
       " (22.85, 32.82),\n",
       " (88.9, 149.48),\n",
       " (62.12, 100.19),\n",
       " (44.49, 69.04),\n",
       " (69.29, 113.18),\n",
       " (41.87, 64.52),\n",
       " (54.63, 86.82),\n",
       " (64.62, 104.71),\n",
       " (41.62, 64.09),\n",
       " (36.47, 55.31),\n",
       " (129.67, 227.75),\n",
       " (60.33, 96.97),\n",
       " (27.41, 40.21),\n",
       " (126.36, 221.28),\n",
       " (55.41, 88.19),\n",
       " (32.83, 49.18),\n",
       " (23.67, 34.15),\n",
       " (32.4, 48.47),\n",
       " (33.63, 50.52),\n",
       " (27.45, 40.28),\n",
       " (51.64, 81.54),\n",
       " (29.75, 44.07),\n",
       " (49.73, 78.18),\n",
       " (25.44, 37.0),\n",
       " (35.68, 53.97),\n",
       " (36.58, 55.49),\n",
       " (53.51, 84.83),\n",
       " (39.6, 60.62),\n",
       " (24.35, 35.24),\n",
       " (94.05, 159.16),\n",
       " (29.65, 43.91),\n",
       " (81.66, 135.95),\n",
       " (61.29, 98.71),\n",
       " (40.58, 62.3),\n",
       " (93.58, 158.27),\n",
       " (48.47, 75.96),\n",
       " (30.99, 46.13),\n",
       " (27.37, 40.15),\n",
       " (48.99, 76.87),\n",
       " (30.86, 45.91),\n",
       " (40.85, 62.77),\n",
       " (31.87, 47.59),\n",
       " (62.26, 100.45),\n",
       " (34.44, 51.88),\n",
       " (48.85, 76.64),\n",
       " (33.25, 49.89),\n",
       " (24.94, 36.2),\n",
       " (82.86, 138.19),\n",
       " (22.38, 32.07),\n",
       " (34.28, 51.62),\n",
       " (24.1, 34.83),\n",
       " (35.89, 54.33),\n",
       " (32.8, 49.14),\n",
       " (30.99, 46.12),\n",
       " (40.92, 62.89),\n",
       " (131.77, 231.87),\n",
       " (119.63, 208.17),\n",
       " (34.35, 51.72),\n",
       " (44.55, 69.14),\n",
       " (46.36, 72.28),\n",
       " (28.09, 41.33),\n",
       " (47.41, 74.12),\n",
       " (83.87, 140.07),\n",
       " (44.51, 69.07),\n",
       " (31.23, 46.52),\n",
       " (58.62, 93.93),\n",
       " (75.96, 125.41),\n",
       " (108.54, 186.75),\n",
       " (48.03, 75.19),\n",
       " (65.68, 106.64),\n",
       " (25.67, 37.38),\n",
       " (79.69, 132.31),\n",
       " (101.83, 173.92),\n",
       " (19.07, 26.83),\n",
       " (47.76, 74.72),\n",
       " (32.74, 49.03),\n",
       " (76.18, 125.81),\n",
       " (30.94, 46.04),\n",
       " (31.78, 47.43),\n",
       " (64.16, 103.87),\n",
       " (52.13, 82.4),\n",
       " (38.6, 58.93),\n",
       " (95.83, 162.52),\n",
       " (43.51, 67.35),\n",
       " (28.3, 41.67),\n",
       " (27.52, 40.4),\n",
       " (29.58, 43.78),\n",
       " (57.81, 92.47),\n",
       " (147.93, 263.83),\n",
       " (36.27, 54.97),\n",
       " (37.85, 57.65),\n",
       " (27.26, 39.97),\n",
       " (65.21, 105.77),\n",
       " (28.85, 42.58),\n",
       " (50.47, 79.48),\n",
       " (28.65, 42.25),\n",
       " (35.49, 53.66),\n",
       " (46.28, 72.15),\n",
       " (71.92, 117.99),\n",
       " (58.25, 93.26),\n",
       " (42.44, 65.51),\n",
       " (27.43, 40.25),\n",
       " (58.99, 94.59),\n",
       " (42.86, 66.23),\n",
       " (57.85, 92.55),\n",
       " (53.97, 85.64),\n",
       " (42.42, 65.46),\n",
       " (65.55, 106.39),\n",
       " (27.55, 40.44),\n",
       " (88.45, 148.63),\n",
       " (70.86, 116.05),\n",
       " (48.63, 76.25),\n",
       " (96.12, 163.08),\n",
       " (89.17, 149.97),\n",
       " (28.36, 41.77),\n",
       " (63.28, 102.29),\n",
       " (23.96, 34.61),\n",
       " (53.58, 84.96),\n",
       " (39.74, 60.87),\n",
       " (96.64, 164.07),\n",
       " (45.71, 71.15),\n",
       " (43.99, 68.17),\n",
       " (47.42, 74.14),\n",
       " (81.89, 136.37),\n",
       " (110.18, 189.92),\n",
       " (48.42, 75.88),\n",
       " (32.09, 47.96),\n",
       " (159.73, 287.42),\n",
       " (29.09, 42.98),\n",
       " (33.74, 50.71),\n",
       " (103.69, 177.48),\n",
       " (26.56, 38.83),\n",
       " (44.67, 69.34),\n",
       " (29.59, 43.8),\n",
       " (44.89, 69.73),\n",
       " (52.87, 83.69),\n",
       " (69.55, 113.66),\n",
       " (100.74, 171.85),\n",
       " (27.32, 40.06),\n",
       " (54.17, 86.0),\n",
       " (25.64, 37.32),\n",
       " (78.5, 130.09),\n",
       " (30.88, 45.93),\n",
       " (17.3, 24.06),\n",
       " (24.8, 35.96),\n",
       " (59.57, 95.61),\n",
       " (78.59, 130.26),\n",
       " (52.83, 83.64),\n",
       " (71.44, 117.11),\n",
       " (47.21, 73.77),\n",
       " (37.21, 56.57),\n",
       " (39.76, 60.9),\n",
       " (42.14, 64.99),\n",
       " (48.6, 76.19),\n",
       " (73.27, 120.47),\n",
       " (41.8, 64.39),\n",
       " (44.37, 68.83),\n",
       " (61.52, 99.11),\n",
       " (65.77, 106.8),\n",
       " (55.06, 87.58),\n",
       " (95.5, 161.9),\n",
       " (36.34, 55.09),\n",
       " (29.27, 43.28),\n",
       " (27.76, 40.79),\n",
       " (39.21, 59.96),\n",
       " (47.4, 74.09),\n",
       " (51.24, 80.83),\n",
       " (40.22, 61.69),\n",
       " (57.7, 92.28),\n",
       " (68.98, 112.62),\n",
       " (85.43, 142.97),\n",
       " (55.02, 87.51),\n",
       " (62.93, 101.66),\n",
       " (62.81, 101.45),\n",
       " (41.6, 64.06),\n",
       " (34.71, 52.34),\n",
       " (34.49, 51.97),\n",
       " (30.31, 44.99),\n",
       " (35.17, 53.11),\n",
       " (47.14, 73.64),\n",
       " (31.88, 47.61),\n",
       " (36.4, 55.2),\n",
       " (28.24, 41.58),\n",
       " (52.3, 82.69),\n",
       " (36.31, 55.04),\n",
       " (67.32, 109.61),\n",
       " (36.21, 54.86),\n",
       " (22.7, 32.59),\n",
       " (45.49, 70.78),\n",
       " (53.02, 83.97),\n",
       " (52.64, 83.3),\n",
       " (98.32, 167.25),\n",
       " (78.6, 130.28),\n",
       " (95.89, 162.65),\n",
       " (38.56, 58.85),\n",
       " (62.32, 100.55),\n",
       " (52.07, 82.29),\n",
       " (26.64, 38.95),\n",
       " (59.08, 94.74),\n",
       " (46.74, 72.95),\n",
       " (163.83, 295.66),\n",
       " (63.61, 102.89),\n",
       " (44.19, 68.52),\n",
       " (32.23, 48.18),\n",
       " (24.86, 36.07),\n",
       " (63.73, 103.1),\n",
       " (51.57, 81.41),\n",
       " (31.55, 47.06),\n",
       " (74.77, 123.23),\n",
       " (88.7, 149.1),\n",
       " (42.66, 65.88),\n",
       " (56.09, 89.41),\n",
       " (26.64, 38.96),\n",
       " (31.19, 46.45),\n",
       " (52.09, 82.32),\n",
       " (25.41, 36.96),\n",
       " (46.84, 73.13),\n",
       " (29.16, 43.09),\n",
       " (52.44, 82.95),\n",
       " (32.55, 48.71),\n",
       " (41.36, 63.65),\n",
       " (50.58, 79.66),\n",
       " (34.21, 51.5),\n",
       " (65.38, 106.09),\n",
       " (58.1, 92.99),\n",
       " (42.33, 65.3),\n",
       " (62.57, 101.0),\n",
       " (94.95, 160.87),\n",
       " (47.36, 74.03),\n",
       " (98.72, 168.0),\n",
       " (57.83, 92.51),\n",
       " (119.63, 208.18),\n",
       " (29.34, 43.39),\n",
       " (26.7, 39.05),\n",
       " (27.9, 41.01),\n",
       " (45.62, 71.01),\n",
       " (62.78, 101.38),\n",
       " (41.01, 63.04),\n",
       " (69.91, 114.32),\n",
       " (70.79, 115.92),\n",
       " (30.03, 44.53),\n",
       " (101.95, 174.14),\n",
       " (41.48, 63.85),\n",
       " (50.48, 79.5),\n",
       " (53.5, 84.82),\n",
       " (24.94, 36.19),\n",
       " (51.83, 81.87),\n",
       " (72.1, 118.33),\n",
       " (37.25, 56.63),\n",
       " (28.55, 42.08),\n",
       " (57.67, 92.23),\n",
       " (65.65, 106.57),\n",
       " (33.71, 50.66),\n",
       " (25.2, 36.61),\n",
       " (46.31, 72.21),\n",
       " (29.7, 43.99),\n",
       " (37.42, 56.91),\n",
       " (46.25, 72.09),\n",
       " (124.43, 217.52),\n",
       " (25.36, 36.88),\n",
       " (24.13, 34.88),\n",
       " (47.13, 73.64),\n",
       " (30.12, 44.67),\n",
       " (25.69, 37.41),\n",
       " (47.34, 73.99),\n",
       " (53.54, 84.89),\n",
       " (75.73, 124.98),\n",
       " (68.42, 111.6),\n",
       " (55.48, 88.33),\n",
       " (25.8, 37.58),\n",
       " (88.07, 147.92),\n",
       " (23.62, 34.06),\n",
       " (27.12, 39.74),\n",
       " (45.41, 70.64),\n",
       " (53.23, 84.34),\n",
       " (67.12, 109.23),\n",
       " (29.54, 43.72),\n",
       " (35.95, 54.44),\n",
       " (32.64, 48.87),\n",
       " (31.43, 46.85),\n",
       " (31.78, 47.43),\n",
       " (52.86, 83.69),\n",
       " (37.83, 57.61),\n",
       " (56.67, 90.43),\n",
       " (35.98, 54.48),\n",
       " (56.86, 90.77),\n",
       " (57.5, 91.92),\n",
       " (32.9, 49.31),\n",
       " (22.93, 32.96),\n",
       " (52.52, 83.08),\n",
       " (74.01, 121.82),\n",
       " (24.99, 36.27),\n",
       " (39.34, 60.18),\n",
       " (56.24, 89.67),\n",
       " (26.69, 39.05),\n",
       " (45.29, 70.43),\n",
       " (27.8, 40.86),\n",
       " (60.12, 96.6),\n",
       " (62.78, 101.4),\n",
       " (125.42, 219.44),\n",
       " (34.35, 51.73),\n",
       " (31.88, 47.6),\n",
       " (39.09, 59.75),\n",
       " (59.22, 95.0),\n",
       " (28.69, 42.32),\n",
       " (28.99, 42.82),\n",
       " (62.06, 100.09),\n",
       " (91.76, 154.84),\n",
       " (51.36, 81.04),\n",
       " (28.51, 42.02),\n",
       " (35.28, 53.29),\n",
       " (30.35, 45.06),\n",
       " (98.25, 167.11),\n",
       " (43.98, 68.16),\n",
       " (61.24, 98.62),\n",
       " (56.52, 90.18),\n",
       " (25.8, 37.59),\n",
       " (31.66, 47.24),\n",
       " (117.65, 204.34),\n",
       " (19.03, 26.77),\n",
       " (49.52, 77.81),\n",
       " (39.42, 60.33),\n",
       " (28.43, 41.89),\n",
       " (53.56, 84.92),\n",
       " (37.83, 57.61),\n",
       " (24.93, 36.18),\n",
       " (35.3, 53.33),\n",
       " (28.61, 42.19),\n",
       " (70.07, 114.61),\n",
       " (79.04, 131.1),\n",
       " (83.16, 138.75),\n",
       " (25.34, 36.84),\n",
       " (48.12, 75.35),\n",
       " (18.64, 26.16),\n",
       " (53.33, 84.51),\n",
       " (52.2, 82.51),\n",
       " (95.42, 161.76),\n",
       " (30.47, 45.25),\n",
       " (19.29, 27.17),\n",
       " (25.55, 37.19),\n",
       " (56.04, 89.31),\n",
       " (35.53, 53.71),\n",
       " (42.68, 65.92),\n",
       " (40.18, 61.61),\n",
       " (61.2, 98.55),\n",
       " (29.04, 42.9),\n",
       " (37.86, 57.67),\n",
       " (48.19, 75.47),\n",
       " (56.63, 90.36),\n",
       " (43.87, 67.97),\n",
       " (30.67, 45.59),\n",
       " (59.62, 95.7),\n",
       " (26.07, 38.03),\n",
       " (24.03, 34.72),\n",
       " (39.92, 61.18),\n",
       " (30.04, 44.54),\n",
       " (71.17, 116.61),\n",
       " (53.56, 84.92),\n",
       " (54.18, 86.02),\n",
       " (27.62, 40.56),\n",
       " (65.88, 106.98),\n",
       " (44.05, 68.28),\n",
       " (16.6, 22.98),\n",
       " (92.98, 157.14),\n",
       " (23.36, 33.64),\n",
       " (24.41, 35.33),\n",
       " (43.99, 68.17),\n",
       " (63.21, 102.17),\n",
       " (42.79, 66.1),\n",
       " (44.47, 69.01),\n",
       " (35.39, 53.48),\n",
       " (35.98, 54.48),\n",
       " (98.77, 168.1),\n",
       " (71.06, 116.42),\n",
       " (53.74, 85.25),\n",
       " (42.61, 65.8),\n",
       " (47.31, 73.94),\n",
       " (26.92, 39.41),\n",
       " (33.57, 50.42),\n",
       " (49.18, 77.22),\n",
       " (76.67, 126.72),\n",
       " (57.65, 92.18),\n",
       " (20.09, 28.44),\n",
       " (37.45, 56.97),\n",
       " (82.14, 136.84),\n",
       " (26.44, 38.63),\n",
       " (19.16, 26.97),\n",
       " (35.37, 53.46),\n",
       " (42.97, 66.41),\n",
       " (52.77, 83.53),\n",
       " (69.32, 113.24),\n",
       " (43.3, 66.98),\n",
       " (24.5, 35.48),\n",
       " (99.41, 169.32),\n",
       " (29.64, 43.88),\n",
       " (49.21, 77.26),\n",
       " (16.7, 23.14),\n",
       " (43.78, 67.81),\n",
       " (51.81, 81.83),\n",
       " (71.4, 117.04),\n",
       " (37.29, 56.69),\n",
       " (30.84, 45.88),\n",
       " (48.51, 76.04),\n",
       " (24.79, 35.96),\n",
       " (39.91, 61.16),\n",
       " (141.34, 250.75),\n",
       " (47.55, 74.36),\n",
       " (31.59, 47.12),\n",
       " (33.48, 50.27),\n",
       " (25.33, 36.82),\n",
       " (83.98, 140.28),\n",
       " (37.77, 57.51),\n",
       " (30.64, 45.54),\n",
       " (41.97, 64.69),\n",
       " (37.56, 57.15),\n",
       " (54.11, 85.9),\n",
       " (71.53, 117.28),\n",
       " (46.7, 72.88),\n",
       " (94.0, 159.07),\n",
       " (44.59, 69.21),\n",
       " (26.21, 38.25),\n",
       " (23.64, 34.1),\n",
       " (26.66, 38.98),\n",
       " (64.98, 105.36),\n",
       " (53.47, 84.76),\n",
       " (42.03, 64.8),\n",
       " (77.41, 128.08),\n",
       " (29.18, 43.12),\n",
       " (63.82, 103.26),\n",
       " (28.2, 41.51),\n",
       " (60.5, 97.28),\n",
       " (31.7, 47.3),\n",
       " (40.24, 61.72),\n",
       " (86.65, 145.26),\n",
       " (20.77, 29.51),\n",
       " (24.65, 35.72),\n",
       " (31.34, 46.69),\n",
       " (77.72, 128.66),\n",
       " (32.27, 48.25),\n",
       " (51.93, 82.04),\n",
       " (67.45, 109.83),\n",
       " (27.58, 40.49),\n",
       " (56.17, 89.55),\n",
       " (24.59, 35.62),\n",
       " (57.99, 92.79),\n",
       " (73.03, 120.03),\n",
       " (63.62, 102.91),\n",
       " (21.25, 30.28),\n",
       " (47.6, 74.45),\n",
       " (31.64, 47.2),\n",
       " (116.18, 201.49),\n",
       " (22.6, 32.43),\n",
       " (28.96, 42.76),\n",
       " (54.5, 86.58),\n",
       " (37.86, 57.66),\n",
       " (33.23, 49.85),\n",
       " (31.59, 47.12),\n",
       " (93.22, 157.6),\n",
       " (41.88, 64.54),\n",
       " (101.83, 173.92),\n",
       " (74.28, 122.33),\n",
       " (22.0, 31.47),\n",
       " (39.0, 59.61),\n",
       " (47.62, 74.48),\n",
       " (44.69, 69.39),\n",
       " (38.66, 59.03),\n",
       " (44.35, 68.79),\n",
       " (26.02, 37.94),\n",
       " (31.45, 46.89),\n",
       " (36.0, 54.51),\n",
       " (37.38, 56.85),\n",
       " (27.96, 41.12),\n",
       " (127.17, 222.86),\n",
       " (42.11, 64.93),\n",
       " (57.54, 91.98),\n",
       " (52.77, 83.52),\n",
       " (25.53, 37.15),\n",
       " (53.87, 85.46),\n",
       " (33.97, 51.09),\n",
       " (21.48, 30.63),\n",
       " (77.06, 127.44),\n",
       " (70.93, 116.17),\n",
       " (39.41, 60.31),\n",
       " (42.02, 64.79),\n",
       " (52.02, 82.19),\n",
       " (49.12, 77.1),\n",
       " (41.05, 63.11),\n",
       " (46.44, 72.43),\n",
       " (47.88, 74.94),\n",
       " (34.98, 52.79),\n",
       " (43.2, 66.81),\n",
       " (55.17, 87.77),\n",
       " (66.04, 107.28),\n",
       " (28.94, 42.73),\n",
       " (20.27, 28.72),\n",
       " (31.66, 47.24),\n",
       " (31.11, 46.31),\n",
       " (71.19, 116.66),\n",
       " (38.7, 59.1),\n",
       " (55.87, 89.01),\n",
       " (37.57, 57.18),\n",
       " (62.67, 101.19),\n",
       " (45.15, 70.18),\n",
       " (94.77, 160.53),\n",
       " (46.77, 73.0),\n",
       " (55.19, 87.81),\n",
       " (52.41, 82.89),\n",
       " (33.55, 50.39),\n",
       " (54.33, 86.29),\n",
       " (34.14, 51.38),\n",
       " (24.89, 36.11),\n",
       " (54.72, 86.97),\n",
       " (65.68, 106.62),\n",
       " (27.38, 40.17),\n",
       " (43.92, 68.06),\n",
       " (28.05, 41.27),\n",
       " (26.55, 38.8),\n",
       " (50.75, 79.96),\n",
       " (35.29, 53.31),\n",
       " (36.81, 55.88),\n",
       " (47.41, 74.11),\n",
       " (29.01, 42.84),\n",
       " (56.9, 90.86),\n",
       " (37.71, 57.42),\n",
       " (24.43, 35.38),\n",
       " (43.72, 67.7),\n",
       " (65.1, 105.58),\n",
       " (40.2, 61.66),\n",
       " (51.54, 81.35),\n",
       " (60.57, 97.41),\n",
       " (59.24, 95.02),\n",
       " (45.22, 70.31),\n",
       " (28.64, 42.23),\n",
       " (20.57, 29.19),\n",
       " (58.83, 94.3),\n",
       " (54.31, 86.25),\n",
       " (20.63, 29.28),\n",
       " (72.12, 118.36),\n",
       " (35.91, 54.36),\n",
       " (52.12, 82.39),\n",
       " (34.01, 51.15),\n",
       " (39.87, 61.08),\n",
       " (44.04, 68.26),\n",
       " (39.39, 60.27),\n",
       " (25.76, 37.52),\n",
       " (25.64, 37.33),\n",
       " (32.32, 48.33),\n",
       " (61.27, 98.68),\n",
       " (34.49, 51.97),\n",
       " (38.06, 58.01),\n",
       " (29.49, 43.64),\n",
       " (41.19, 63.35),\n",
       " (85.57, 143.23),\n",
       " (86.48, 144.93),\n",
       " (52.31, 82.71),\n",
       " (36.75, 55.78),\n",
       " (29.82, 44.19),\n",
       " (36.09, 54.66),\n",
       " (48.52, 76.06),\n",
       " (49.61, 77.97),\n",
       " (35.0, 52.83),\n",
       " (33.35, 50.06),\n",
       " (48.7, 76.37),\n",
       " (43.54, 67.41),\n",
       " (70.84, 116.01),\n",
       " (37.58, 57.18),\n",
       " (32.71, 48.99),\n",
       " (28.83, 42.55),\n",
       " (54.3, 86.23),\n",
       " (37.02, 56.23),\n",
       " (47.48, 74.24),\n",
       " (61.9, 99.8),\n",
       " (86.23, 144.47),\n",
       " (26.77, 39.18),\n",
       " (21.3, 30.35),\n",
       " (36.83, 55.92),\n",
       " (52.98, 83.9),\n",
       " (23.62, 34.06),\n",
       " (52.41, 82.89),\n",
       " (27.19, 39.86),\n",
       " (47.27, 73.88),\n",
       " (37.31, 56.74),\n",
       " (54.63, 86.82),\n",
       " (29.55, 43.74),\n",
       " (50.78, 80.02),\n",
       " (25.54, 37.17),\n",
       " (28.75, 42.42),\n",
       " (48.73, 76.42),\n",
       " (57.9, 92.64),\n",
       " (82.69, 137.86),\n",
       " (164.45, 296.9),\n",
       " (35.08, 52.96),\n",
       " (49.42, 77.62),\n",
       " (32.68, 48.94),\n",
       " (30.58, 45.43),\n",
       " (83.89, 140.1),\n",
       " (25.9, 37.75),\n",
       " (56.6, 90.31),\n",
       " (62.72, 101.28),\n",
       " (36.44, 55.26),\n",
       " (29.37, 43.44),\n",
       " (35.45, 53.58),\n",
       " (30.73, 45.68),\n",
       " (20.94, 29.77),\n",
       " (64.57, 104.62),\n",
       " (37.55, 57.13),\n",
       " (40.79, 62.67),\n",
       " (32.05, 47.88),\n",
       " (23.91, 34.53),\n",
       " (46.92, 73.27),\n",
       " (30.67, 45.59),\n",
       " (27.77, 40.8),\n",
       " (54.61, 86.78),\n",
       " (32.63, 48.85),\n",
       " (63.9, 103.41),\n",
       " (37.08, 56.34),\n",
       " (27.06, 39.65),\n",
       " (36.76, 55.79),\n",
       " (52.46, 82.98),\n",
       " (40.05, 61.4),\n",
       " (60.61, 97.48),\n",
       " (52.92, 83.79),\n",
       " (31.24, 46.54),\n",
       " (29.56, 43.74),\n",
       " (50.11, 78.84),\n",
       " (47.57, 74.39),\n",
       " (31.57, 47.09),\n",
       " (39.46, 60.39),\n",
       " (49.21, 77.26),\n",
       " (88.57, 148.85),\n",
       " (66.0, 107.2),\n",
       " (33.07, 49.58),\n",
       " (58.01, 92.83),\n",
       " (27.6, 40.52),\n",
       " (27.41, 40.22),\n",
       " (34.07, 51.27),\n",
       " (31.32, 46.66),\n",
       " (56.74, 90.56),\n",
       " (46.1, 71.84),\n",
       " (94.58, 160.16),\n",
       " (57.16, 91.32),\n",
       " (47.37, 74.04),\n",
       " (51.21, 80.77),\n",
       " (61.37, 98.85),\n",
       " (50.32, 79.2),\n",
       " (30.12, 44.68),\n",
       " (33.96, 51.08),\n",
       " (93.26, 157.67),\n",
       " (41.81, 64.41),\n",
       " (33.59, 50.45),\n",
       " (59.5, 95.49),\n",
       " (16.05, 22.13),\n",
       " (28.21, 41.53),\n",
       " (68.02, 110.87),\n",
       " (62.61, 101.09),\n",
       " (56.05, 89.33),\n",
       " (32.31, 48.32),\n",
       " (47.9, 74.97),\n",
       " (22.89, 32.89),\n",
       " (31.41, 46.82),\n",
       " (77.25, 127.79),\n",
       " (118.59, 206.15),\n",
       " (19.31, 27.21),\n",
       " (30.42, 45.18),\n",
       " (34.27, 51.59),\n",
       " (34.05, 51.23),\n",
       " (41.15, 63.29),\n",
       " (81.71, 136.04),\n",
       " (42.18, 65.05),\n",
       " (30.95, 46.06),\n",
       " (31.33, 46.68),\n",
       " (112.91, 195.16),\n",
       " (102.18, 174.59),\n",
       " (73.03, 120.02),\n",
       " (62.27, 100.46),\n",
       " (23.14, 33.29),\n",
       " (39.4, 60.28),\n",
       " (39.49, 60.44),\n",
       " (37.6, 57.21),\n",
       " (49.21, 77.25),\n",
       " (30.63, 45.51),\n",
       " (68.56, 111.85),\n",
       " (52.77, 83.53),\n",
       " (53.11, 84.12),\n",
       " (48.05, 75.23),\n",
       " (52.15, 82.43),\n",
       " (30.23, 44.86),\n",
       " (71.43, 117.09),\n",
       " (34.76, 52.42),\n",
       " (39.97, 61.26),\n",
       " (37.28, 56.68),\n",
       " (28.34, 41.73),\n",
       " (32.62, 48.83),\n",
       " (70.86, 116.06),\n",
       " (20.53, 29.13),\n",
       " (34.34, 51.72),\n",
       " (30.03, 44.52),\n",
       " (45.11, 70.12),\n",
       " (29.19, 43.13),\n",
       " (27.33, 40.08),\n",
       " (78.28, 129.7),\n",
       " (47.51, 74.29),\n",
       " (73.99, 121.78),\n",
       " (43.51, 67.34),\n",
       " (31.87, 47.59),\n",
       " (43.81, 67.87),\n",
       " (52.15, 82.43),\n",
       " (23.93, 34.57),\n",
       " (58.99, 94.58),\n",
       " (53.82, 85.38),\n",
       " (63.24, 102.22),\n",
       " (27.84, 40.92),\n",
       " (44.0, 68.19),\n",
       " (26.42, 38.6),\n",
       " (30.26, 44.91),\n",
       " (31.51, 46.99),\n",
       " (69.6, 113.76),\n",
       " (52.65, 83.31),\n",
       " (29.73, 44.02),\n",
       " (27.83, 40.9),\n",
       " (37.91, 57.75),\n",
       " (28.8, 42.5),\n",
       " (48.42, 75.88),\n",
       " (22.01, 31.48),\n",
       " (52.35, 82.78),\n",
       " (46.41, 72.37),\n",
       " (27.89, 41.0),\n",
       " (48.45, 75.94),\n",
       " (63.15, 102.06),\n",
       " (48.41, 75.86),\n",
       " (27.3, 40.03),\n",
       " (39.43, 60.34),\n",
       " (34.4, 51.81),\n",
       " (44.07, 68.32),\n",
       " (58.1, 92.99),\n",
       " (31.35, 46.72),\n",
       " (44.28, 68.68),\n",
       " (71.92, 117.98),\n",
       " (20.87, 29.67),\n",
       " (26.32, 38.44),\n",
       " (67.0, 109.02),\n",
       " (84.44, 141.12),\n",
       " (64.17, 103.9),\n",
       " (54.42, 86.44),\n",
       " (59.33, 95.18),\n",
       " (52.82, 83.62),\n",
       " (58.66, 93.99),\n",
       " (37.04, 56.27),\n",
       " (48.5, 76.03),\n",
       " (43.23, 66.86),\n",
       " (37.13, 56.42),\n",
       " (28.74, 42.4),\n",
       " (56.46, 90.07),\n",
       " (103.17, 176.48),\n",
       " (38.94, 59.5),\n",
       " (30.95, 46.05),\n",
       " (62.62, 101.1),\n",
       " (34.35, 51.73),\n",
       " (27.39, 40.18),\n",
       " (33.77, 50.76),\n",
       " (56.87, 90.8),\n",
       " (30.02, 44.51),\n",
       " (43.77, 67.79),\n",
       " (19.67, 27.77),\n",
       " (60.07, 96.51),\n",
       " (45.47, 70.74),\n",
       " (59.3, 95.14),\n",
       " (62.68, 101.21),\n",
       " (52.8, 83.58),\n",
       " (52.86, 83.69),\n",
       " (28.58, 42.14),\n",
       " (27.28, 40.01),\n",
       " (30.66, 45.58),\n",
       " (84.85, 141.89),\n",
       " (32.21, 48.15),\n",
       " (71.22, 116.71),\n",
       " (32.36, 48.4),\n",
       " (50.74, 79.94),\n",
       " (48.65, 76.27),\n",
       " (48.84, 76.62),\n",
       " (18.76, 26.35),\n",
       " (60.3, 96.94),\n",
       " (57.88, 92.6),\n",
       " (27.12, 39.74),\n",
       " (20.45, 29.0),\n",
       " (24.95, 36.21),\n",
       " (42.37, 65.37),\n",
       " (58.17, 93.12),\n",
       " (48.48, 75.99),\n",
       " (51.35, 81.02),\n",
       " (67.56, 110.05),\n",
       " (28.43, 41.89),\n",
       " (25.9, 37.75),\n",
       " (74.75, 123.18),\n",
       " (29.76, 44.08),\n",
       " (31.21, 46.48),\n",
       " (18.59, 26.07),\n",
       " (66.4, 107.93),\n",
       " (31.44, 46.87),\n",
       " (61.15, 98.46),\n",
       " (46.3, 72.18),\n",
       " (121.03, 210.89),\n",
       " (42.37, 65.37),\n",
       " (51.35, 81.02),\n",
       " (44.88, 69.72),\n",
       " (27.56, 40.46),\n",
       " (25.24, 36.67),\n",
       " (61.36, 98.83),\n",
       " (73.47, 120.84),\n",
       " (27.38, 40.17),\n",
       " (42.86, 66.22),\n",
       " (35.74, 54.07),\n",
       " (27.56, 40.46),\n",
       " (44.73, 69.46),\n",
       " (57.15, 91.29),\n",
       " (33.19, 49.79),\n",
       " (46.76, 72.99),\n",
       " (91.89, 155.08),\n",
       " (26.51, 38.74),\n",
       " (24.1, 34.84),\n",
       " (25.76, 37.52),\n",
       " (65.43, 106.18),\n",
       " (59.34, 95.21),\n",
       " (29.92, 44.34),\n",
       " (78.31, 129.75),\n",
       " (72.93, 119.84),\n",
       " (26.37, 38.52),\n",
       " (29.22, 43.19),\n",
       " (36.12, 54.71),\n",
       " (69.98, 114.45),\n",
       " (141.74, 251.53),\n",
       " (50.71, 79.89),\n",
       " (55.71, 88.73),\n",
       " (26.15, 38.16),\n",
       " (35.77, 54.12),\n",
       " (58.63, 93.93),\n",
       " (22.96, 33.01),\n",
       " (31.13, 46.35),\n",
       " (24.67, 35.75),\n",
       " (31.91, 47.66),\n",
       " (23.75, 34.28),\n",
       " (65.57, 106.42),\n",
       " (75.73, 124.98),\n",
       " (81.14, 134.98),\n",
       " (52.23, 82.57),\n",
       " (60.1, 96.57),\n",
       " (76.45, 126.31),\n",
       " (46.46, 72.46),\n",
       " (24.42, 35.35),\n",
       " (134.86, 237.96),\n",
       " (24.13, 34.89),\n",
       " (37.75, 57.48),\n",
       " (27.19, 39.85),\n",
       " (86.47, 144.92),\n",
       " (32.78, 49.1),\n",
       " (62.37, 100.65),\n",
       " (25.75, 37.51),\n",
       " (60.09, 96.55),\n",
       " (137.05, 242.28),\n",
       " (43.18, 66.78),\n",
       " (49.33, 77.48),\n",
       " (26.5, 38.72),\n",
       " (47.91, 74.99),\n",
       " (72.51, 119.07),\n",
       " (27.51, 40.38),\n",
       " (59.76, 95.97),\n",
       " (69.08, 112.8),\n",
       " (29.55, 43.74),\n",
       " (19.58, 27.62),\n",
       " (25.87, 37.7),\n",
       " (31.11, 46.33),\n",
       " (27.25, 39.95),\n",
       " (99.06, 168.65),\n",
       " (29.45, 43.58),\n",
       " (33.0, 49.47),\n",
       " (54.56, 86.69),\n",
       " (41.32, 63.57),\n",
       " (50.66, 79.81),\n",
       " (22.52, 32.29),\n",
       " (26.59, 38.88),\n",
       " (71.93, 118.01),\n",
       " (34.7, 52.33),\n",
       " (38.68, 59.05),\n",
       " (31.8, 47.47),\n",
       " (62.76, 101.35),\n",
       " (17.36, 24.15),\n",
       " (51.55, 81.37),\n",
       " (39.12, 59.82),\n",
       " (47.84, 74.86),\n",
       " (21.39, 30.49),\n",
       " (56.84, 90.74),\n",
       " (24.04, 34.73),\n",
       " (69.46, 113.5),\n",
       " (24.74, 35.88),\n",
       " (19.38, 27.32),\n",
       " (36.24, 54.93),\n",
       " (29.88, 44.27),\n",
       " (26.95, 39.46),\n",
       " (66.37, 107.87),\n",
       " (55.86, 89.0),\n",
       " (39.44, 60.36),\n",
       " (38.8, 59.27),\n",
       " (40.68, 62.47),\n",
       " (39.81, 60.98),\n",
       " (27.48, 40.33),\n",
       " (49.0, 76.9),\n",
       " (49.46, 77.71),\n",
       " (68.19, 111.18),\n",
       " (30.63, 45.52),\n",
       " (39.17, 59.9),\n",
       " (58.11, 93.01),\n",
       " (126.17, 220.91),\n",
       " (58.17, 93.12),\n",
       " (122.04, 212.86),\n",
       " (38.49, 58.73),\n",
       " (49.68, 78.09),\n",
       " (35.03, 52.87),\n",
       " ...)"
      ]
     },
     "execution_count": 983,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate price interval for MAPE median\n",
    "y_pred_interval_svm_reg = tuple([(round(math.exp(el-el*MAPE_median_svm_reg),2),round(math.exp(el+el*MAPE_median_svm_reg),2)) for el in y_test_pred_svm_reg])\n",
    "y_pred_interval_svm_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model and Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model and cv\n",
    "save_model(best_model_svm_reg, title=\"best_model_svm_reg_01\", save=\"joblib\")\n",
    "save_model(grid_svm_reg, title=\"best_cv_svm_reg_01\", save=\"joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create pipeline to use in RandomizedSearchCV and GridSearchCV\n",
    "pipeline_rf_reg = Pipeline([('preprocessor', preprocessor),\n",
    "                            ('rf_reg',\n",
    "                             RandomForestRegressor(n_estimators=1500,\n",
    "                                                   max_features='sqrt',\n",
    "                                                   random_state=random_state,\n",
    "                                                   max_depth=4,\n",
    "                                                   min_samples_split=10,\n",
    "                                                   min_samples_leaf=1,\n",
    "                                                   bootstrap=False,\n",
    "                                                   n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Pre-Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'ccp_alpha', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display possible hyperparameters for XGBoost Regressor\n",
    "test_rf_reg = RandomForestRegressor()\n",
    "test_rf_reg.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default values for Random Forest Regressor** (as base for hyperparameter search):\n",
    "\n",
    "n_estimators=100, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter distribution\n",
    "param_distribs_rf_reg = {\n",
    "    'rf_reg__n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "    'rf_reg__max_features': ['auto', 'sqrt'],\n",
    "    'rf_reg__max_depth': [None, 1, 2, 3, 4, 5, 7, 10, 15, 20, 30, 40, 50, 75, 100],\n",
    "    'rf_reg__min_samples_split': [2, 5, 10],\n",
    "    'rf_reg__min_samples_leaf': [1, 2, 4],\n",
    "    'rf_reg__bootstrap': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 35.8min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 45.8min finished\n"
     ]
    }
   ],
   "source": [
    "# Create and fit RandomizedSearchCV, save \"best_model\"\n",
    "rnd_rf_reg = RandomizedSearchCV(pipeline_rf_reg,\n",
    "                                 param_distribs_rf_reg,\n",
    "                                 cv=5,\n",
    "                                 scoring=scoring,\n",
    "                                 n_iter=10,\n",
    "                                 return_train_score=True,\n",
    "                                 verbose=4,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "best_model_rnd_rf_reg = rnd_rf_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "-0.23\n",
      "Best parameters:\n",
      "{'rf_reg__n_estimators': 1000, 'rf_reg__min_samples_split': 10, 'rf_reg__min_samples_leaf': 1, 'rf_reg__max_features': 'sqrt', 'rf_reg__max_depth': 100, 'rf_reg__bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(rnd_rf_reg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(rnd_rf_reg.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(rnd_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid_rf_reg = {\n",
    "#    'rf_reg__n_estimators': [1200, 2000],\n",
    "#    'rf_reg__max_features': ['auto', 'sqrt'],\n",
    "    'rf_reg__max_depth': [10, 15],\n",
    "    'rf_reg__min_samples_split': [6, 10],\n",
    "#    'rf_reg__min_samples_leaf': [1, 2],\n",
    "#    'rf_reg__bootstrap': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.0min finished\n"
     ]
    }
   ],
   "source": [
    "# Create and fit GridSearchCV, save \"best_model\"\n",
    "grid_rf_reg = GridSearchCV(pipeline_rf_reg,\n",
    "                            param_grid_rf_reg,\n",
    "                            cv=5,\n",
    "                            scoring=scoring,\n",
    "                            return_train_score=True,\n",
    "                            verbose=4,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "grid_rf_reg = grid_rf_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_rf_reg = grid_rf_reg.best_estimator_['rf_reg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "-0.24\n",
      "Best parameters:\n",
      "{'rf_reg__max_depth': 15, 'rf_reg__min_samples_split': 6}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(grid_rf_reg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_rf_reg.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_rf_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>room_type_Private room</th>\n",
       "      <td>0.227147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates</th>\n",
       "      <td>0.138154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>0.113998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calc_host_lst_count_sqrt_log</th>\n",
       "      <td>0.045111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_tv</th>\n",
       "      <td>0.042288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms_log</th>\n",
       "      <td>0.037592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>availability_90</th>\n",
       "      <td>0.033099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates_per_bed</th>\n",
       "      <td>0.032034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_elevator</th>\n",
       "      <td>0.027565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimum_nights_log</th>\n",
       "      <td>0.023683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maximum_nights</th>\n",
       "      <td>0.023266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_private_entrance</th>\n",
       "      <td>0.020177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_child_friendly</th>\n",
       "      <td>0.017680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Boutique hotel</th>\n",
       "      <td>0.017303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Shared room</th>\n",
       "      <td>0.016305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_other</th>\n",
       "      <td>0.011836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Hotel room</th>\n",
       "      <td>0.009851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10117</th>\n",
       "      <td>0.009247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_smoking_allowed</th>\n",
       "      <td>0.008488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_balcony</th>\n",
       "      <td>0.008482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wk_mth_discount</th>\n",
       "      <td>0.008462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_super_strict</th>\n",
       "      <td>0.008288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10119</th>\n",
       "      <td>0.008287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_is_superhost</th>\n",
       "      <td>0.006347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_strict</th>\n",
       "      <td>0.005638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13359</th>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instant_bookable</th>\n",
       "      <td>0.005314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_pets_allowed</th>\n",
       "      <td>0.004192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10179</th>\n",
       "      <td>0.003971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_moderate</th>\n",
       "      <td>0.003840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_breakfast</th>\n",
       "      <td>0.003534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_essentials</th>\n",
       "      <td>0.003280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_House</th>\n",
       "      <td>0.002769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10245</th>\n",
       "      <td>0.002275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10435</th>\n",
       "      <td>0.002229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10405</th>\n",
       "      <td>0.002156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10719</th>\n",
       "      <td>0.002057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10247</th>\n",
       "      <td>0.001955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10559</th>\n",
       "      <td>0.001928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10243</th>\n",
       "      <td>0.001922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10999</th>\n",
       "      <td>0.001875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10178</th>\n",
       "      <td>0.001856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10439</th>\n",
       "      <td>0.001837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10997</th>\n",
       "      <td>0.001799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10969</th>\n",
       "      <td>0.001592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10785</th>\n",
       "      <td>0.001322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13407</th>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13353</th>\n",
       "      <td>0.001232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10437</th>\n",
       "      <td>0.001177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12047</th>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10963</th>\n",
       "      <td>0.001140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13357</th>\n",
       "      <td>0.001138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10965</th>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12049</th>\n",
       "      <td>0.001007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13347</th>\n",
       "      <td>0.000954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_nan</th>\n",
       "      <td>0.000943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10553</th>\n",
       "      <td>0.000939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12347</th>\n",
       "      <td>0.000888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12053</th>\n",
       "      <td>0.000886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12051</th>\n",
       "      <td>0.000848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12437</th>\n",
       "      <td>0.000818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13349</th>\n",
       "      <td>0.000812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10407</th>\n",
       "      <td>0.000761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12059</th>\n",
       "      <td>0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Secondary unit</th>\n",
       "      <td>0.000736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13086</th>\n",
       "      <td>0.000736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12043</th>\n",
       "      <td>0.000731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10777</th>\n",
       "      <td>0.000691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12045</th>\n",
       "      <td>0.000688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10557</th>\n",
       "      <td>0.000680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10967</th>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10249</th>\n",
       "      <td>0.000666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10589</th>\n",
       "      <td>0.000646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12055</th>\n",
       "      <td>0.000645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12099</th>\n",
       "      <td>0.000626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10317</th>\n",
       "      <td>0.000620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10787</th>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10555</th>\n",
       "      <td>0.000603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10707</th>\n",
       "      <td>0.000590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12103</th>\n",
       "      <td>0.000575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10551</th>\n",
       "      <td>0.000574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12435</th>\n",
       "      <td>0.000572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10315</th>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13187</th>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14057</th>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12157</th>\n",
       "      <td>0.000517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13189</th>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10783</th>\n",
       "      <td>0.000495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14197</th>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10711</th>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10715</th>\n",
       "      <td>0.000471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Bed and breakfast</th>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10623</th>\n",
       "      <td>0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10409</th>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10365</th>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10629</th>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10829</th>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10585</th>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10961</th>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10781</th>\n",
       "      <td>0.000368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10827</th>\n",
       "      <td>0.000357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13088</th>\n",
       "      <td>0.000344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13355</th>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13409</th>\n",
       "      <td>0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Unique space</th>\n",
       "      <td>0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10367</th>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10587</th>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14059</th>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12161</th>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10625</th>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13351</th>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10717</th>\n",
       "      <td>0.000227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12163</th>\n",
       "      <td>0.000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10318</th>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10823</th>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13156</th>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10627</th>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12101</th>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10713</th>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    weight\n",
       "room_type_Private room            0.227147\n",
       "accommodates                      0.138154\n",
       "bedrooms                          0.113998\n",
       "calc_host_lst_count_sqrt_log      0.045111\n",
       "am_tv                             0.042288\n",
       "bathrooms_log                     0.037592\n",
       "availability_90                   0.033099\n",
       "accommodates_per_bed              0.032034\n",
       "am_elevator                       0.027565\n",
       "minimum_nights_log                0.023683\n",
       "maximum_nights                    0.023266\n",
       "am_private_entrance               0.020177\n",
       "am_child_friendly                 0.017680\n",
       "property_type_Boutique hotel      0.017303\n",
       "room_type_Shared room             0.016305\n",
       "zipcode_zip_other                 0.011836\n",
       "room_type_Hotel room              0.009851\n",
       "zipcode_zip_10117                 0.009247\n",
       "am_smoking_allowed                0.008488\n",
       "am_balcony                        0.008482\n",
       "wk_mth_discount                   0.008462\n",
       "cancellation_policy_super_strict  0.008288\n",
       "zipcode_zip_10119                 0.008287\n",
       "host_is_superhost                 0.006347\n",
       "cancellation_policy_strict        0.005638\n",
       "zipcode_zip_13359                 0.005633\n",
       "instant_bookable                  0.005314\n",
       "am_pets_allowed                   0.004192\n",
       "zipcode_zip_10179                 0.003971\n",
       "cancellation_policy_moderate      0.003840\n",
       "am_breakfast                      0.003534\n",
       "am_essentials                     0.003280\n",
       "property_type_House               0.002769\n",
       "zipcode_zip_10245                 0.002275\n",
       "zipcode_zip_10435                 0.002229\n",
       "zipcode_zip_10405                 0.002156\n",
       "zipcode_zip_10719                 0.002057\n",
       "zipcode_zip_10247                 0.001955\n",
       "zipcode_zip_10559                 0.001928\n",
       "zipcode_zip_10243                 0.001922\n",
       "zipcode_zip_10999                 0.001875\n",
       "zipcode_zip_10178                 0.001856\n",
       "zipcode_zip_10439                 0.001837\n",
       "zipcode_zip_10997                 0.001799\n",
       "zipcode_zip_10969                 0.001592\n",
       "zipcode_zip_10785                 0.001322\n",
       "zipcode_zip_13407                 0.001236\n",
       "zipcode_zip_13353                 0.001232\n",
       "zipcode_zip_10437                 0.001177\n",
       "zipcode_zip_12047                 0.001153\n",
       "zipcode_zip_10963                 0.001140\n",
       "zipcode_zip_13357                 0.001138\n",
       "zipcode_zip_10965                 0.001124\n",
       "zipcode_zip_12049                 0.001007\n",
       "zipcode_zip_13347                 0.000954\n",
       "zipcode_zip_nan                   0.000943\n",
       "zipcode_zip_10553                 0.000939\n",
       "zipcode_zip_12347                 0.000888\n",
       "zipcode_zip_12053                 0.000886\n",
       "zipcode_zip_12051                 0.000848\n",
       "zipcode_zip_12437                 0.000818\n",
       "zipcode_zip_13349                 0.000812\n",
       "zipcode_zip_10407                 0.000761\n",
       "zipcode_zip_12059                 0.000754\n",
       "property_type_Secondary unit      0.000736\n",
       "zipcode_zip_13086                 0.000736\n",
       "zipcode_zip_12043                 0.000731\n",
       "zipcode_zip_10777                 0.000691\n",
       "zipcode_zip_12045                 0.000688\n",
       "zipcode_zip_10557                 0.000680\n",
       "zipcode_zip_10967                 0.000667\n",
       "zipcode_zip_10249                 0.000666\n",
       "zipcode_zip_10589                 0.000646\n",
       "zipcode_zip_12055                 0.000645\n",
       "zipcode_zip_12099                 0.000626\n",
       "zipcode_zip_10317                 0.000620\n",
       "zipcode_zip_10787                 0.000617\n",
       "zipcode_zip_10555                 0.000603\n",
       "zipcode_zip_10707                 0.000590\n",
       "zipcode_zip_12103                 0.000575\n",
       "zipcode_zip_10551                 0.000574\n",
       "zipcode_zip_12435                 0.000572\n",
       "zipcode_zip_10315                 0.000562\n",
       "zipcode_zip_13187                 0.000560\n",
       "zipcode_zip_14057                 0.000537\n",
       "zipcode_zip_12157                 0.000517\n",
       "zipcode_zip_13189                 0.000501\n",
       "zipcode_zip_10783                 0.000495\n",
       "zipcode_zip_14197                 0.000491\n",
       "zipcode_zip_10711                 0.000475\n",
       "zipcode_zip_10715                 0.000471\n",
       "property_type_Bed and breakfast   0.000457\n",
       "zipcode_zip_10623                 0.000451\n",
       "zipcode_zip_10409                 0.000438\n",
       "zipcode_zip_10365                 0.000430\n",
       "zipcode_zip_10629                 0.000407\n",
       "zipcode_zip_10829                 0.000388\n",
       "zipcode_zip_10585                 0.000384\n",
       "zipcode_zip_10961                 0.000384\n",
       "zipcode_zip_10781                 0.000368\n",
       "zipcode_zip_10827                 0.000357\n",
       "zipcode_zip_13088                 0.000344\n",
       "zipcode_zip_13355                 0.000343\n",
       "zipcode_zip_13409                 0.000340\n",
       "property_type_Unique space        0.000334\n",
       "zipcode_zip_10367                 0.000279\n",
       "zipcode_zip_10587                 0.000279\n",
       "zipcode_zip_14059                 0.000275\n",
       "zipcode_zip_12161                 0.000272\n",
       "zipcode_zip_10625                 0.000243\n",
       "zipcode_zip_13351                 0.000229\n",
       "zipcode_zip_10717                 0.000227\n",
       "zipcode_zip_12163                 0.000218\n",
       "zipcode_zip_10318                 0.000203\n",
       "zipcode_zip_10823                 0.000177\n",
       "zipcode_zip_13156                 0.000133\n",
       "zipcode_zip_10627                 0.000114\n",
       "zipcode_zip_12101                 0.000108\n",
       "zipcode_zip_10713                 0.000073"
      ]
     },
     "execution_count": 1010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display feature importances\n",
    "fi_rf_reg = get_feat_importances(best_model_rf_reg)\n",
    "fi_rf_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:07:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# Load existing model\n",
    "#load_best_model = load_model(title=\"best_model_rf_reg_01\", dataset_loc=dataset_loc, dataset_date=dataset_date)\n",
    "#load_best_cv = load_model(title=\"best_cv_rf_reg_01\", dataset_loc=dataset_loc, dataset_date=dataset_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Curves (Overfitting)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_train_pred_rf_reg = best_model_rf_reg.predict(X_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.10\n",
      "RMSE: 0.31\n",
      "MAE: 0.25\n",
      "R2: 0.70\n",
      "MAPE: 6.31\n",
      "MAPE median: 5.07\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_train, y_train_pred_rf_reg, model=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_test_pred_rf_reg = best_model_rf_reg.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.15\n",
      "RMSE: 0.38\n",
      "MAE: 0.29\n",
      "R2: 0.54\n",
      "MAPE: 7.61\n",
      "MAPE median: 5.83\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_test, y_test_pred_rf_reg, model=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36322871, 0.39248337])"
      ]
     },
     "execution_count": 1003,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display confidence interval (scipy stats)\n",
    "confidence = 0.95\n",
    "squared_errors = (y_test_pred_rf_reg - y_test)**2\n",
    "np.sqrt(\n",
    "    stats.t.interval(confidence,\n",
    "                     len(squared_errors) - 1,\n",
    "                     loc=squared_errors.mean(),\n",
    "                     scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Median Price Intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save MAPE_median as variable\n",
    "MAPE_median_rf_reg = (median_absolute_percentage_error(y_test, y_test_pred_rf_reg))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52.49, 84.34),\n",
       " (63.69, 104.73),\n",
       " (48.79, 77.71),\n",
       " (31.74, 48.02),\n",
       " (37.33, 57.58),\n",
       " (31.18, 47.07),\n",
       " (40.96, 63.88),\n",
       " (39.19, 60.8),\n",
       " (50.29, 80.39),\n",
       " (43.01, 67.48),\n",
       " (25.4, 37.42),\n",
       " (40.63, 63.31),\n",
       " (60.1, 98.14),\n",
       " (26.03, 38.45),\n",
       " (34.87, 53.35),\n",
       " (65.81, 108.65),\n",
       " (68.9, 114.37),\n",
       " (52.99, 85.24),\n",
       " (57.08, 92.64),\n",
       " (47.3, 75.06),\n",
       " (85.92, 146.45),\n",
       " (50.15, 80.13),\n",
       " (42.29, 66.21),\n",
       " (76.92, 129.38),\n",
       " (25.16, 37.02),\n",
       " (50.02, 79.91),\n",
       " (76.51, 128.6),\n",
       " (33.78, 51.48),\n",
       " (28.21, 42.09),\n",
       " (45.88, 72.54),\n",
       " (26.96, 40.0),\n",
       " (56.11, 90.88),\n",
       " (55.91, 90.52),\n",
       " (29.42, 44.11),\n",
       " (44.83, 70.68),\n",
       " (58.97, 96.07),\n",
       " (36.57, 56.26),\n",
       " (51.65, 82.83),\n",
       " (33.59, 51.16),\n",
       " (32.83, 49.87),\n",
       " (31.65, 47.86),\n",
       " (25.48, 37.55),\n",
       " (38.19, 59.07),\n",
       " (35.32, 54.12),\n",
       " (30.45, 45.84),\n",
       " (46.57, 73.76),\n",
       " (47.55, 75.5),\n",
       " (72.92, 121.87),\n",
       " (59.19, 96.49),\n",
       " (90.86, 155.9),\n",
       " (34.06, 51.96),\n",
       " (66.47, 109.87),\n",
       " (52.47, 84.3),\n",
       " (48.38, 76.98),\n",
       " (35.34, 54.15),\n",
       " (46.65, 73.9),\n",
       " (57.55, 93.5),\n",
       " (33.75, 51.44),\n",
       " (53.35, 85.89),\n",
       " (34.6, 52.89),\n",
       " (54.31, 87.61),\n",
       " (55.14, 89.13),\n",
       " (59.51, 97.07),\n",
       " (46.6, 73.82),\n",
       " (53.62, 86.38),\n",
       " (45.53, 71.91),\n",
       " (27.74, 41.3),\n",
       " (30.72, 46.29),\n",
       " (45.95, 72.67),\n",
       " (42.77, 67.05),\n",
       " (49.66, 79.26),\n",
       " (87.26, 149.01),\n",
       " (38.47, 59.55),\n",
       " (55.25, 89.31),\n",
       " (121.63, 216.12),\n",
       " (42.85, 67.21),\n",
       " (17.95, 25.37),\n",
       " (62.71, 102.94),\n",
       " (41.25, 64.39),\n",
       " (28.8, 43.07),\n",
       " (27.48, 40.85),\n",
       " (23.99, 35.1),\n",
       " (37.71, 58.24),\n",
       " (26.3, 38.9),\n",
       " (58.84, 95.84),\n",
       " (54.84, 88.57),\n",
       " (44.0, 69.22),\n",
       " (67.78, 112.29),\n",
       " (39.91, 62.05),\n",
       " (54.44, 87.86),\n",
       " (66.76, 110.4),\n",
       " (47.89, 76.11),\n",
       " (41.24, 64.38),\n",
       " (112.29, 197.63),\n",
       " (53.12, 85.47),\n",
       " (30.66, 46.19),\n",
       " (93.65, 161.27),\n",
       " (54.66, 88.26),\n",
       " (29.75, 44.65),\n",
       " (23.08, 33.61),\n",
       " (31.18, 47.07),\n",
       " (38.07, 58.86),\n",
       " (31.47, 47.56),\n",
       " (42.53, 66.64),\n",
       " (28.53, 42.61),\n",
       " (58.21, 94.7),\n",
       " (28.95, 43.31),\n",
       " (39.94, 62.11),\n",
       " (37.78, 58.35),\n",
       " (49.03, 78.13),\n",
       " (32.86, 49.92),\n",
       " (23.76, 34.72),\n",
       " (76.03, 127.7),\n",
       " (27.51, 40.91),\n",
       " (65.8, 108.63),\n",
       " (66.77, 110.42),\n",
       " (39.11, 60.67),\n",
       " (80.84, 136.78),\n",
       " (45.15, 71.24),\n",
       " (34.31, 52.39),\n",
       " (30.76, 46.36),\n",
       " (50.02, 79.92),\n",
       " (33.47, 50.96),\n",
       " (42.84, 67.18),\n",
       " (31.64, 47.84),\n",
       " (61.75, 101.17),\n",
       " (34.81, 53.25),\n",
       " (49.21, 78.46),\n",
       " (34.1, 52.03),\n",
       " (29.19, 43.72),\n",
       " (81.92, 138.82),\n",
       " (23.88, 34.92),\n",
       " (31.96, 48.38),\n",
       " (26.51, 39.26),\n",
       " (35.77, 54.9),\n",
       " (32.85, 49.89),\n",
       " (31.63, 47.83),\n",
       " (48.22, 76.7),\n",
       " (98.87, 171.37),\n",
       " (90.77, 155.73),\n",
       " (32.49, 49.29),\n",
       " (41.82, 65.4),\n",
       " (47.77, 75.9),\n",
       " (29.12, 43.6),\n",
       " (46.52, 73.67),\n",
       " (78.43, 132.23),\n",
       " (44.43, 69.98),\n",
       " (36.16, 55.56),\n",
       " (53.67, 86.47),\n",
       " (63.78, 104.89),\n",
       " (68.88, 114.33),\n",
       " (54.77, 88.44),\n",
       " (56.04, 90.74),\n",
       " (31.73, 48.01),\n",
       " (70.59, 117.52),\n",
       " (90.86, 155.9),\n",
       " (25.83, 38.13),\n",
       " (49.19, 78.43),\n",
       " (32.11, 48.65),\n",
       " (85.8, 146.22),\n",
       " (34.55, 52.81),\n",
       " (32.0, 48.45),\n",
       " (59.38, 96.83),\n",
       " (44.69, 70.44),\n",
       " (44.25, 69.67),\n",
       " (81.24, 137.55),\n",
       " (46.12, 72.96),\n",
       " (30.02, 45.12),\n",
       " (27.17, 40.35),\n",
       " (31.23, 47.15),\n",
       " (50.48, 80.73),\n",
       " (130.48, 233.8),\n",
       " (37.67, 58.17),\n",
       " (45.77, 72.34),\n",
       " (31.96, 48.39),\n",
       " (70.45, 117.26),\n",
       " (28.98, 43.37),\n",
       " (56.65, 91.87),\n",
       " (30.37, 45.7),\n",
       " (39.5, 61.34),\n",
       " (49.77, 79.46),\n",
       " (65.53, 108.13),\n",
       " (53.46, 86.09),\n",
       " (51.13, 81.9),\n",
       " (29.23, 43.78),\n",
       " (52.46, 84.28),\n",
       " (42.53, 66.64),\n",
       " (63.03, 103.52),\n",
       " (50.64, 81.01),\n",
       " (45.32, 71.55),\n",
       " (55.74, 90.21),\n",
       " (28.94, 43.3),\n",
       " (62.06, 101.74),\n",
       " (64.78, 106.73),\n",
       " (55.53, 89.82),\n",
       " (57.0, 92.5),\n",
       " (63.56, 104.48),\n",
       " (31.26, 47.21),\n",
       " (54.01, 87.07),\n",
       " (29.38, 44.04),\n",
       " (59.81, 97.62),\n",
       " (33.14, 50.4),\n",
       " (100.23, 174.02),\n",
       " (45.51, 71.89),\n",
       " (52.69, 84.71),\n",
       " (43.08, 67.61),\n",
       " (56.44, 91.47),\n",
       " (78.76, 132.86),\n",
       " (48.18, 76.62),\n",
       " (31.66, 47.88),\n",
       " (114.42, 201.82),\n",
       " (28.9, 43.23),\n",
       " (36.71, 56.51),\n",
       " (67.74, 112.22),\n",
       " (29.97, 45.03),\n",
       " (51.16, 81.95),\n",
       " (29.8, 44.74),\n",
       " (46.32, 73.33),\n",
       " (40.79, 63.59),\n",
       " (57.18, 92.83),\n",
       " (91.48, 157.09),\n",
       " (24.06, 35.21),\n",
       " (57.99, 94.29),\n",
       " (30.98, 46.74),\n",
       " (72.06, 120.26),\n",
       " (32.24, 48.86),\n",
       " (19.75, 28.22),\n",
       " (29.23, 43.79),\n",
       " (57.08, 92.64),\n",
       " (67.35, 111.49),\n",
       " (60.38, 98.66),\n",
       " (62.73, 102.97),\n",
       " (46.97, 74.48),\n",
       " (36.62, 56.36),\n",
       " (43.12, 67.66),\n",
       " (48.29, 76.82),\n",
       " (52.56, 84.47),\n",
       " (62.52, 102.58),\n",
       " (45.95, 72.66),\n",
       " (43.43, 68.22),\n",
       " (58.79, 95.75),\n",
       " (62.12, 101.85),\n",
       " (53.78, 86.67),\n",
       " (93.86, 161.68),\n",
       " (33.71, 51.37),\n",
       " (33.07, 50.27),\n",
       " (30.68, 46.23),\n",
       " (44.14, 69.46),\n",
       " (47.64, 75.66),\n",
       " (45.73, 72.27),\n",
       " (33.83, 51.58),\n",
       " (56.66, 91.88),\n",
       " (52.52, 84.4),\n",
       " (71.55, 119.31),\n",
       " (55.83, 90.38),\n",
       " (48.44, 77.09),\n",
       " (57.72, 93.8),\n",
       " (44.65, 70.36),\n",
       " (31.51, 47.64),\n",
       " (32.75, 49.73),\n",
       " (29.58, 44.37),\n",
       " (33.37, 50.79),\n",
       " (48.78, 77.7),\n",
       " (30.56, 46.03),\n",
       " (41.87, 65.48),\n",
       " (33.4, 50.84),\n",
       " (53.92, 86.91),\n",
       " (33.7, 51.35),\n",
       " (65.52, 108.11),\n",
       " (44.2, 69.57),\n",
       " (26.27, 38.85),\n",
       " (50.12, 80.08),\n",
       " (50.22, 80.26),\n",
       " (59.45, 96.95),\n",
       " (94.49, 162.9),\n",
       " (72.74, 121.53),\n",
       " (79.48, 134.22),\n",
       " (36.68, 56.47),\n",
       " (57.93, 94.18),\n",
       " (50.13, 80.1),\n",
       " (25.4, 37.41),\n",
       " (60.69, 99.23),\n",
       " (39.13, 60.7),\n",
       " (96.93, 167.62),\n",
       " (52.78, 84.87),\n",
       " (47.18, 74.85),\n",
       " (32.0, 48.46),\n",
       " (25.7, 37.91),\n",
       " (63.95, 105.21),\n",
       " (51.36, 82.3),\n",
       " (30.51, 45.93),\n",
       " (53.13, 85.48),\n",
       " (83.98, 142.74),\n",
       " (43.6, 68.52),\n",
       " (68.87, 114.32),\n",
       " (29.93, 44.95),\n",
       " (33.54, 51.08),\n",
       " (49.01, 78.11),\n",
       " (30.88, 46.56),\n",
       " (56.52, 91.62),\n",
       " (29.05, 43.49),\n",
       " (49.98, 79.84),\n",
       " (32.22, 48.83),\n",
       " (49.81, 79.52),\n",
       " (54.53, 88.01),\n",
       " (35.92, 55.16),\n",
       " (55.77, 90.26),\n",
       " (54.04, 87.13),\n",
       " (56.11, 90.87),\n",
       " (58.68, 95.56),\n",
       " (73.15, 122.3),\n",
       " (50.25, 80.32),\n",
       " (73.74, 123.41),\n",
       " (70.39, 117.14),\n",
       " (87.76, 149.95),\n",
       " (31.55, 47.7),\n",
       " (25.28, 37.21),\n",
       " (27.53, 40.95),\n",
       " (47.04, 74.59),\n",
       " (64.63, 106.47),\n",
       " (46.01, 72.76),\n",
       " (70.9, 118.1),\n",
       " (59.61, 97.25),\n",
       " (29.68, 44.54),\n",
       " (94.4, 162.71),\n",
       " (48.48, 77.15),\n",
       " (45.5, 71.87),\n",
       " (52.89, 85.06),\n",
       " (24.1, 35.27),\n",
       " (50.83, 81.37),\n",
       " (65.9, 108.81),\n",
       " (46.2, 73.1),\n",
       " (27.4, 40.73),\n",
       " (57.4, 93.22),\n",
       " (58.98, 96.1),\n",
       " (33.09, 50.3),\n",
       " (29.24, 43.81),\n",
       " (43.24, 67.89),\n",
       " (30.49, 45.91),\n",
       " (44.85, 70.71),\n",
       " (41.83, 65.41),\n",
       " (108.48, 190.14),\n",
       " (29.7, 44.58),\n",
       " (26.28, 38.87),\n",
       " (51.92, 83.31),\n",
       " (29.96, 45.02),\n",
       " (32.69, 49.63),\n",
       " (48.12, 76.52),\n",
       " (52.76, 84.82),\n",
       " (86.83, 148.17),\n",
       " (59.82, 97.63),\n",
       " (49.78, 79.48),\n",
       " (27.7, 41.22),\n",
       " (70.19, 116.77),\n",
       " (25.44, 37.48),\n",
       " (27.53, 40.95),\n",
       " (51.89, 83.26),\n",
       " (53.63, 86.4),\n",
       " (80.82, 136.75),\n",
       " (30.13, 45.31),\n",
       " (33.22, 50.53),\n",
       " (40.74, 63.51),\n",
       " (31.98, 48.42),\n",
       " (31.07, 46.89),\n",
       " (52.76, 84.83),\n",
       " (36.18, 55.6),\n",
       " (55.49, 89.75),\n",
       " (30.63, 46.15),\n",
       " (53.68, 86.48),\n",
       " (66.79, 110.45),\n",
       " (31.39, 47.42),\n",
       " (26.11, 38.59),\n",
       " (45.55, 71.96),\n",
       " (56.64, 91.85),\n",
       " (26.39, 39.04),\n",
       " (41.88, 65.5),\n",
       " (61.0, 99.8),\n",
       " (30.52, 45.96),\n",
       " (39.18, 60.79),\n",
       " (28.51, 42.58),\n",
       " (54.03, 87.11),\n",
       " (62.49, 102.52),\n",
       " (94.06, 162.06),\n",
       " (33.61, 51.19),\n",
       " (34.12, 52.07),\n",
       " (48.32, 76.87),\n",
       " (58.53, 95.29),\n",
       " (30.95, 46.68),\n",
       " (27.43, 40.78),\n",
       " (53.3, 85.8),\n",
       " (84.71, 144.14),\n",
       " (54.03, 87.11),\n",
       " (30.0, 45.07),\n",
       " (36.11, 55.48),\n",
       " (30.37, 45.7),\n",
       " (99.9, 173.36),\n",
       " (46.49, 73.62),\n",
       " (58.94, 96.02),\n",
       " (60.03, 98.01),\n",
       " (28.26, 42.16),\n",
       " (30.38, 45.73),\n",
       " (77.84, 131.12),\n",
       " (29.18, 43.7),\n",
       " (47.71, 75.78),\n",
       " (51.41, 82.39),\n",
       " (31.46, 47.54),\n",
       " (46.16, 73.04),\n",
       " (35.29, 54.07),\n",
       " (32.96, 50.08),\n",
       " (31.49, 47.59),\n",
       " (32.02, 48.49),\n",
       " (66.55, 110.02),\n",
       " (73.24, 122.46),\n",
       " (64.16, 105.61),\n",
       " (28.57, 42.69),\n",
       " (52.73, 84.78),\n",
       " (22.25, 32.26),\n",
       " (50.13, 80.1),\n",
       " (48.4, 77.02),\n",
       " (78.8, 132.92),\n",
       " (38.62, 59.82),\n",
       " (24.79, 36.41),\n",
       " (36.16, 55.57),\n",
       " (60.17, 98.27),\n",
       " (39.52, 61.37),\n",
       " (42.43, 66.45),\n",
       " (46.82, 74.2),\n",
       " (65.69, 108.42),\n",
       " (32.88, 49.96),\n",
       " (47.44, 75.31),\n",
       " (47.88, 76.09),\n",
       " (57.57, 93.53),\n",
       " (44.67, 70.39),\n",
       " (33.09, 50.31),\n",
       " (50.4, 80.59),\n",
       " (31.05, 46.85),\n",
       " (29.05, 43.49),\n",
       " (41.83, 65.41),\n",
       " (27.75, 41.31),\n",
       " (72.35, 120.79),\n",
       " (49.92, 79.73),\n",
       " (45.13, 71.22),\n",
       " (30.12, 45.29),\n",
       " (67.33, 111.46),\n",
       " (44.5, 70.1),\n",
       " (23.55, 34.38),\n",
       " (73.12, 122.24),\n",
       " (26.27, 38.85),\n",
       " (27.56, 40.99),\n",
       " (40.98, 63.93),\n",
       " (57.93, 94.19),\n",
       " (45.93, 72.63),\n",
       " (47.31, 75.07),\n",
       " (32.26, 48.91),\n",
       " (32.0, 48.45),\n",
       " (86.1, 146.79),\n",
       " (62.51, 102.56),\n",
       " (62.56, 102.65),\n",
       " (46.33, 73.34),\n",
       " (48.79, 77.72),\n",
       " (28.99, 43.39),\n",
       " (32.42, 49.18),\n",
       " (46.68, 73.95),\n",
       " (62.86, 103.21),\n",
       " (53.63, 86.39),\n",
       " (25.01, 36.77),\n",
       " (32.76, 49.74),\n",
       " (66.96, 110.76),\n",
       " (27.9, 41.56),\n",
       " (22.83, 33.2),\n",
       " (32.1, 48.63),\n",
       " (56.54, 91.66),\n",
       " (59.2, 96.51),\n",
       " (59.18, 96.47),\n",
       " (43.73, 68.74),\n",
       " (32.87, 49.93),\n",
       " (100.62, 174.77),\n",
       " (29.5, 44.25),\n",
       " (33.99, 51.85),\n",
       " (29.19, 43.72),\n",
       " (38.21, 59.1),\n",
       " (52.21, 83.83),\n",
       " (68.7, 113.99),\n",
       " (37.22, 57.39),\n",
       " (27.6, 41.07),\n",
       " (51.34, 82.28),\n",
       " (26.34, 38.97),\n",
       " (35.28, 54.05),\n",
       " (107.64, 188.48),\n",
       " (48.45, 77.1),\n",
       " (33.23, 50.54),\n",
       " (30.37, 45.7),\n",
       " (28.58, 42.7),\n",
       " (83.18, 141.22),\n",
       " (33.5, 51.01),\n",
       " (31.16, 47.04),\n",
       " (44.04, 69.3),\n",
       " (37.02, 57.05),\n",
       " (54.32, 87.64),\n",
       " (65.65, 108.34),\n",
       " (49.35, 78.7),\n",
       " (80.94, 136.98),\n",
       " (43.19, 67.79),\n",
       " (30.96, 46.69),\n",
       " (25.49, 37.56),\n",
       " (31.58, 47.75),\n",
       " (68.32, 113.3),\n",
       " (47.82, 75.98),\n",
       " (47.14, 74.77),\n",
       " (77.14, 129.79),\n",
       " (35.27, 54.03),\n",
       " (51.47, 82.5),\n",
       " (37.23, 57.42),\n",
       " (56.78, 92.1),\n",
       " (30.77, 46.39),\n",
       " (37.7, 58.23),\n",
       " (77.75, 130.95),\n",
       " (26.45, 39.15),\n",
       " (30.25, 45.49),\n",
       " (30.59, 46.07),\n",
       " (68.64, 113.88),\n",
       " (30.6, 46.09),\n",
       " (53.8, 86.7),\n",
       " (55.78, 90.29),\n",
       " (30.97, 46.71),\n",
       " (54.71, 88.35),\n",
       " (28.24, 42.13),\n",
       " (66.22, 109.41),\n",
       " (63.68, 104.72),\n",
       " (74.8, 125.4),\n",
       " (26.46, 39.17),\n",
       " (45.73, 72.28),\n",
       " (31.66, 47.88),\n",
       " (100.83, 175.18),\n",
       " (25.73, 37.96),\n",
       " (30.52, 45.95),\n",
       " (50.11, 80.06),\n",
       " (35.17, 53.87),\n",
       " (31.07, 46.89),\n",
       " (34.87, 53.36),\n",
       " (82.03, 139.05),\n",
       " (47.46, 75.34),\n",
       " (90.86, 155.9),\n",
       " (59.27, 96.64),\n",
       " (24.64, 36.16),\n",
       " (37.21, 57.37),\n",
       " (47.08, 74.66),\n",
       " (44.03, 69.28),\n",
       " (42.91, 67.3),\n",
       " (45.37, 71.63),\n",
       " (30.92, 46.62),\n",
       " (33.01, 50.17),\n",
       " (44.91, 70.82),\n",
       " (39.39, 61.15),\n",
       " (30.1, 45.25),\n",
       " (129.4, 231.65),\n",
       " (43.78, 68.84),\n",
       " (53.54, 86.24),\n",
       " (46.6, 73.82),\n",
       " (26.37, 39.02),\n",
       " (44.43, 69.97),\n",
       " (35.61, 54.62),\n",
       " (26.68, 39.53),\n",
       " (74.6, 125.02),\n",
       " (69.1, 114.74),\n",
       " (34.95, 53.49),\n",
       " (43.94, 69.11),\n",
       " (53.37, 85.92),\n",
       " (45.91, 72.59),\n",
       " (36.26, 55.75),\n",
       " (51.95, 83.37),\n",
       " (50.39, 80.57),\n",
       " (37.62, 58.09),\n",
       " (39.37, 61.12),\n",
       " (53.58, 86.3),\n",
       " (53.91, 86.89),\n",
       " (29.09, 43.56),\n",
       " (18.94, 26.93),\n",
       " (42.85, 67.2),\n",
       " (37.93, 58.63),\n",
       " (56.69, 91.94),\n",
       " (49.7, 79.33),\n",
       " (51.29, 82.18),\n",
       " (44.33, 69.81),\n",
       " (52.37, 84.13),\n",
       " (44.71, 70.47),\n",
       " (80.25, 135.67),\n",
       " (54.17, 87.36),\n",
       " (45.65, 72.14),\n",
       " (56.8, 92.13),\n",
       " (38.65, 59.87),\n",
       " (41.1, 64.14),\n",
       " (38.77, 60.07),\n",
       " (30.69, 46.24),\n",
       " (54.62, 88.19),\n",
       " (63.17, 103.78),\n",
       " (33.92, 51.73),\n",
       " (48.76, 77.66),\n",
       " (27.52, 40.93),\n",
       " (30.67, 46.21),\n",
       " (52.37, 84.12),\n",
       " (41.25, 64.4),\n",
       " (32.82, 49.84),\n",
       " (48.73, 77.6),\n",
       " (33.34, 50.74),\n",
       " (56.28, 91.18),\n",
       " (48.05, 76.4),\n",
       " (29.18, 43.7),\n",
       " (41.58, 64.97),\n",
       " (56.55, 91.69),\n",
       " (36.07, 55.41),\n",
       " (53.98, 87.02),\n",
       " (50.53, 80.82),\n",
       " (48.58, 77.34),\n",
       " (46.67, 73.94),\n",
       " (29.8, 44.74),\n",
       " (23.59, 34.45),\n",
       " (52.14, 83.71),\n",
       " (52.49, 84.34),\n",
       " (20.74, 29.82),\n",
       " (56.48, 91.55),\n",
       " (32.72, 49.68),\n",
       " (54.79, 88.49),\n",
       " (32.3, 48.96),\n",
       " (35.39, 54.24),\n",
       " (39.89, 62.02),\n",
       " (40.85, 63.69),\n",
       " (29.83, 44.79),\n",
       " (24.71, 36.28),\n",
       " (31.4, 47.44),\n",
       " (57.61, 93.6),\n",
       " (35.25, 54.01),\n",
       " (33.9, 51.68),\n",
       " (32.81, 49.84),\n",
       " (45.25, 71.43),\n",
       " (62.86, 103.21),\n",
       " (89.69, 153.66),\n",
       " (61.11, 100.0),\n",
       " (32.76, 49.74),\n",
       " (31.63, 47.83),\n",
       " (33.07, 50.27),\n",
       " (47.76, 75.88),\n",
       " (45.8, 72.4),\n",
       " (33.77, 51.47),\n",
       " (29.21, 43.76),\n",
       " (50.06, 79.99),\n",
       " (41.79, 65.34),\n",
       " (54.55, 88.05),\n",
       " (38.98, 60.44),\n",
       " (36.37, 55.93),\n",
       " (32.62, 49.51),\n",
       " (53.03, 85.32),\n",
       " (34.3, 52.37),\n",
       " (46.94, 74.42),\n",
       " (62.53, 102.6),\n",
       " (67.83, 112.39),\n",
       " (26.32, 38.94),\n",
       " (24.21, 35.46),\n",
       " (34.25, 52.29),\n",
       " (50.15, 80.14),\n",
       " (25.57, 37.69),\n",
       " (50.27, 80.36),\n",
       " (29.24, 43.81),\n",
       " (43.59, 68.49),\n",
       " (37.54, 57.94),\n",
       " (62.24, 102.07),\n",
       " (33.96, 51.79),\n",
       " (49.03, 78.13),\n",
       " (29.67, 44.52),\n",
       " (30.52, 45.96),\n",
       " (50.83, 81.35),\n",
       " (54.34, 87.67),\n",
       " (72.22, 120.57),\n",
       " (117.85, 208.62),\n",
       " (31.82, 48.15),\n",
       " (45.69, 72.21),\n",
       " (36.84, 56.73),\n",
       " (31.64, 47.86),\n",
       " (84.15, 143.06),\n",
       " (31.28, 47.24),\n",
       " (59.76, 97.52),\n",
       " (55.17, 89.18),\n",
       " (32.56, 49.4),\n",
       " (25.76, 38.01),\n",
       " (30.74, 46.32),\n",
       " (36.89, 56.82),\n",
       " (28.37, 42.35),\n",
       " (56.6, 91.76),\n",
       " (36.5, 56.14),\n",
       " (36.3, 55.81),\n",
       " (42.32, 66.27),\n",
       " (38.56, 59.7),\n",
       " (39.84, 61.94),\n",
       " (35.39, 54.25),\n",
       " (29.31, 43.92),\n",
       " (51.47, 82.51),\n",
       " (32.73, 49.7),\n",
       " (41.69, 65.16),\n",
       " (39.48, 61.31),\n",
       " (28.44, 42.47),\n",
       " (32.56, 49.41),\n",
       " (52.0, 83.45),\n",
       " (42.16, 65.99),\n",
       " (60.98, 99.76),\n",
       " (53.56, 86.26),\n",
       " (30.27, 45.53),\n",
       " (31.55, 47.7),\n",
       " (53.07, 85.39),\n",
       " (45.09, 71.15),\n",
       " (33.28, 50.64),\n",
       " (37.41, 57.72),\n",
       " (54.63, 88.2),\n",
       " (75.67, 127.03),\n",
       " (65.61, 108.27),\n",
       " (32.26, 48.9),\n",
       " (47.33, 75.11),\n",
       " (27.66, 41.17),\n",
       " (28.08, 41.86),\n",
       " (32.79, 49.79),\n",
       " (32.25, 48.88),\n",
       " (50.44, 80.66),\n",
       " (58.69, 95.56),\n",
       " (90.04, 154.33),\n",
       " (51.73, 82.97),\n",
       " (47.24, 74.95),\n",
       " (47.83, 76.01),\n",
       " (52.51, 84.37),\n",
       " (47.23, 74.94),\n",
       " (29.91, 44.93),\n",
       " (43.23, 67.87),\n",
       " (97.85, 169.39),\n",
       " (43.71, 68.71),\n",
       " (32.22, 48.83),\n",
       " (58.61, 95.42),\n",
       " (18.76, 26.65),\n",
       " (32.86, 49.93),\n",
       " (67.99, 112.67),\n",
       " (48.94, 77.98),\n",
       " (55.61, 89.98),\n",
       " (29.44, 44.14),\n",
       " (36.32, 55.85),\n",
       " (24.81, 36.44),\n",
       " (30.66, 46.2),\n",
       " (59.91, 97.79),\n",
       " (88.28, 150.95),\n",
       " (25.83, 38.13),\n",
       " (28.71, 42.92),\n",
       " (33.91, 51.7),\n",
       " (44.11, 69.41),\n",
       " (38.29, 59.24),\n",
       " (63.4, 104.2),\n",
       " (47.4, 75.23),\n",
       " (31.1, 46.93),\n",
       " (32.07, 48.57),\n",
       " (95.72, 165.27),\n",
       " (98.1, 169.88),\n",
       " (70.52, 117.39),\n",
       " (57.97, 94.25),\n",
       " (26.11, 38.59),\n",
       " (34.08, 52.0),\n",
       " (40.57, 63.21),\n",
       " (36.69, 56.48),\n",
       " (49.12, 78.31),\n",
       " (33.19, 50.49),\n",
       " (57.82, 93.98),\n",
       " (48.84, 77.79),\n",
       " (50.47, 80.72),\n",
       " (44.44, 70.0),\n",
       " (50.39, 80.58),\n",
       " (27.03, 40.11),\n",
       " (64.92, 107.0),\n",
       " (33.29, 50.65),\n",
       " (41.73, 65.23),\n",
       " (35.6, 54.6),\n",
       " (29.44, 44.14),\n",
       " (35.08, 53.71),\n",
       " (65.8, 108.63),\n",
       " (23.96, 35.04),\n",
       " (32.53, 49.36),\n",
       " (31.22, 47.14),\n",
       " (48.06, 76.41),\n",
       " (29.01, 43.41),\n",
       " (24.82, 36.45),\n",
       " (76.54, 128.66),\n",
       " (44.75, 70.54),\n",
       " (61.17, 100.1),\n",
       " (47.73, 75.83),\n",
       " (30.79, 46.42),\n",
       " (42.88, 67.26),\n",
       " (52.14, 83.7),\n",
       " (29.18, 43.71),\n",
       " (62.52, 102.58),\n",
       " (54.62, 88.19),\n",
       " (53.78, 86.67),\n",
       " (29.86, 44.85),\n",
       " (46.98, 74.5),\n",
       " (37.25, 57.45),\n",
       " (31.62, 47.82),\n",
       " (29.66, 44.51),\n",
       " (69.51, 115.5),\n",
       " (53.45, 86.07),\n",
       " (33.38, 50.8),\n",
       " (29.5, 44.24),\n",
       " (41.92, 65.56),\n",
       " (31.49, 47.59),\n",
       " (39.15, 60.73),\n",
       " (27.35, 40.65),\n",
       " (52.7, 84.72),\n",
       " (47.03, 74.57),\n",
       " (29.74, 44.64),\n",
       " (41.24, 64.38),\n",
       " (82.69, 140.3),\n",
       " (49.36, 78.72),\n",
       " (28.85, 43.16),\n",
       " (42.51, 66.6),\n",
       " (34.34, 52.45),\n",
       " (40.11, 62.41),\n",
       " (47.74, 75.84),\n",
       " (31.8, 48.11),\n",
       " (45.32, 71.54),\n",
       " (66.86, 110.58),\n",
       " (24.49, 35.92),\n",
       " (28.54, 42.62),\n",
       " (56.39, 91.38),\n",
       " (81.4, 137.84),\n",
       " (61.49, 100.69),\n",
       " (48.43, 77.06),\n",
       " (52.07, 83.58),\n",
       " (47.94, 76.2),\n",
       " (63.11, 103.67),\n",
       " (37.89, 58.56),\n",
       " (48.9, 77.92),\n",
       " (48.86, 77.84),\n",
       " (41.53, 64.88),\n",
       " (27.54, 40.95),\n",
       " (50.19, 80.22),\n",
       " (90.8, 155.8),\n",
       " (37.09, 57.16),\n",
       " (32.8, 49.81),\n",
       " (55.67, 90.08),\n",
       " (39.65, 61.6),\n",
       " (26.45, 39.16),\n",
       " (34.65, 52.98),\n",
       " (49.9, 79.69),\n",
       " (29.13, 43.61),\n",
       " (46.76, 74.1),\n",
       " (24.08, 35.25),\n",
       " (56.25, 91.13),\n",
       " (51.17, 81.97),\n",
       " (52.48, 84.33),\n",
       " (54.29, 87.59),\n",
       " (47.97, 76.25),\n",
       " (49.36, 78.72),\n",
       " (31.77, 48.07),\n",
       " (30.39, 45.74),\n",
       " (34.21, 52.22),\n",
       " (79.76, 134.74),\n",
       " (31.83, 48.18),\n",
       " (70.05, 116.52),\n",
       " (33.69, 51.34),\n",
       " (56.07, 90.8),\n",
       " (46.16, 73.04),\n",
       " (48.48, 77.15),\n",
       " (24.85, 36.5),\n",
       " (52.1, 83.65),\n",
       " (58.33, 94.92),\n",
       " (30.52, 45.95),\n",
       " (27.5, 40.89),\n",
       " (29.44, 44.13),\n",
       " (44.33, 69.81),\n",
       " (62.11, 101.83),\n",
       " (61.37, 100.46),\n",
       " (66.56, 110.04),\n",
       " (61.44, 100.6),\n",
       " (30.28, 45.56),\n",
       " (26.93, 39.95),\n",
       " (56.91, 92.33),\n",
       " (31.81, 48.14),\n",
       " (36.1, 55.47),\n",
       " (22.73, 33.04),\n",
       " (54.81, 88.53),\n",
       " (30.71, 46.28),\n",
       " (53.84, 86.78),\n",
       " (46.93, 74.4),\n",
       " (99.33, 172.27),\n",
       " (35.01, 53.59),\n",
       " (51.71, 82.94),\n",
       " (57.49, 93.38),\n",
       " (30.51, 45.93),\n",
       " (47.0, 74.53),\n",
       " (54.88, 88.64),\n",
       " (64.2, 105.68),\n",
       " (30.19, 45.39),\n",
       " (44.61, 70.3),\n",
       " (34.2, 52.21),\n",
       " (31.57, 47.73),\n",
       " (51.21, 82.05),\n",
       " (49.91, 79.7),\n",
       " (35.74, 54.84),\n",
       " (42.31, 66.25),\n",
       " (74.3, 124.46),\n",
       " (30.62, 46.12),\n",
       " (26.1, 38.56),\n",
       " (30.49, 45.9),\n",
       " (57.78, 93.91),\n",
       " (70.97, 118.23),\n",
       " (42.17, 66.0),\n",
       " (73.24, 122.46),\n",
       " (64.71, 106.61),\n",
       " (27.51, 40.91),\n",
       " (32.0, 48.46),\n",
       " (38.21, 59.11),\n",
       " (57.79, 93.93),\n",
       " (88.52, 151.41),\n",
       " (47.27, 75.01),\n",
       " (53.45, 86.07),\n",
       " (30.62, 46.12),\n",
       " (34.96, 53.5),\n",
       " (62.05, 101.72),\n",
       " (27.05, 40.15),\n",
       " (29.06, 43.51),\n",
       " (27.61, 41.08),\n",
       " (34.51, 52.74),\n",
       " (30.28, 45.55),\n",
       " (60.61, 99.07),\n",
       " (64.27, 105.8),\n",
       " (73.7, 123.33),\n",
       " (50.77, 81.25),\n",
       " (47.26, 74.99),\n",
       " (68.65, 113.91),\n",
       " (41.55, 64.91),\n",
       " (26.19, 38.72),\n",
       " (102.07, 177.6),\n",
       " (29.69, 44.55),\n",
       " (35.25, 54.01),\n",
       " (29.85, 44.82),\n",
       " (68.56, 113.74),\n",
       " (35.58, 54.56),\n",
       " (62.96, 103.4),\n",
       " (29.07, 43.51),\n",
       " (62.79, 103.08),\n",
       " (107.73, 188.67),\n",
       " (42.79, 67.08),\n",
       " (46.26, 73.22),\n",
       " (29.32, 43.94),\n",
       " (42.17, 66.0),\n",
       " (58.84, 95.84),\n",
       " (30.29, 45.57),\n",
       " (55.49, 89.76),\n",
       " (69.77, 115.98),\n",
       " (25.91, 38.26),\n",
       " (26.48, 39.2),\n",
       " (25.5, 37.57),\n",
       " (33.18, 50.47),\n",
       " (30.21, 45.43),\n",
       " (58.36, 94.97),\n",
       " (42.99, 67.44),\n",
       " (31.62, 47.81),\n",
       " (56.25, 91.13),\n",
       " (40.78, 63.57),\n",
       " (51.85, 83.18),\n",
       " (25.12, 36.96),\n",
       " (30.38, 45.72),\n",
       " (63.32, 104.04),\n",
       " (30.7, 46.26),\n",
       " (33.92, 51.73),\n",
       " (31.15, 47.02),\n",
       " (54.98, 88.82),\n",
       " (17.36, 24.44),\n",
       " (61.67, 101.01),\n",
       " (42.89, 67.27),\n",
       " (44.52, 70.14),\n",
       " (26.1, 38.58),\n",
       " (56.58, 91.73),\n",
       " (31.77, 48.06),\n",
       " (62.7, 102.91),\n",
       " (29.56, 44.34),\n",
       " (23.95, 35.03),\n",
       " (33.53, 51.06),\n",
       " (30.89, 46.58),\n",
       " (31.8, 48.13),\n",
       " (58.65, 95.49),\n",
       " (45.97, 72.69),\n",
       " (43.58, 68.48),\n",
       " (45.13, 71.22),\n",
       " (48.54, 77.27),\n",
       " (36.8, 56.66),\n",
       " (30.76, 46.36),\n",
       " (42.6, 66.76),\n",
       " (46.12, 72.97),\n",
       " (71.6, 119.41),\n",
       " (30.19, 45.4),\n",
       " (34.67, 53.01),\n",
       " (53.37, 85.93),\n",
       " (98.77, 171.18),\n",
       " (50.99, 81.65),\n",
       " (89.19, 152.7),\n",
       " (34.84, 53.3),\n",
       " (49.07, 78.21),\n",
       " (35.2, 53.91),\n",
       " ...)"
      ]
     },
     "execution_count": 1005,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate price interval for MAPE median\n",
    "y_pred_interval_rf_reg = tuple([(round(math.exp(el-el*MAPE_median_rf_reg),2),round(math.exp(el+el*MAPE_median_rf_reg),2)) for el in y_test_pred_rf_reg])\n",
    "y_pred_interval_rf_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model and Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model and cv\n",
    "save_model(best_model_rf_reg, title=\"best_model_rf_reg_01\", save=\"joblib\")\n",
    "save_model(grid_rf_reg, title=\"best_cv_rf_reg_01\", save=\"joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg Model 4: CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1397,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create pipeline to use in RandomizedSearchCV and GridSearchCV\n",
    "pipeline_cat_reg = Pipeline([('preprocessor', preprocessor),\n",
    "                             ('cat_reg',\n",
    "                              CatBoostRegressor(n_estimators=173,\n",
    "                                                learning_rate=0.25,\n",
    "                                                l2_leaf_reg=7,\n",
    "                                                loss_function=\"RMSE\",\n",
    "                                                random_state=random_state,\n",
    "                                                depth=6))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Pre-Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss_function'])"
      ]
     },
     "execution_count": 1056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display possible hyperparameters for XGBoost Regressor\n",
    "test_cat_reg = CatBoostRegressor()\n",
    "test_cat_reg.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default values for CatBoostRegressor** (as base for hyperparameter search):\n",
    "\n",
    "iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function='RMSE', border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, one_hot_max_size=None, random_strength=None, name=None, ignored_features=None, train_dir=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_frequency=None, sampling_unit=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, boost_from_average=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter distribution\n",
    "param_distribs_cat_reg = {\n",
    "    'cat_reg__n_estimators': randint(low=130, high=180),    # initial: randint(low=10, high=200)\n",
    "    'cat_reg__l2_leaf_reg': randint(low=2, high=11),       # initial: randint(low=1, high=15)\n",
    "    'cat_reg__depth': randint(low=4, high=6),             # initial: randint(low=1, high=15)\n",
    "    'cat_reg__learning_rate': [0.15, 0.18, 0.2, 0.22, 0.25, 0.27, 0.3] # initial: [0.01, 0.02, 0.05, 0.1, 0.2, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   23.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.1056262\ttotal: 2.84ms\tremaining: 386ms\n",
      "1:\tlearn: 0.0989804\ttotal: 5.5ms\tremaining: 371ms\n",
      "2:\tlearn: 0.0937988\ttotal: 8.32ms\tremaining: 372ms\n",
      "3:\tlearn: 0.0899848\ttotal: 10.7ms\tremaining: 357ms\n",
      "4:\tlearn: 0.0876377\ttotal: 13.2ms\tremaining: 348ms\n",
      "5:\tlearn: 0.0857052\ttotal: 15.7ms\tremaining: 343ms\n",
      "6:\tlearn: 0.0840714\ttotal: 18.2ms\tremaining: 338ms\n",
      "7:\tlearn: 0.0829786\ttotal: 20.6ms\tremaining: 333ms\n",
      "8:\tlearn: 0.0819374\ttotal: 23.1ms\tremaining: 329ms\n",
      "9:\tlearn: 0.0808761\ttotal: 26.1ms\tremaining: 331ms\n",
      "10:\tlearn: 0.0800886\ttotal: 28.7ms\tremaining: 329ms\n",
      "11:\tlearn: 0.0793616\ttotal: 31.3ms\tremaining: 326ms\n",
      "12:\tlearn: 0.0789661\ttotal: 33.7ms\tremaining: 322ms\n",
      "13:\tlearn: 0.0783590\ttotal: 36.2ms\tremaining: 318ms\n",
      "14:\tlearn: 0.0780044\ttotal: 38.5ms\tremaining: 313ms\n",
      "15:\tlearn: 0.0773772\ttotal: 41.1ms\tremaining: 311ms\n",
      "16:\tlearn: 0.0770678\ttotal: 43.6ms\tremaining: 307ms\n",
      "17:\tlearn: 0.0767012\ttotal: 45.9ms\tremaining: 304ms\n",
      "18:\tlearn: 0.0764759\ttotal: 48.4ms\tremaining: 301ms\n",
      "19:\tlearn: 0.0761932\ttotal: 50.9ms\tremaining: 298ms\n",
      "20:\tlearn: 0.0759238\ttotal: 53.2ms\tremaining: 294ms\n",
      "21:\tlearn: 0.0757017\ttotal: 55.7ms\tremaining: 291ms\n",
      "22:\tlearn: 0.0754935\ttotal: 58.1ms\tremaining: 288ms\n",
      "23:\tlearn: 0.0752253\ttotal: 60.4ms\tremaining: 284ms\n",
      "24:\tlearn: 0.0750634\ttotal: 62.9ms\tremaining: 282ms\n",
      "25:\tlearn: 0.0748983\ttotal: 65.3ms\tremaining: 279ms\n",
      "26:\tlearn: 0.0747689\ttotal: 67.6ms\tremaining: 276ms\n",
      "27:\tlearn: 0.0746655\ttotal: 69.9ms\tremaining: 272ms\n",
      "28:\tlearn: 0.0745142\ttotal: 72.3ms\tremaining: 269ms\n",
      "29:\tlearn: 0.0743669\ttotal: 74.7ms\tremaining: 267ms\n",
      "30:\tlearn: 0.0741866\ttotal: 77.1ms\tremaining: 264ms\n",
      "31:\tlearn: 0.0740777\ttotal: 79.7ms\tremaining: 262ms\n",
      "32:\tlearn: 0.0738901\ttotal: 82.6ms\tremaining: 260ms\n",
      "33:\tlearn: 0.0737947\ttotal: 85ms\tremaining: 257ms\n",
      "34:\tlearn: 0.0737045\ttotal: 87.4ms\tremaining: 255ms\n",
      "35:\tlearn: 0.0735453\ttotal: 89.8ms\tremaining: 252ms\n",
      "36:\tlearn: 0.0734844\ttotal: 92.3ms\tremaining: 249ms\n",
      "37:\tlearn: 0.0733769\ttotal: 94.7ms\tremaining: 247ms\n",
      "38:\tlearn: 0.0732784\ttotal: 97.1ms\tremaining: 244ms\n",
      "39:\tlearn: 0.0731989\ttotal: 99.5ms\tremaining: 241ms\n",
      "40:\tlearn: 0.0730911\ttotal: 102ms\tremaining: 239ms\n",
      "41:\tlearn: 0.0729723\ttotal: 104ms\tremaining: 236ms\n",
      "42:\tlearn: 0.0727820\ttotal: 107ms\tremaining: 234ms\n",
      "43:\tlearn: 0.0726761\ttotal: 109ms\tremaining: 231ms\n",
      "44:\tlearn: 0.0726265\ttotal: 112ms\tremaining: 229ms\n",
      "45:\tlearn: 0.0725302\ttotal: 114ms\tremaining: 226ms\n",
      "46:\tlearn: 0.0724134\ttotal: 117ms\tremaining: 224ms\n",
      "47:\tlearn: 0.0722621\ttotal: 119ms\tremaining: 222ms\n",
      "48:\tlearn: 0.0721483\ttotal: 122ms\tremaining: 219ms\n",
      "49:\tlearn: 0.0720638\ttotal: 126ms\tremaining: 219ms\n",
      "50:\tlearn: 0.0719226\ttotal: 130ms\tremaining: 219ms\n",
      "51:\tlearn: 0.0718120\ttotal: 134ms\tremaining: 219ms\n",
      "52:\tlearn: 0.0716762\ttotal: 138ms\tremaining: 219ms\n",
      "53:\tlearn: 0.0715583\ttotal: 142ms\tremaining: 218ms\n",
      "54:\tlearn: 0.0714664\ttotal: 146ms\tremaining: 217ms\n",
      "55:\tlearn: 0.0713887\ttotal: 149ms\tremaining: 216ms\n",
      "56:\tlearn: 0.0712740\ttotal: 153ms\tremaining: 214ms\n",
      "57:\tlearn: 0.0711595\ttotal: 157ms\tremaining: 213ms\n",
      "58:\tlearn: 0.0710876\ttotal: 162ms\tremaining: 214ms\n",
      "59:\tlearn: 0.0710008\ttotal: 165ms\tremaining: 212ms\n",
      "60:\tlearn: 0.0709030\ttotal: 168ms\tremaining: 210ms\n",
      "61:\tlearn: 0.0708347\ttotal: 172ms\tremaining: 208ms\n",
      "62:\tlearn: 0.0707226\ttotal: 175ms\tremaining: 205ms\n",
      "63:\tlearn: 0.0706442\ttotal: 178ms\tremaining: 203ms\n",
      "64:\tlearn: 0.0705597\ttotal: 180ms\tremaining: 199ms\n",
      "65:\tlearn: 0.0704563\ttotal: 183ms\tremaining: 197ms\n",
      "66:\tlearn: 0.0703563\ttotal: 186ms\tremaining: 194ms\n",
      "67:\tlearn: 0.0701915\ttotal: 189ms\tremaining: 192ms\n",
      "68:\tlearn: 0.0700901\ttotal: 192ms\tremaining: 190ms\n",
      "69:\tlearn: 0.0700143\ttotal: 196ms\tremaining: 187ms\n",
      "70:\tlearn: 0.0699342\ttotal: 199ms\tremaining: 185ms\n",
      "71:\tlearn: 0.0698637\ttotal: 204ms\tremaining: 184ms\n",
      "72:\tlearn: 0.0697911\ttotal: 207ms\tremaining: 182ms\n",
      "73:\tlearn: 0.0697042\ttotal: 211ms\tremaining: 180ms\n",
      "74:\tlearn: 0.0696492\ttotal: 215ms\tremaining: 178ms\n",
      "75:\tlearn: 0.0695999\ttotal: 219ms\tremaining: 175ms\n",
      "76:\tlearn: 0.0694977\ttotal: 223ms\tremaining: 173ms\n",
      "77:\tlearn: 0.0694028\ttotal: 226ms\tremaining: 171ms\n",
      "78:\tlearn: 0.0693140\ttotal: 230ms\tremaining: 169ms\n",
      "79:\tlearn: 0.0692437\ttotal: 234ms\tremaining: 167ms\n",
      "80:\tlearn: 0.0691526\ttotal: 237ms\tremaining: 164ms\n",
      "81:\tlearn: 0.0690871\ttotal: 241ms\tremaining: 162ms\n",
      "82:\tlearn: 0.0690397\ttotal: 244ms\tremaining: 159ms\n",
      "83:\tlearn: 0.0689185\ttotal: 247ms\tremaining: 156ms\n",
      "84:\tlearn: 0.0688514\ttotal: 250ms\tremaining: 153ms\n",
      "85:\tlearn: 0.0687822\ttotal: 252ms\tremaining: 150ms\n",
      "86:\tlearn: 0.0687192\ttotal: 255ms\tremaining: 146ms\n",
      "87:\tlearn: 0.0686233\ttotal: 257ms\tremaining: 143ms\n",
      "88:\tlearn: 0.0685571\ttotal: 260ms\tremaining: 140ms\n",
      "89:\tlearn: 0.0684885\ttotal: 262ms\tremaining: 137ms\n",
      "90:\tlearn: 0.0684265\ttotal: 265ms\tremaining: 134ms\n",
      "91:\tlearn: 0.0683076\ttotal: 267ms\tremaining: 131ms\n",
      "92:\tlearn: 0.0682196\ttotal: 270ms\tremaining: 128ms\n",
      "93:\tlearn: 0.0681473\ttotal: 272ms\tremaining: 125ms\n",
      "94:\tlearn: 0.0680894\ttotal: 275ms\tremaining: 121ms\n",
      "95:\tlearn: 0.0680338\ttotal: 277ms\tremaining: 118ms\n",
      "96:\tlearn: 0.0679751\ttotal: 280ms\tremaining: 115ms\n",
      "97:\tlearn: 0.0679348\ttotal: 282ms\tremaining: 112ms\n",
      "98:\tlearn: 0.0678875\ttotal: 284ms\tremaining: 109ms\n",
      "99:\tlearn: 0.0678408\ttotal: 287ms\tremaining: 106ms\n",
      "100:\tlearn: 0.0677459\ttotal: 289ms\tremaining: 103ms\n",
      "101:\tlearn: 0.0677030\ttotal: 292ms\tremaining: 100ms\n",
      "102:\tlearn: 0.0676069\ttotal: 294ms\tremaining: 97ms\n",
      "103:\tlearn: 0.0675579\ttotal: 296ms\tremaining: 94.1ms\n",
      "104:\tlearn: 0.0675199\ttotal: 299ms\tremaining: 91.1ms\n",
      "105:\tlearn: 0.0674503\ttotal: 301ms\tremaining: 88.1ms\n",
      "106:\tlearn: 0.0673929\ttotal: 304ms\tremaining: 85.1ms\n",
      "107:\tlearn: 0.0673438\ttotal: 307ms\tremaining: 82.3ms\n",
      "108:\tlearn: 0.0672995\ttotal: 309ms\tremaining: 79.4ms\n",
      "109:\tlearn: 0.0672101\ttotal: 312ms\tremaining: 76.5ms\n",
      "110:\tlearn: 0.0671735\ttotal: 314ms\tremaining: 73.5ms\n",
      "111:\tlearn: 0.0671210\ttotal: 316ms\tremaining: 70.6ms\n",
      "112:\tlearn: 0.0670615\ttotal: 319ms\tremaining: 67.7ms\n",
      "113:\tlearn: 0.0670244\ttotal: 321ms\tremaining: 64.8ms\n",
      "114:\tlearn: 0.0669588\ttotal: 324ms\tremaining: 61.9ms\n",
      "115:\tlearn: 0.0668833\ttotal: 326ms\tremaining: 59ms\n",
      "116:\tlearn: 0.0667932\ttotal: 329ms\tremaining: 56.2ms\n",
      "117:\tlearn: 0.0667527\ttotal: 331ms\tremaining: 53.4ms\n",
      "118:\tlearn: 0.0666787\ttotal: 334ms\tremaining: 50.5ms\n",
      "119:\tlearn: 0.0666274\ttotal: 336ms\tremaining: 47.6ms\n",
      "120:\tlearn: 0.0665710\ttotal: 339ms\tremaining: 44.8ms\n",
      "121:\tlearn: 0.0665333\ttotal: 341ms\tremaining: 41.9ms\n",
      "122:\tlearn: 0.0664932\ttotal: 343ms\tremaining: 39.1ms\n",
      "123:\tlearn: 0.0664497\ttotal: 346ms\tremaining: 36.2ms\n",
      "124:\tlearn: 0.0664124\ttotal: 348ms\tremaining: 33.4ms\n",
      "125:\tlearn: 0.0663542\ttotal: 350ms\tremaining: 30.6ms\n",
      "126:\tlearn: 0.0662939\ttotal: 353ms\tremaining: 27.8ms\n",
      "127:\tlearn: 0.0662566\ttotal: 355ms\tremaining: 25ms\n",
      "128:\tlearn: 0.0662334\ttotal: 357ms\tremaining: 22.2ms\n",
      "129:\tlearn: 0.0661694\ttotal: 360ms\tremaining: 19.4ms\n",
      "130:\tlearn: 0.0661161\ttotal: 362ms\tremaining: 16.6ms\n",
      "131:\tlearn: 0.0660716\ttotal: 365ms\tremaining: 13.8ms\n",
      "132:\tlearn: 0.0660107\ttotal: 367ms\tremaining: 11ms\n",
      "133:\tlearn: 0.0659566\ttotal: 370ms\tremaining: 8.28ms\n",
      "134:\tlearn: 0.0659240\ttotal: 372ms\tremaining: 5.51ms\n",
      "135:\tlearn: 0.0658773\ttotal: 375ms\tremaining: 2.75ms\n",
      "136:\tlearn: 0.0658507\ttotal: 377ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "# Create and fit RandomizedSearchCV, save \"best_model\"\n",
    "rnd_cat_reg = RandomizedSearchCV(pipeline_cat_reg,\n",
    "                                 param_distribs_cat_reg,\n",
    "                                 cv=5,\n",
    "                                 scoring=scoring,\n",
    "                                 n_iter=15,\n",
    "                                 return_train_score=True,\n",
    "                                 verbose=4,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "best_model_rnd_cat_reg = rnd_cat_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1370,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "-0.23\n",
      "Best parameters:\n",
      "{'cat_reg__depth': 5, 'cat_reg__l2_leaf_reg': 8, 'cat_reg__learning_rate': 0.22, 'cat_reg__n_estimators': 137}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(rnd_cat_reg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(rnd_cat_reg.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(rnd_cat_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1398,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid_cat_reg = {\n",
    "#    'cat_reg__n_estimators': [150, 155],\n",
    "#    'cat_reg__l2_leaf_reg': [3, 4],\n",
    "    'cat_reg__depth': [5, 6],\n",
    "#    'cat_reg__learning_rate': [0.15, 0.2, 0.25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1399,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    4.4s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5181187\ttotal: 2.95ms\tremaining: 508ms\n",
      "1:\tlearn: 0.4815579\ttotal: 5.68ms\tremaining: 486ms\n",
      "2:\tlearn: 0.4570183\ttotal: 8.65ms\tremaining: 490ms\n",
      "3:\tlearn: 0.4400758\ttotal: 11.3ms\tremaining: 476ms\n",
      "4:\tlearn: 0.4289910\ttotal: 13.7ms\tremaining: 462ms\n",
      "5:\tlearn: 0.4209999\ttotal: 16.3ms\tremaining: 453ms\n",
      "6:\tlearn: 0.4141739\ttotal: 18.9ms\tremaining: 447ms\n",
      "7:\tlearn: 0.4086418\ttotal: 21.5ms\tremaining: 444ms\n",
      "8:\tlearn: 0.4029687\ttotal: 24.2ms\tremaining: 441ms\n",
      "9:\tlearn: 0.4003921\ttotal: 26.7ms\tremaining: 435ms\n",
      "10:\tlearn: 0.3968916\ttotal: 29.4ms\tremaining: 432ms\n",
      "11:\tlearn: 0.3946512\ttotal: 32ms\tremaining: 430ms\n",
      "12:\tlearn: 0.3928321\ttotal: 34.5ms\tremaining: 425ms\n",
      "13:\tlearn: 0.3907135\ttotal: 37.2ms\tremaining: 423ms\n",
      "14:\tlearn: 0.3890330\ttotal: 39.9ms\tremaining: 420ms\n",
      "15:\tlearn: 0.3870456\ttotal: 43.2ms\tremaining: 424ms\n",
      "16:\tlearn: 0.3855917\ttotal: 45.9ms\tremaining: 421ms\n",
      "17:\tlearn: 0.3845600\ttotal: 48.5ms\tremaining: 417ms\n",
      "18:\tlearn: 0.3835890\ttotal: 51.1ms\tremaining: 414ms\n",
      "19:\tlearn: 0.3819666\ttotal: 53.6ms\tremaining: 410ms\n",
      "20:\tlearn: 0.3810367\ttotal: 56.3ms\tremaining: 408ms\n",
      "21:\tlearn: 0.3800945\ttotal: 59.5ms\tremaining: 409ms\n",
      "22:\tlearn: 0.3791030\ttotal: 62.2ms\tremaining: 406ms\n",
      "23:\tlearn: 0.3780925\ttotal: 65.1ms\tremaining: 404ms\n",
      "24:\tlearn: 0.3773141\ttotal: 68ms\tremaining: 402ms\n",
      "25:\tlearn: 0.3764395\ttotal: 70.7ms\tremaining: 400ms\n",
      "26:\tlearn: 0.3758171\ttotal: 73.5ms\tremaining: 397ms\n",
      "27:\tlearn: 0.3750296\ttotal: 76.3ms\tremaining: 395ms\n",
      "28:\tlearn: 0.3744563\ttotal: 78.9ms\tremaining: 392ms\n",
      "29:\tlearn: 0.3737381\ttotal: 81.4ms\tremaining: 388ms\n",
      "30:\tlearn: 0.3732742\ttotal: 84.1ms\tremaining: 385ms\n",
      "31:\tlearn: 0.3726712\ttotal: 86.7ms\tremaining: 382ms\n",
      "32:\tlearn: 0.3720367\ttotal: 89.3ms\tremaining: 379ms\n",
      "33:\tlearn: 0.3715932\ttotal: 91.9ms\tremaining: 376ms\n",
      "34:\tlearn: 0.3712869\ttotal: 94.3ms\tremaining: 372ms\n",
      "35:\tlearn: 0.3709726\ttotal: 97ms\tremaining: 369ms\n",
      "36:\tlearn: 0.3705730\ttotal: 99.6ms\tremaining: 366ms\n",
      "37:\tlearn: 0.3694634\ttotal: 102ms\tremaining: 364ms\n",
      "38:\tlearn: 0.3688641\ttotal: 105ms\tremaining: 361ms\n",
      "39:\tlearn: 0.3682294\ttotal: 108ms\tremaining: 358ms\n",
      "40:\tlearn: 0.3672959\ttotal: 110ms\tremaining: 356ms\n",
      "41:\tlearn: 0.3665582\ttotal: 113ms\tremaining: 353ms\n",
      "42:\tlearn: 0.3658769\ttotal: 116ms\tremaining: 350ms\n",
      "43:\tlearn: 0.3652444\ttotal: 118ms\tremaining: 347ms\n",
      "44:\tlearn: 0.3648216\ttotal: 121ms\tremaining: 344ms\n",
      "45:\tlearn: 0.3642565\ttotal: 123ms\tremaining: 341ms\n",
      "46:\tlearn: 0.3638112\ttotal: 126ms\tremaining: 338ms\n",
      "47:\tlearn: 0.3634654\ttotal: 130ms\tremaining: 338ms\n",
      "48:\tlearn: 0.3630850\ttotal: 134ms\tremaining: 340ms\n",
      "49:\tlearn: 0.3625556\ttotal: 139ms\tremaining: 341ms\n",
      "50:\tlearn: 0.3620499\ttotal: 143ms\tremaining: 342ms\n",
      "51:\tlearn: 0.3613709\ttotal: 147ms\tremaining: 343ms\n",
      "52:\tlearn: 0.3609522\ttotal: 152ms\tremaining: 344ms\n",
      "53:\tlearn: 0.3603847\ttotal: 156ms\tremaining: 344ms\n",
      "54:\tlearn: 0.3600216\ttotal: 161ms\tremaining: 345ms\n",
      "55:\tlearn: 0.3592386\ttotal: 166ms\tremaining: 346ms\n",
      "56:\tlearn: 0.3586679\ttotal: 174ms\tremaining: 353ms\n",
      "57:\tlearn: 0.3583025\ttotal: 177ms\tremaining: 351ms\n",
      "58:\tlearn: 0.3579439\ttotal: 184ms\tremaining: 356ms\n",
      "59:\tlearn: 0.3575342\ttotal: 189ms\tremaining: 355ms\n",
      "60:\tlearn: 0.3569443\ttotal: 195ms\tremaining: 358ms\n",
      "61:\tlearn: 0.3563122\ttotal: 200ms\tremaining: 358ms\n",
      "62:\tlearn: 0.3553919\ttotal: 205ms\tremaining: 357ms\n",
      "63:\tlearn: 0.3550669\ttotal: 209ms\tremaining: 355ms\n",
      "64:\tlearn: 0.3547782\ttotal: 213ms\tremaining: 354ms\n",
      "65:\tlearn: 0.3543277\ttotal: 218ms\tremaining: 353ms\n",
      "66:\tlearn: 0.3539706\ttotal: 222ms\tremaining: 352ms\n",
      "67:\tlearn: 0.3535485\ttotal: 227ms\tremaining: 350ms\n",
      "68:\tlearn: 0.3531551\ttotal: 231ms\tremaining: 349ms\n",
      "69:\tlearn: 0.3525126\ttotal: 236ms\tremaining: 347ms\n",
      "70:\tlearn: 0.3519903\ttotal: 240ms\tremaining: 344ms\n",
      "71:\tlearn: 0.3514582\ttotal: 244ms\tremaining: 343ms\n",
      "72:\tlearn: 0.3508541\ttotal: 249ms\tremaining: 341ms\n",
      "73:\tlearn: 0.3505738\ttotal: 253ms\tremaining: 339ms\n",
      "74:\tlearn: 0.3500497\ttotal: 258ms\tremaining: 337ms\n",
      "75:\tlearn: 0.3495491\ttotal: 262ms\tremaining: 335ms\n",
      "76:\tlearn: 0.3488774\ttotal: 268ms\tremaining: 334ms\n",
      "77:\tlearn: 0.3483607\ttotal: 273ms\tremaining: 332ms\n",
      "78:\tlearn: 0.3479564\ttotal: 277ms\tremaining: 330ms\n",
      "79:\tlearn: 0.3477140\ttotal: 282ms\tremaining: 327ms\n",
      "80:\tlearn: 0.3474767\ttotal: 286ms\tremaining: 325ms\n",
      "81:\tlearn: 0.3472982\ttotal: 290ms\tremaining: 322ms\n",
      "82:\tlearn: 0.3469693\ttotal: 294ms\tremaining: 319ms\n",
      "83:\tlearn: 0.3466263\ttotal: 299ms\tremaining: 317ms\n",
      "84:\tlearn: 0.3460588\ttotal: 304ms\tremaining: 314ms\n",
      "85:\tlearn: 0.3457366\ttotal: 308ms\tremaining: 312ms\n",
      "86:\tlearn: 0.3455296\ttotal: 313ms\tremaining: 309ms\n",
      "87:\tlearn: 0.3452358\ttotal: 317ms\tremaining: 306ms\n",
      "88:\tlearn: 0.3450638\ttotal: 321ms\tremaining: 303ms\n",
      "89:\tlearn: 0.3448278\ttotal: 325ms\tremaining: 300ms\n",
      "90:\tlearn: 0.3446728\ttotal: 330ms\tremaining: 297ms\n",
      "91:\tlearn: 0.3443708\ttotal: 334ms\tremaining: 294ms\n",
      "92:\tlearn: 0.3442448\ttotal: 338ms\tremaining: 291ms\n",
      "93:\tlearn: 0.3440159\ttotal: 347ms\tremaining: 292ms\n",
      "94:\tlearn: 0.3438238\ttotal: 351ms\tremaining: 289ms\n",
      "95:\tlearn: 0.3434349\ttotal: 359ms\tremaining: 288ms\n",
      "96:\tlearn: 0.3430826\ttotal: 364ms\tremaining: 285ms\n",
      "97:\tlearn: 0.3428807\ttotal: 368ms\tremaining: 281ms\n",
      "98:\tlearn: 0.3425415\ttotal: 372ms\tremaining: 278ms\n",
      "99:\tlearn: 0.3420916\ttotal: 377ms\tremaining: 275ms\n",
      "100:\tlearn: 0.3416972\ttotal: 387ms\tremaining: 276ms\n",
      "101:\tlearn: 0.3415242\ttotal: 396ms\tremaining: 276ms\n",
      "102:\tlearn: 0.3414472\ttotal: 401ms\tremaining: 272ms\n",
      "103:\tlearn: 0.3412645\ttotal: 404ms\tremaining: 268ms\n",
      "104:\tlearn: 0.3408980\ttotal: 408ms\tremaining: 264ms\n",
      "105:\tlearn: 0.3404821\ttotal: 413ms\tremaining: 261ms\n",
      "106:\tlearn: 0.3401715\ttotal: 417ms\tremaining: 257ms\n",
      "107:\tlearn: 0.3395514\ttotal: 421ms\tremaining: 254ms\n",
      "108:\tlearn: 0.3392896\ttotal: 425ms\tremaining: 250ms\n",
      "109:\tlearn: 0.3388975\ttotal: 430ms\tremaining: 246ms\n",
      "110:\tlearn: 0.3386710\ttotal: 434ms\tremaining: 242ms\n",
      "111:\tlearn: 0.3384374\ttotal: 438ms\tremaining: 238ms\n",
      "112:\tlearn: 0.3380735\ttotal: 442ms\tremaining: 235ms\n",
      "113:\tlearn: 0.3378825\ttotal: 449ms\tremaining: 232ms\n",
      "114:\tlearn: 0.3376891\ttotal: 453ms\tremaining: 229ms\n",
      "115:\tlearn: 0.3373357\ttotal: 458ms\tremaining: 225ms\n",
      "116:\tlearn: 0.3370164\ttotal: 462ms\tremaining: 221ms\n",
      "117:\tlearn: 0.3368981\ttotal: 465ms\tremaining: 217ms\n",
      "118:\tlearn: 0.3366653\ttotal: 469ms\tremaining: 213ms\n",
      "119:\tlearn: 0.3363003\ttotal: 472ms\tremaining: 208ms\n",
      "120:\tlearn: 0.3359741\ttotal: 474ms\tremaining: 204ms\n",
      "121:\tlearn: 0.3357195\ttotal: 477ms\tremaining: 199ms\n",
      "122:\tlearn: 0.3351591\ttotal: 480ms\tremaining: 195ms\n",
      "123:\tlearn: 0.3349393\ttotal: 482ms\tremaining: 191ms\n",
      "124:\tlearn: 0.3346872\ttotal: 485ms\tremaining: 186ms\n",
      "125:\tlearn: 0.3341105\ttotal: 488ms\tremaining: 182ms\n",
      "126:\tlearn: 0.3339215\ttotal: 490ms\tremaining: 178ms\n",
      "127:\tlearn: 0.3335878\ttotal: 493ms\tremaining: 173ms\n",
      "128:\tlearn: 0.3331104\ttotal: 496ms\tremaining: 169ms\n",
      "129:\tlearn: 0.3328913\ttotal: 498ms\tremaining: 165ms\n",
      "130:\tlearn: 0.3326485\ttotal: 501ms\tremaining: 161ms\n",
      "131:\tlearn: 0.3323717\ttotal: 504ms\tremaining: 157ms\n",
      "132:\tlearn: 0.3322008\ttotal: 507ms\tremaining: 152ms\n",
      "133:\tlearn: 0.3319196\ttotal: 509ms\tremaining: 148ms\n",
      "134:\tlearn: 0.3316035\ttotal: 512ms\tremaining: 144ms\n",
      "135:\tlearn: 0.3314045\ttotal: 515ms\tremaining: 140ms\n",
      "136:\tlearn: 0.3310830\ttotal: 517ms\tremaining: 136ms\n",
      "137:\tlearn: 0.3304739\ttotal: 520ms\tremaining: 132ms\n",
      "138:\tlearn: 0.3302859\ttotal: 523ms\tremaining: 128ms\n",
      "139:\tlearn: 0.3299513\ttotal: 525ms\tremaining: 124ms\n",
      "140:\tlearn: 0.3295284\ttotal: 528ms\tremaining: 120ms\n",
      "141:\tlearn: 0.3291803\ttotal: 531ms\tremaining: 116ms\n",
      "142:\tlearn: 0.3288957\ttotal: 535ms\tremaining: 112ms\n",
      "143:\tlearn: 0.3286666\ttotal: 537ms\tremaining: 108ms\n",
      "144:\tlearn: 0.3285082\ttotal: 540ms\tremaining: 104ms\n",
      "145:\tlearn: 0.3282982\ttotal: 543ms\tremaining: 100ms\n",
      "146:\tlearn: 0.3280616\ttotal: 545ms\tremaining: 96.4ms\n",
      "147:\tlearn: 0.3278967\ttotal: 548ms\tremaining: 92.5ms\n",
      "148:\tlearn: 0.3277059\ttotal: 551ms\tremaining: 88.7ms\n",
      "149:\tlearn: 0.3275172\ttotal: 553ms\tremaining: 84.8ms\n",
      "150:\tlearn: 0.3271607\ttotal: 556ms\tremaining: 81ms\n",
      "151:\tlearn: 0.3270975\ttotal: 559ms\tremaining: 77.2ms\n",
      "152:\tlearn: 0.3268695\ttotal: 561ms\tremaining: 73.4ms\n",
      "153:\tlearn: 0.3267177\ttotal: 564ms\tremaining: 69.6ms\n",
      "154:\tlearn: 0.3265187\ttotal: 567ms\tremaining: 65.8ms\n",
      "155:\tlearn: 0.3263198\ttotal: 569ms\tremaining: 62.1ms\n",
      "156:\tlearn: 0.3262068\ttotal: 572ms\tremaining: 58.3ms\n",
      "157:\tlearn: 0.3258530\ttotal: 575ms\tremaining: 54.6ms\n",
      "158:\tlearn: 0.3255932\ttotal: 579ms\tremaining: 51ms\n",
      "159:\tlearn: 0.3252181\ttotal: 583ms\tremaining: 47.4ms\n",
      "160:\tlearn: 0.3249940\ttotal: 589ms\tremaining: 43.9ms\n",
      "161:\tlearn: 0.3247372\ttotal: 594ms\tremaining: 40.3ms\n",
      "162:\tlearn: 0.3246059\ttotal: 598ms\tremaining: 36.7ms\n",
      "163:\tlearn: 0.3242402\ttotal: 604ms\tremaining: 33.1ms\n",
      "164:\tlearn: 0.3241096\ttotal: 607ms\tremaining: 29.4ms\n",
      "165:\tlearn: 0.3238177\ttotal: 610ms\tremaining: 25.7ms\n",
      "166:\tlearn: 0.3235791\ttotal: 613ms\tremaining: 22ms\n",
      "167:\tlearn: 0.3232734\ttotal: 616ms\tremaining: 18.3ms\n",
      "168:\tlearn: 0.3228549\ttotal: 619ms\tremaining: 14.6ms\n",
      "169:\tlearn: 0.3227143\ttotal: 621ms\tremaining: 11ms\n",
      "170:\tlearn: 0.3224844\ttotal: 624ms\tremaining: 7.3ms\n",
      "171:\tlearn: 0.3221869\ttotal: 627ms\tremaining: 3.64ms\n",
      "172:\tlearn: 0.3220206\ttotal: 629ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "# Create and fit GridSearchCV, save \"best_model\"\n",
    "grid_cat_reg = GridSearchCV(pipeline_cat_reg,\n",
    "                            param_grid_cat_reg,\n",
    "                            cv=5,\n",
    "                            scoring=scoring,\n",
    "                            return_train_score=True,\n",
    "                            verbose=4,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "grid_cat_reg = grid_cat_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_cat_reg = grid_cat_reg.best_estimator_['cat_reg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1401,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "-0.22\n",
      "Best parameters:\n",
      "{'cat_reg__depth': 6}\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_, best_params_ and best_estimator_\n",
    "print('Best score:\\n{:.2f}'.format(grid_cat_reg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_cat_reg.best_params_))\n",
    "#print(\"Best estimator:\\n{}\".format(grid_xgb_reg.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1402,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>room_type_Private room</th>\n",
       "      <td>28.067064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>10.536815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates</th>\n",
       "      <td>6.425896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>availability_90</th>\n",
       "      <td>4.978674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calc_host_lst_count_sqrt_log</th>\n",
       "      <td>4.965670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_elevator</th>\n",
       "      <td>4.727967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms_log</th>\n",
       "      <td>3.674083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimum_nights_log</th>\n",
       "      <td>3.419661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Boutique hotel</th>\n",
       "      <td>3.371647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Shared room</th>\n",
       "      <td>3.067622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maximum_nights</th>\n",
       "      <td>2.692494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates_per_bed</th>\n",
       "      <td>2.337358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_other</th>\n",
       "      <td>1.965233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_tv</th>\n",
       "      <td>1.843464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_private_entrance</th>\n",
       "      <td>1.054086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wk_mth_discount</th>\n",
       "      <td>1.035193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_is_superhost</th>\n",
       "      <td>0.970918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10119</th>\n",
       "      <td>0.968911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_balcony</th>\n",
       "      <td>0.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10117</th>\n",
       "      <td>0.810333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instant_bookable</th>\n",
       "      <td>0.775005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type_Hotel room</th>\n",
       "      <td>0.738694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_essentials</th>\n",
       "      <td>0.716376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_super_strict</th>\n",
       "      <td>0.675913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_strict</th>\n",
       "      <td>0.631558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_smoking_allowed</th>\n",
       "      <td>0.538730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_child_friendly</th>\n",
       "      <td>0.519459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_breakfast</th>\n",
       "      <td>0.498101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am_pets_allowed</th>\n",
       "      <td>0.431015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10997</th>\n",
       "      <td>0.344243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_House</th>\n",
       "      <td>0.340723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13359</th>\n",
       "      <td>0.328963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10405</th>\n",
       "      <td>0.289884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10245</th>\n",
       "      <td>0.278008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancellation_policy_moderate</th>\n",
       "      <td>0.233898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10178</th>\n",
       "      <td>0.233727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10999</th>\n",
       "      <td>0.218410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10435</th>\n",
       "      <td>0.211992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12047</th>\n",
       "      <td>0.187463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13407</th>\n",
       "      <td>0.154474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13353</th>\n",
       "      <td>0.140717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13189</th>\n",
       "      <td>0.126197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10179</th>\n",
       "      <td>0.120847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12051</th>\n",
       "      <td>0.119306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10965</th>\n",
       "      <td>0.118093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13347</th>\n",
       "      <td>0.116328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10967</th>\n",
       "      <td>0.107761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10719</th>\n",
       "      <td>0.100954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12347</th>\n",
       "      <td>0.100599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10437</th>\n",
       "      <td>0.094665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10317</th>\n",
       "      <td>0.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10247</th>\n",
       "      <td>0.091167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10439</th>\n",
       "      <td>0.089923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13086</th>\n",
       "      <td>0.086492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10249</th>\n",
       "      <td>0.080119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10715</th>\n",
       "      <td>0.075084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10315</th>\n",
       "      <td>0.075073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10365</th>\n",
       "      <td>0.074603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10589</th>\n",
       "      <td>0.073532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10785</th>\n",
       "      <td>0.071981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13357</th>\n",
       "      <td>0.071616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13187</th>\n",
       "      <td>0.071107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10587</th>\n",
       "      <td>0.066921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13409</th>\n",
       "      <td>0.065310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12435</th>\n",
       "      <td>0.062919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12157</th>\n",
       "      <td>0.062446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10969</th>\n",
       "      <td>0.061377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Secondary unit</th>\n",
       "      <td>0.059519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10559</th>\n",
       "      <td>0.059023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14057</th>\n",
       "      <td>0.056978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12059</th>\n",
       "      <td>0.056296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10557</th>\n",
       "      <td>0.056030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13351</th>\n",
       "      <td>0.055871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12437</th>\n",
       "      <td>0.053633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12055</th>\n",
       "      <td>0.045672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12099</th>\n",
       "      <td>0.045028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10318</th>\n",
       "      <td>0.042064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_nan</th>\n",
       "      <td>0.040986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10777</th>\n",
       "      <td>0.040848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10555</th>\n",
       "      <td>0.040781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13349</th>\n",
       "      <td>0.038687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12045</th>\n",
       "      <td>0.037915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10707</th>\n",
       "      <td>0.037879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12053</th>\n",
       "      <td>0.037741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10717</th>\n",
       "      <td>0.035253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10553</th>\n",
       "      <td>0.032232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12103</th>\n",
       "      <td>0.031687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10551</th>\n",
       "      <td>0.031056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10963</th>\n",
       "      <td>0.030441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10409</th>\n",
       "      <td>0.029276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14197</th>\n",
       "      <td>0.027218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13156</th>\n",
       "      <td>0.027131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10243</th>\n",
       "      <td>0.027103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Bed and breakfast</th>\n",
       "      <td>0.027102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10407</th>\n",
       "      <td>0.025937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12101</th>\n",
       "      <td>0.025857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10827</th>\n",
       "      <td>0.025049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13088</th>\n",
       "      <td>0.022766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10961</th>\n",
       "      <td>0.022285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12163</th>\n",
       "      <td>0.021708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10367</th>\n",
       "      <td>0.019044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10627</th>\n",
       "      <td>0.018193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12049</th>\n",
       "      <td>0.017672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10585</th>\n",
       "      <td>0.017054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10625</th>\n",
       "      <td>0.014615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10781</th>\n",
       "      <td>0.010777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10829</th>\n",
       "      <td>0.010418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10629</th>\n",
       "      <td>0.009799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10713</th>\n",
       "      <td>0.009230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10787</th>\n",
       "      <td>0.008717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_14059</th>\n",
       "      <td>0.008382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_13355</th>\n",
       "      <td>0.005083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10623</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10711</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10783</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12161</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type_Unique space</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_10823</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_zip_12043</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     weight\n",
       "room_type_Private room            28.067064\n",
       "bedrooms                          10.536815\n",
       "accommodates                       6.425896\n",
       "availability_90                    4.978674\n",
       "calc_host_lst_count_sqrt_log       4.965670\n",
       "am_elevator                        4.727967\n",
       "bathrooms_log                      3.674083\n",
       "minimum_nights_log                 3.419661\n",
       "property_type_Boutique hotel       3.371647\n",
       "room_type_Shared room              3.067622\n",
       "maximum_nights                     2.692494\n",
       "accommodates_per_bed               2.337358\n",
       "zipcode_zip_other                  1.965233\n",
       "am_tv                              1.843464\n",
       "am_private_entrance                1.054086\n",
       "wk_mth_discount                    1.035193\n",
       "host_is_superhost                  0.970918\n",
       "zipcode_zip_10119                  0.968911\n",
       "am_balcony                         0.927600\n",
       "zipcode_zip_10117                  0.810333\n",
       "instant_bookable                   0.775005\n",
       "room_type_Hotel room               0.738694\n",
       "am_essentials                      0.716376\n",
       "cancellation_policy_super_strict   0.675913\n",
       "cancellation_policy_strict         0.631558\n",
       "am_smoking_allowed                 0.538730\n",
       "am_child_friendly                  0.519459\n",
       "am_breakfast                       0.498101\n",
       "am_pets_allowed                    0.431015\n",
       "zipcode_zip_10997                  0.344243\n",
       "property_type_House                0.340723\n",
       "zipcode_zip_13359                  0.328963\n",
       "zipcode_zip_10405                  0.289884\n",
       "zipcode_zip_10245                  0.278008\n",
       "cancellation_policy_moderate       0.233898\n",
       "zipcode_zip_10178                  0.233727\n",
       "zipcode_zip_10999                  0.218410\n",
       "zipcode_zip_10435                  0.211992\n",
       "zipcode_zip_12047                  0.187463\n",
       "zipcode_zip_13407                  0.154474\n",
       "zipcode_zip_13353                  0.140717\n",
       "zipcode_zip_13189                  0.126197\n",
       "zipcode_zip_10179                  0.120847\n",
       "zipcode_zip_12051                  0.119306\n",
       "zipcode_zip_10965                  0.118093\n",
       "zipcode_zip_13347                  0.116328\n",
       "zipcode_zip_10967                  0.107761\n",
       "zipcode_zip_10719                  0.100954\n",
       "zipcode_zip_12347                  0.100599\n",
       "zipcode_zip_10437                  0.094665\n",
       "zipcode_zip_10317                  0.091500\n",
       "zipcode_zip_10247                  0.091167\n",
       "zipcode_zip_10439                  0.089923\n",
       "zipcode_zip_13086                  0.086492\n",
       "zipcode_zip_10249                  0.080119\n",
       "zipcode_zip_10715                  0.075084\n",
       "zipcode_zip_10315                  0.075073\n",
       "zipcode_zip_10365                  0.074603\n",
       "zipcode_zip_10589                  0.073532\n",
       "zipcode_zip_10785                  0.071981\n",
       "zipcode_zip_13357                  0.071616\n",
       "zipcode_zip_13187                  0.071107\n",
       "zipcode_zip_10587                  0.066921\n",
       "zipcode_zip_13409                  0.065310\n",
       "zipcode_zip_12435                  0.062919\n",
       "zipcode_zip_12157                  0.062446\n",
       "zipcode_zip_10969                  0.061377\n",
       "property_type_Secondary unit       0.059519\n",
       "zipcode_zip_10559                  0.059023\n",
       "zipcode_zip_14057                  0.056978\n",
       "zipcode_zip_12059                  0.056296\n",
       "zipcode_zip_10557                  0.056030\n",
       "zipcode_zip_13351                  0.055871\n",
       "zipcode_zip_12437                  0.053633\n",
       "zipcode_zip_12055                  0.045672\n",
       "zipcode_zip_12099                  0.045028\n",
       "zipcode_zip_10318                  0.042064\n",
       "zipcode_zip_nan                    0.040986\n",
       "zipcode_zip_10777                  0.040848\n",
       "zipcode_zip_10555                  0.040781\n",
       "zipcode_zip_13349                  0.038687\n",
       "zipcode_zip_12045                  0.037915\n",
       "zipcode_zip_10707                  0.037879\n",
       "zipcode_zip_12053                  0.037741\n",
       "zipcode_zip_10717                  0.035253\n",
       "zipcode_zip_10553                  0.032232\n",
       "zipcode_zip_12103                  0.031687\n",
       "zipcode_zip_10551                  0.031056\n",
       "zipcode_zip_10963                  0.030441\n",
       "zipcode_zip_10409                  0.029276\n",
       "zipcode_zip_14197                  0.027218\n",
       "zipcode_zip_13156                  0.027131\n",
       "zipcode_zip_10243                  0.027103\n",
       "property_type_Bed and breakfast    0.027102\n",
       "zipcode_zip_10407                  0.025937\n",
       "zipcode_zip_12101                  0.025857\n",
       "zipcode_zip_10827                  0.025049\n",
       "zipcode_zip_13088                  0.022766\n",
       "zipcode_zip_10961                  0.022285\n",
       "zipcode_zip_12163                  0.021708\n",
       "zipcode_zip_10367                  0.019044\n",
       "zipcode_zip_10627                  0.018193\n",
       "zipcode_zip_12049                  0.017672\n",
       "zipcode_zip_10585                  0.017054\n",
       "zipcode_zip_10625                  0.014615\n",
       "zipcode_zip_10781                  0.010777\n",
       "zipcode_zip_10829                  0.010418\n",
       "zipcode_zip_10629                  0.009799\n",
       "zipcode_zip_10713                  0.009230\n",
       "zipcode_zip_10787                  0.008717\n",
       "zipcode_zip_14059                  0.008382\n",
       "zipcode_zip_13355                  0.005083\n",
       "zipcode_zip_10623                  0.000000\n",
       "zipcode_zip_10711                  0.000000\n",
       "zipcode_zip_10783                  0.000000\n",
       "zipcode_zip_12161                  0.000000\n",
       "property_type_Unique space         0.000000\n",
       "zipcode_zip_10823                  0.000000\n",
       "zipcode_zip_12043                  0.000000"
      ]
     },
     "execution_count": 1402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display feature importances\n",
    "fi_cat_reg = get_feat_importances(best_model_cat_reg)\n",
    "fi_cat_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1403,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load existing model\n",
    "#load_best_model = load_model(title=\"best_model_xgb_reg\", dataset_loc=dataset_loc, dataset_date=dataset_date)\n",
    "#load_best_cv = load_model(title=\"best_cv_xgb_reg\", dataset_loc=dataset_loc, dataset_date=dataset_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Curves (Overfitting)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1404,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_train_pred_cat_reg = best_model_cat_reg.predict(X_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1405,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.10\n",
      "RMSE: 0.32\n",
      "MAE: 0.25\n",
      "R2: 0.69\n",
      "MAPE: 6.32\n",
      "MAPE median: 4.97\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_train, y_train_pred_cat_reg, model=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1406,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_test_pred_cat_reg = best_model_cat_reg.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.14\n",
      "RMSE: 0.37\n",
      "MAE: 0.28\n",
      "R2: 0.58\n",
      "MAPE: 7.22\n",
      "MAPE median: 5.37\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_test, y_test_pred_cat_reg, model=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1408,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35436153, 0.38434794])"
      ]
     },
     "execution_count": 1408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display confidence interval (scipy stats)\n",
    "confidence = 0.95\n",
    "squared_errors = (y_test_pred_cat_reg - y_test)**2\n",
    "np.sqrt(\n",
    "    stats.t.interval(confidence,\n",
    "                     len(squared_errors) - 1,\n",
    "                     loc=squared_errors.mean(),\n",
    "                     scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Median Price Intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1409,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save MAPE_median as variable\n",
    "MAPE_median_cat_reg = (median_absolute_percentage_error(y_test, y_test_pred_cat_reg))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49.95, 77.88),\n",
       " (65.1, 104.61),\n",
       " (50.01, 77.99),\n",
       " (33.39, 49.74),\n",
       " (38.42, 58.14),\n",
       " (26.71, 38.78),\n",
       " (48.31, 75.04),\n",
       " (36.5, 54.93),\n",
       " (52.5, 82.33),\n",
       " (43.32, 66.47),\n",
       " (25.34, 36.59),\n",
       " (29.22, 42.87),\n",
       " (60.0, 95.52),\n",
       " (24.12, 34.63),\n",
       " (37.14, 56.0),\n",
       " (81.05, 133.53),\n",
       " (73.21, 119.22),\n",
       " (60.97, 97.25),\n",
       " (52.19, 81.79),\n",
       " (50.01, 77.99),\n",
       " (104.87, 177.9),\n",
       " (47.01, 72.81),\n",
       " (41.61, 63.55),\n",
       " (85.46, 141.63),\n",
       " (24.46, 35.18),\n",
       " (56.82, 89.91),\n",
       " (78.53, 128.91),\n",
       " (32.36, 48.03),\n",
       " (24.59, 35.38),\n",
       " (46.2, 71.4),\n",
       " (26.73, 38.82),\n",
       " (67.16, 108.3),\n",
       " (55.56, 87.69),\n",
       " (29.3, 43.0),\n",
       " (40.77, 62.12),\n",
       " (53.14, 83.45),\n",
       " (36.3, 54.59),\n",
       " (53.03, 83.26),\n",
       " (30.91, 45.64),\n",
       " (29.46, 43.27),\n",
       " (31.3, 46.28),\n",
       " (25.93, 37.54),\n",
       " (40.23, 61.21),\n",
       " (30.05, 44.22),\n",
       " (30.17, 44.42),\n",
       " (41.98, 64.19),\n",
       " (45.59, 70.35),\n",
       " (73.79, 120.28),\n",
       " (65.13, 104.67),\n",
       " (100.08, 168.88),\n",
       " (37.05, 55.84),\n",
       " (78.74, 129.29),\n",
       " (52.04, 81.52),\n",
       " (48.8, 75.89),\n",
       " (36.4, 54.75),\n",
       " (47.73, 74.05),\n",
       " (67.2, 108.37),\n",
       " (34.32, 51.29),\n",
       " (64.69, 103.88),\n",
       " (33.1, 49.26),\n",
       " (59.64, 94.89),\n",
       " (56.18, 88.78),\n",
       " (55.62, 87.79),\n",
       " (44.32, 68.18),\n",
       " (49.57, 77.22),\n",
       " (48.29, 75.01),\n",
       " (24.94, 35.94),\n",
       " (30.13, 44.37),\n",
       " (47.59, 73.8),\n",
       " (40.86, 62.27),\n",
       " (48.05, 74.6),\n",
       " (89.29, 148.72),\n",
       " (37.82, 57.15),\n",
       " (60.51, 96.43),\n",
       " (185.25, 335.23),\n",
       " (47.94, 74.41),\n",
       " (17.72, 24.57),\n",
       " (56.45, 89.26),\n",
       " (34.98, 52.37),\n",
       " (30.0, 44.14),\n",
       " (23.88, 34.24),\n",
       " (22.62, 32.24),\n",
       " (33.59, 50.07),\n",
       " (22.16, 31.5),\n",
       " (58.88, 93.53),\n",
       " (59.18, 94.08),\n",
       " (41.79, 63.86),\n",
       " (76.25, 124.75),\n",
       " (38.71, 58.64),\n",
       " (62.78, 100.47),\n",
       " (62.45, 99.89),\n",
       " (48.03, 74.56),\n",
       " (40.39, 61.47),\n",
       " (142.52, 250.34),\n",
       " (55.73, 87.99),\n",
       " (36.27, 54.53),\n",
       " (126.96, 220.09),\n",
       " (61.14, 97.55),\n",
       " (28.11, 41.05),\n",
       " (23.54, 33.69),\n",
       " (32.05, 47.52),\n",
       " (45.3, 69.85),\n",
       " (28.44, 41.6),\n",
       " (44.0, 67.62),\n",
       " (29.67, 43.61),\n",
       " (59.44, 94.54),\n",
       " (27.18, 39.55),\n",
       " (41.9, 64.05),\n",
       " (38.4, 58.12),\n",
       " (50.94, 79.61),\n",
       " (33.86, 50.52),\n",
       " (23.58, 33.76),\n",
       " (74.49, 121.55),\n",
       " (27.0, 39.25),\n",
       " (84.48, 139.83),\n",
       " (61.84, 98.8),\n",
       " (42.17, 64.5),\n",
       " (85.43, 141.58),\n",
       " (45.55, 70.29),\n",
       " (33.56, 50.02),\n",
       " (24.49, 35.21),\n",
       " (51.88, 81.25),\n",
       " (32.46, 48.2),\n",
       " (43.2, 66.25),\n",
       " (32.34, 48.0),\n",
       " (70.42, 114.18),\n",
       " (35.76, 53.68),\n",
       " (56.24, 88.89),\n",
       " (34.13, 50.96),\n",
       " (27.95, 40.79),\n",
       " (95.96, 161.15),\n",
       " (21.81, 30.96),\n",
       " (31.69, 46.92),\n",
       " (22.53, 32.09),\n",
       " (37.99, 57.43),\n",
       " (32.97, 49.04),\n",
       " (30.0, 44.14),\n",
       " (43.8, 67.29),\n",
       " (110.73, 189.0),\n",
       " (122.25, 211.02),\n",
       " (32.04, 47.5),\n",
       " (41.23, 62.9),\n",
       " (45.65, 70.46),\n",
       " (27.73, 40.45),\n",
       " (48.05, 74.59),\n",
       " (76.46, 125.14),\n",
       " (47.71, 74.0),\n",
       " (34.81, 52.1),\n",
       " (57.72, 91.49),\n",
       " (73.36, 119.5),\n",
       " (89.42, 148.97),\n",
       " (52.89, 83.01),\n",
       " (64.47, 103.48),\n",
       " (24.47, 35.19),\n",
       " (84.76, 140.34),\n",
       " (100.08, 168.88),\n",
       " (22.26, 31.66),\n",
       " (48.76, 75.82),\n",
       " (31.07, 45.91),\n",
       " (88.79, 147.8),\n",
       " (25.94, 37.55),\n",
       " (31.79, 47.09),\n",
       " (53.18, 83.52),\n",
       " (50.98, 79.67),\n",
       " (42.27, 64.68),\n",
       " (98.85, 166.57),\n",
       " (42.84, 65.65),\n",
       " (28.45, 41.61),\n",
       " (24.79, 35.7),\n",
       " (28.43, 41.59),\n",
       " (58.77, 93.34),\n",
       " (136.25, 238.1),\n",
       " (40.84, 62.25),\n",
       " (42.61, 65.25),\n",
       " (29.89, 43.96),\n",
       " (71.83, 116.73),\n",
       " (29.06, 42.6),\n",
       " (51.42, 80.44),\n",
       " (29.54, 43.39),\n",
       " (36.29, 54.58),\n",
       " (52.47, 82.28),\n",
       " (64.9, 104.26),\n",
       " (55.12, 86.92),\n",
       " (49.66, 77.38),\n",
       " (29.94, 44.05),\n",
       " (62.23, 99.48),\n",
       " (43.67, 67.05),\n",
       " (83.44, 137.91),\n",
       " (53.17, 83.5),\n",
       " (50.07, 78.1),\n",
       " (64.53, 103.6),\n",
       " (25.74, 37.23),\n",
       " (82.5, 136.18),\n",
       " (71.2, 115.59),\n",
       " (54.49, 85.82),\n",
       " (63.36, 101.5),\n",
       " (93.12, 155.84),\n",
       " (31.75, 47.02),\n",
       " (61.44, 98.08),\n",
       " (24.96, 35.98),\n",
       " (49.47, 77.05),\n",
       " (34.73, 51.96),\n",
       " (99.4, 167.6),\n",
       " (45.26, 69.79),\n",
       " (51.97, 81.41),\n",
       " (44.24, 68.03),\n",
       " (61.25, 97.74),\n",
       " (96.78, 162.69),\n",
       " (52.08, 81.59),\n",
       " (33.2, 49.42),\n",
       " (142.1, 249.52),\n",
       " (28.91, 42.36),\n",
       " (33.23, 49.47),\n",
       " (93.48, 156.51),\n",
       " (27.2, 39.58),\n",
       " (48.75, 75.81),\n",
       " (27.4, 39.91),\n",
       " (47.81, 74.19),\n",
       " (45.25, 69.78),\n",
       " (66.1, 106.39),\n",
       " (106.37, 180.73),\n",
       " (22.44, 31.95),\n",
       " (56.85, 89.96),\n",
       " (26.03, 37.7),\n",
       " (80.76, 132.99),\n",
       " (31.94, 47.33),\n",
       " (16.46, 22.62),\n",
       " (27.24, 39.65),\n",
       " (65.1, 104.6),\n",
       " (74.46, 121.49),\n",
       " (56.7, 89.69),\n",
       " (73.3, 119.39),\n",
       " (47.93, 74.39),\n",
       " (36.74, 55.33),\n",
       " (39.47, 59.93),\n",
       " (44.25, 68.06),\n",
       " (54.02, 84.98),\n",
       " (73.65, 120.02),\n",
       " (48.89, 76.06),\n",
       " (42.43, 64.94),\n",
       " (59.23, 94.17),\n",
       " (61.04, 97.38),\n",
       " (57.49, 91.09),\n",
       " (106.53, 181.04),\n",
       " (34.48, 51.55),\n",
       " (30.73, 45.35),\n",
       " (30.84, 45.52),\n",
       " (47.91, 74.34),\n",
       " (49.41, 76.94),\n",
       " (45.07, 69.47),\n",
       " (36.7, 55.26),\n",
       " (59.01, 93.76),\n",
       " (53.86, 84.7),\n",
       " (75.07, 122.6),\n",
       " (51.29, 80.21),\n",
       " (48.65, 75.64),\n",
       " (59.25, 94.2),\n",
       " (42.17, 64.51),\n",
       " (30.39, 44.78),\n",
       " (31.15, 46.04),\n",
       " (28.85, 42.27),\n",
       " (33.02, 49.12),\n",
       " (46.42, 71.79),\n",
       " (31.77, 47.05),\n",
       " (42.13, 64.43),\n",
       " (18.93, 26.44),\n",
       " (52.85, 82.95),\n",
       " (32.87, 48.87),\n",
       " (71.73, 116.55),\n",
       " (39.73, 60.37),\n",
       " (24.09, 34.57),\n",
       " (47.39, 73.46),\n",
       " (47.92, 74.37),\n",
       " (64.61, 103.73),\n",
       " (104.73, 177.63),\n",
       " (78.88, 129.54),\n",
       " (86.66, 143.86),\n",
       " (40.15, 61.08),\n",
       " (52.92, 83.06),\n",
       " (50.51, 78.86),\n",
       " (23.53, 33.69),\n",
       " (60.27, 96.0),\n",
       " (38.71, 58.63),\n",
       " (112.9, 193.14),\n",
       " (60.54, 96.49),\n",
       " (43.68, 67.08),\n",
       " (30.87, 45.58),\n",
       " (26.95, 39.17),\n",
       " (54.93, 86.58),\n",
       " (56.82, 89.9),\n",
       " (31.6, 46.77),\n",
       " (64.29, 103.16),\n",
       " (99.91, 168.55),\n",
       " (43.55, 66.86),\n",
       " (95.73, 160.72),\n",
       " (24.76, 35.65),\n",
       " (32.42, 48.13),\n",
       " (49.32, 76.79),\n",
       " (25.39, 36.67),\n",
       " (53.26, 83.66),\n",
       " (28.47, 41.65),\n",
       " (54.31, 85.5),\n",
       " (31.82, 47.14),\n",
       " (39.47, 59.92),\n",
       " (57.12, 90.43),\n",
       " (30.36, 44.74),\n",
       " (64.13, 102.88),\n",
       " (56.72, 89.73),\n",
       " (56.02, 88.5),\n",
       " (66.34, 106.83),\n",
       " (82.9, 136.93),\n",
       " (43.9, 67.45),\n",
       " (85.68, 142.04),\n",
       " (66.86, 107.77),\n",
       " (94.1, 157.67),\n",
       " (32.14, 47.67),\n",
       " (24.48, 35.2),\n",
       " (27.0, 39.26),\n",
       " (49.1, 76.41),\n",
       " (67.5, 108.91),\n",
       " (41.1, 62.68),\n",
       " (65.55, 105.41),\n",
       " (64.83, 104.13),\n",
       " (29.66, 43.59),\n",
       " (120.88, 208.39),\n",
       " (38.18, 57.75),\n",
       " (48.36, 75.14),\n",
       " (58.41, 92.7),\n",
       " (24.11, 34.61),\n",
       " (51.3, 80.24),\n",
       " (72.02, 117.07),\n",
       " (44.88, 69.13),\n",
       " (25.62, 37.03),\n",
       " (65.44, 105.22),\n",
       " (67.04, 108.09),\n",
       " (30.93, 45.68),\n",
       " (28.47, 41.65),\n",
       " (46.84, 72.5),\n",
       " (30.69, 45.28),\n",
       " (49.1, 76.41),\n",
       " (39.79, 60.47),\n",
       " (135.27, 236.19),\n",
       " (27.91, 40.74),\n",
       " (25.58, 36.97),\n",
       " (52.12, 81.66),\n",
       " (30.91, 45.65),\n",
       " (27.36, 39.85),\n",
       " (50.48, 78.81),\n",
       " (53.37, 83.85),\n",
       " (87.11, 144.68),\n",
       " (73.16, 119.13),\n",
       " (50.02, 78.01),\n",
       " (24.04, 34.5),\n",
       " (88.78, 147.78),\n",
       " (21.87, 31.05),\n",
       " (25.78, 37.3),\n",
       " (48.1, 74.68),\n",
       " (54.45, 85.73),\n",
       " (81.8, 134.9),\n",
       " (30.51, 44.98),\n",
       " (30.66, 45.23),\n",
       " (33.99, 50.74),\n",
       " (32.95, 49.01),\n",
       " (32.46, 48.19),\n",
       " (54.0, 84.95),\n",
       " (39.8, 60.47),\n",
       " (56.38, 89.14),\n",
       " (34.74, 51.99),\n",
       " (58.63, 93.11),\n",
       " (56.08, 88.6),\n",
       " (33.9, 50.59),\n",
       " (24.49, 35.22),\n",
       " (44.69, 68.8),\n",
       " (73.12, 119.05),\n",
       " (23.36, 33.41),\n",
       " (38.97, 59.07),\n",
       " (66.82, 107.69),\n",
       " (30.19, 44.46),\n",
       " (40.91, 62.37),\n",
       " (27.93, 40.77),\n",
       " (59.96, 95.46),\n",
       " (70.67, 114.63),\n",
       " (112.74, 192.83),\n",
       " (34.0, 50.75),\n",
       " (30.51, 44.98),\n",
       " (49.1, 76.4),\n",
       " (63.02, 100.89),\n",
       " (29.49, 43.31),\n",
       " (26.71, 38.79),\n",
       " (60.51, 96.42),\n",
       " (92.5, 154.7),\n",
       " (57.09, 90.39),\n",
       " (31.07, 45.9),\n",
       " (31.62, 46.81),\n",
       " (32.01, 47.46),\n",
       " (90.08, 150.18),\n",
       " (47.5, 73.65),\n",
       " (66.07, 106.35),\n",
       " (56.65, 89.6),\n",
       " (25.81, 37.33),\n",
       " (29.56, 43.43),\n",
       " (69.41, 112.36),\n",
       " (22.41, 31.91),\n",
       " (49.88, 77.77),\n",
       " (39.61, 60.16),\n",
       " (29.43, 43.21),\n",
       " (49.55, 77.2),\n",
       " (37.25, 56.17),\n",
       " (28.36, 41.47),\n",
       " (33.73, 50.3),\n",
       " (34.33, 51.29),\n",
       " (66.97, 107.97),\n",
       " (78.23, 128.35),\n",
       " (69.39, 112.32),\n",
       " (26.92, 39.12),\n",
       " (52.16, 81.73),\n",
       " (21.67, 30.73),\n",
       " (56.58, 89.48),\n",
       " (46.24, 71.47),\n",
       " (101.31, 171.18),\n",
       " (33.79, 50.41),\n",
       " (21.09, 29.82),\n",
       " (29.36, 43.09),\n",
       " (56.12, 88.67),\n",
       " (39.13, 59.35),\n",
       " (45.12, 69.54),\n",
       " (39.87, 60.61),\n",
       " (68.95, 111.52),\n",
       " (32.67, 48.54),\n",
       " (48.98, 76.21),\n",
       " (48.37, 75.15),\n",
       " (54.25, 85.39),\n",
       " (46.42, 71.78),\n",
       " (32.7, 48.59),\n",
       " (52.94, 83.1),\n",
       " (28.15, 41.13),\n",
       " (25.68, 37.12),\n",
       " (41.27, 62.98),\n",
       " (25.77, 37.28),\n",
       " (92.2, 154.13),\n",
       " (57.54, 91.18),\n",
       " (47.75, 74.08),\n",
       " (27.72, 40.43),\n",
       " (66.56, 107.23),\n",
       " (44.1, 67.8),\n",
       " (18.86, 26.33),\n",
       " (86.87, 144.24),\n",
       " (25.62, 37.03),\n",
       " (26.43, 38.34),\n",
       " (45.38, 69.99),\n",
       " (60.04, 95.6),\n",
       " (45.17, 69.63),\n",
       " (47.54, 73.71),\n",
       " (33.27, 49.54),\n",
       " (32.71, 48.61),\n",
       " (113.68, 194.62),\n",
       " (72.05, 117.12),\n",
       " (61.55, 98.28),\n",
       " (44.82, 69.04),\n",
       " (51.67, 80.89),\n",
       " (29.97, 44.1),\n",
       " (33.15, 49.33),\n",
       " (49.97, 77.92),\n",
       " (73.72, 120.14),\n",
       " (63.65, 102.01),\n",
       " (21.12, 29.87),\n",
       " (34.24, 51.15),\n",
       " (81.86, 135.02),\n",
       " (26.07, 37.75),\n",
       " (21.05, 29.75),\n",
       " (32.1, 47.6),\n",
       " (52.7, 82.68),\n",
       " (56.44, 89.23),\n",
       " (69.5, 112.51),\n",
       " (43.03, 65.98),\n",
       " (28.77, 42.14),\n",
       " (106.5, 180.98),\n",
       " (26.01, 37.67),\n",
       " (35.84, 53.82),\n",
       " (21.58, 30.6),\n",
       " (38.64, 58.52),\n",
       " (55.54, 87.65),\n",
       " (71.11, 115.42),\n",
       " (37.44, 56.5),\n",
       " (29.13, 42.72),\n",
       " (50.51, 78.86),\n",
       " (25.93, 37.53),\n",
       " (33.66, 50.19),\n",
       " (135.24, 236.13),\n",
       " (51.31, 80.26),\n",
       " (32.65, 48.51),\n",
       " (27.93, 40.77),\n",
       " (28.49, 41.67),\n",
       " (95.62, 160.51),\n",
       " (33.73, 50.3),\n",
       " (28.66, 41.95),\n",
       " (45.58, 70.35),\n",
       " (38.46, 58.22),\n",
       " (53.41, 83.92),\n",
       " (70.0, 113.41),\n",
       " (59.5, 94.63),\n",
       " (90.95, 151.8),\n",
       " (41.71, 63.73),\n",
       " (28.38, 41.5),\n",
       " (23.33, 33.37),\n",
       " (29.43, 43.22),\n",
       " (67.18, 108.34),\n",
       " (54.14, 85.2),\n",
       " (47.49, 73.62),\n",
       " (73.24, 119.28),\n",
       " (27.65, 40.32),\n",
       " (58.78, 93.37),\n",
       " (33.58, 50.05),\n",
       " (55.1, 86.89),\n",
       " (29.96, 44.09),\n",
       " (35.4, 53.09),\n",
       " (82.86, 136.85),\n",
       " (22.27, 31.68),\n",
       " (27.94, 40.78),\n",
       " (31.28, 46.25),\n",
       " (75.03, 122.52),\n",
       " (29.6, 43.49),\n",
       " (56.19, 88.79),\n",
       " (53.31, 83.74),\n",
       " (28.92, 42.38),\n",
       " (61.81, 98.74),\n",
       " (25.02, 36.07),\n",
       " (62.49, 99.95),\n",
       " (62.47, 99.92),\n",
       " (68.93, 111.49),\n",
       " (22.98, 32.8),\n",
       " (48.12, 74.71),\n",
       " (32.49, 48.24),\n",
       " (99.62, 168.01),\n",
       " (25.34, 36.57),\n",
       " (29.52, 43.36),\n",
       " (52.6, 82.5),\n",
       " (39.22, 59.5),\n",
       " (30.48, 44.94),\n",
       " (33.97, 50.7),\n",
       " (82.81, 136.76),\n",
       " (49.39, 76.91),\n",
       " (100.08, 168.88),\n",
       " (61.5, 98.19),\n",
       " (22.28, 31.7),\n",
       " (39.11, 59.32),\n",
       " (49.79, 77.6),\n",
       " (41.06, 62.61),\n",
       " (40.25, 61.24),\n",
       " (43.42, 66.63),\n",
       " (27.74, 40.46),\n",
       " (29.23, 42.88),\n",
       " (40.39, 61.48),\n",
       " (40.63, 61.88),\n",
       " (26.71, 38.79),\n",
       " (152.38, 269.7),\n",
       " (41.02, 62.55),\n",
       " (50.58, 78.98),\n",
       " (47.83, 74.22),\n",
       " (25.78, 37.29),\n",
       " (50.51, 78.87),\n",
       " (36.28, 54.55),\n",
       " (22.83, 32.57),\n",
       " (78.44, 128.75),\n",
       " (77.06, 126.22),\n",
       " (39.12, 59.32),\n",
       " (51.94, 81.35),\n",
       " (47.76, 74.1),\n",
       " (51.05, 79.79),\n",
       " (33.51, 49.93),\n",
       " (53.13, 83.43),\n",
       " (51.41, 80.43),\n",
       " (34.96, 52.35),\n",
       " (39.57, 60.09),\n",
       " (51.46, 80.51),\n",
       " (55.88, 88.26),\n",
       " (23.99, 34.42),\n",
       " (20.46, 28.83),\n",
       " (38.46, 58.22),\n",
       " (30.79, 45.44),\n",
       " (64.99, 104.42),\n",
       " (46.08, 71.2),\n",
       " (57.81, 91.64),\n",
       " (44.09, 67.78),\n",
       " (63.9, 102.47),\n",
       " (47.03, 72.83),\n",
       " (87.32, 145.07),\n",
       " (48.73, 75.77),\n",
       " (51.12, 79.93),\n",
       " (52.13, 81.69),\n",
       " (34.9, 52.25),\n",
       " (51.98, 81.42),\n",
       " (37.27, 56.21),\n",
       " (27.64, 40.29),\n",
       " (55.59, 87.74),\n",
       " (66.71, 107.5),\n",
       " (31.83, 47.16),\n",
       " (50.91, 79.55),\n",
       " (27.04, 39.32),\n",
       " (29.16, 42.77),\n",
       " (54.94, 86.6),\n",
       " (37.39, 56.42),\n",
       " (34.42, 51.45),\n",
       " (48.86, 76.0),\n",
       " (32.22, 47.81),\n",
       " (69.5, 112.52),\n",
       " (45.99, 71.04),\n",
       " (26.15, 37.88),\n",
       " (48.1, 74.68),\n",
       " (58.52, 92.9),\n",
       " (39.28, 59.59),\n",
       " (53.53, 84.13),\n",
       " (55.22, 87.08),\n",
       " (63.3, 101.39),\n",
       " (47.34, 73.36),\n",
       " (30.35, 44.72),\n",
       " (19.98, 28.08),\n",
       " (58.82, 93.44),\n",
       " (56.07, 88.59),\n",
       " (18.68, 26.04),\n",
       " (71.53, 116.18),\n",
       " (37.91, 57.28),\n",
       " (65.44, 105.22),\n",
       " (31.33, 46.33),\n",
       " (39.21, 59.48),\n",
       " (46.31, 71.58),\n",
       " (41.79, 63.86),\n",
       " (28.43, 41.59),\n",
       " (25.65, 37.08),\n",
       " (32.42, 48.12),\n",
       " (62.65, 100.24),\n",
       " (37.19, 56.08),\n",
       " (37.62, 56.79),\n",
       " (28.0, 40.89),\n",
       " (41.92, 64.08),\n",
       " (75.21, 122.85),\n",
       " (94.15, 157.77),\n",
       " (48.91, 76.08),\n",
       " (36.9, 55.6),\n",
       " (30.03, 44.19),\n",
       " (32.04, 47.49),\n",
       " (49.17, 76.53),\n",
       " (49.3, 76.76),\n",
       " (37.24, 56.17),\n",
       " (27.57, 40.18),\n",
       " (48.36, 75.14),\n",
       " (43.07, 66.03),\n",
       " (63.73, 102.16),\n",
       " (37.97, 57.4),\n",
       " (32.61, 48.45),\n",
       " (29.9, 43.98),\n",
       " (50.2, 78.32),\n",
       " (35.14, 52.65),\n",
       " (45.6, 70.37),\n",
       " (61.97, 99.03),\n",
       " (89.92, 149.88),\n",
       " (25.52, 36.87),\n",
       " (22.59, 32.19),\n",
       " (33.25, 49.5),\n",
       " (53.34, 83.79),\n",
       " (22.47, 31.99),\n",
       " (52.85, 82.93),\n",
       " (27.56, 40.17),\n",
       " (42.12, 64.42),\n",
       " (33.1, 49.26),\n",
       " (77.59, 127.2),\n",
       " (30.79, 45.44),\n",
       " (51.16, 79.99),\n",
       " (28.27, 41.32),\n",
       " (30.29, 44.62),\n",
       " (50.77, 79.32),\n",
       " (56.94, 90.12),\n",
       " (104.57, 177.33),\n",
       " (149.01, 263.07),\n",
       " (30.52, 44.99),\n",
       " (48.94, 76.13),\n",
       " (33.16, 49.35),\n",
       " (32.22, 47.81),\n",
       " (82.44, 136.08),\n",
       " (26.93, 39.14),\n",
       " (59.66, 94.92),\n",
       " (61.84, 98.79),\n",
       " (33.05, 49.17),\n",
       " (23.68, 33.93),\n",
       " (31.04, 45.85),\n",
       " (32.21, 47.78),\n",
       " (21.92, 31.13),\n",
       " (56.91, 90.06),\n",
       " (35.0, 52.41),\n",
       " (35.82, 53.78),\n",
       " (33.43, 49.81),\n",
       " (30.66, 45.22),\n",
       " (40.24, 61.22),\n",
       " (36.32, 54.63),\n",
       " (29.73, 43.71),\n",
       " (49.49, 77.09),\n",
       " (32.18, 47.73),\n",
       " (49.59, 77.27),\n",
       " (33.32, 49.61),\n",
       " (24.76, 35.65),\n",
       " (37.82, 57.14),\n",
       " (52.64, 82.57),\n",
       " (43.41, 66.63),\n",
       " (66.77, 107.61),\n",
       " (59.85, 95.26),\n",
       " (29.81, 43.83),\n",
       " (29.32, 43.03),\n",
       " (53.68, 84.4),\n",
       " (47.93, 74.38),\n",
       " (33.86, 50.52),\n",
       " (34.46, 51.52),\n",
       " (49.18, 76.55),\n",
       " (73.62, 119.96),\n",
       " (67.68, 109.25),\n",
       " (32.61, 48.45),\n",
       " (51.24, 80.14),\n",
       " (23.92, 34.31),\n",
       " (26.38, 38.25),\n",
       " (35.38, 53.05),\n",
       " (28.55, 41.78),\n",
       " (49.25, 76.66),\n",
       " (66.96, 107.95),\n",
       " (93.75, 157.02),\n",
       " (56.63, 89.57),\n",
       " (45.02, 69.38),\n",
       " (50.58, 78.98),\n",
       " (57.12, 90.43),\n",
       " (48.89, 76.05),\n",
       " (29.73, 43.7),\n",
       " (35.36, 53.01),\n",
       " (127.4, 220.95),\n",
       " (41.7, 63.7),\n",
       " (32.02, 47.46),\n",
       " (56.98, 90.18),\n",
       " (15.57, 21.26),\n",
       " (31.63, 46.83),\n",
       " (82.9, 136.93),\n",
       " (60.13, 95.76),\n",
       " (60.39, 96.21),\n",
       " (26.82, 38.97),\n",
       " (36.24, 54.49),\n",
       " (22.55, 32.12),\n",
       " (31.09, 45.94),\n",
       " (82.74, 136.62),\n",
       " (109.38, 186.43),\n",
       " (22.28, 31.7),\n",
       " (30.1, 44.31),\n",
       " (34.73, 51.96),\n",
       " (40.94, 62.41),\n",
       " (39.08, 59.26),\n",
       " (69.86, 113.16),\n",
       " (46.92, 72.64),\n",
       " (31.51, 46.63),\n",
       " (33.72, 50.29),\n",
       " (121.62, 209.82),\n",
       " (101.13, 170.84),\n",
       " (76.49, 125.18),\n",
       " (61.8, 98.72),\n",
       " (24.12, 34.63),\n",
       " (36.09, 54.24),\n",
       " (41.36, 63.13),\n",
       " (34.75, 52.0),\n",
       " (48.64, 75.61),\n",
       " (31.63, 46.82),\n",
       " (71.33, 115.82),\n",
       " (50.97, 79.66),\n",
       " (53.6, 84.26),\n",
       " (46.21, 71.43),\n",
       " (52.91, 83.05),\n",
       " (26.55, 38.54),\n",
       " (67.79, 109.44),\n",
       " (35.11, 52.6),\n",
       " (39.32, 59.67),\n",
       " (35.77, 53.7),\n",
       " (29.2, 42.83),\n",
       " (31.3, 46.29),\n",
       " (73.74, 120.19),\n",
       " (19.87, 27.9),\n",
       " (31.67, 46.89),\n",
       " (29.12, 42.7),\n",
       " (50.49, 78.82),\n",
       " (27.22, 39.62),\n",
       " (32.49, 48.24),\n",
       " (82.07, 135.39),\n",
       " (45.93, 70.93),\n",
       " (70.63, 114.56),\n",
       " (44.56, 68.59),\n",
       " (32.28, 47.89),\n",
       " (45.22, 69.73),\n",
       " (61.19, 97.64),\n",
       " (26.08, 37.78),\n",
       " (58.32, 92.55),\n",
       " (51.77, 81.05),\n",
       " (63.18, 101.17),\n",
       " (26.94, 39.16),\n",
       " (46.74, 72.33),\n",
       " (30.79, 45.45),\n",
       " (29.37, 43.11),\n",
       " (30.42, 44.84),\n",
       " (67.01, 108.03),\n",
       " (55.01, 86.73),\n",
       " (28.89, 42.33),\n",
       " (27.62, 40.27),\n",
       " (40.54, 61.73),\n",
       " (28.12, 41.08),\n",
       " (40.48, 61.63),\n",
       " (23.35, 33.4),\n",
       " (45.53, 70.25),\n",
       " (44.59, 68.64),\n",
       " (30.92, 45.66),\n",
       " (44.91, 69.18),\n",
       " (86.33, 143.25),\n",
       " (48.34, 75.1),\n",
       " (25.65, 37.08),\n",
       " (41.32, 63.05),\n",
       " (35.2, 52.74),\n",
       " (41.42, 63.22),\n",
       " (55.66, 87.86),\n",
       " (33.87, 50.53),\n",
       " (46.8, 72.44),\n",
       " (73.29, 119.37),\n",
       " (22.13, 31.46),\n",
       " (24.35, 34.99),\n",
       " (64.8, 104.08),\n",
       " (81.52, 134.39),\n",
       " (77.25, 126.57),\n",
       " (53.2, 83.55),\n",
       " (51.12, 79.92),\n",
       " (55.06, 86.81),\n",
       " (56.35, 89.08),\n",
       " (35.89, 53.89),\n",
       " (56.67, 89.63),\n",
       " (49.23, 76.64),\n",
       " (39.68, 60.28),\n",
       " (27.26, 39.68),\n",
       " (55.04, 86.77),\n",
       " (99.83, 168.39),\n",
       " (37.65, 56.86),\n",
       " (33.9, 50.59),\n",
       " (61.83, 98.77),\n",
       " (34.92, 52.28),\n",
       " (27.48, 40.04),\n",
       " (36.19, 54.4),\n",
       " (53.46, 84.01),\n",
       " (28.58, 41.82),\n",
       " (46.79, 72.42),\n",
       " (21.16, 29.93),\n",
       " (58.39, 92.67),\n",
       " (47.84, 74.23),\n",
       " (59.39, 94.44),\n",
       " (64.35, 103.27),\n",
       " (49.68, 77.41),\n",
       " (52.92, 83.06),\n",
       " (28.18, 41.18),\n",
       " (30.11, 44.33),\n",
       " (31.91, 47.29),\n",
       " (74.45, 121.47),\n",
       " (32.84, 48.83),\n",
       " (71.28, 115.73),\n",
       " (33.01, 49.11),\n",
       " (53.53, 84.13),\n",
       " (49.24, 76.65),\n",
       " (45.69, 70.53),\n",
       " (21.1, 29.84),\n",
       " (60.4, 96.24),\n",
       " (54.03, 85.0),\n",
       " (28.08, 41.01),\n",
       " (23.91, 34.29),\n",
       " (28.77, 42.14),\n",
       " (46.24, 71.47),\n",
       " (70.02, 113.45),\n",
       " (52.27, 81.93),\n",
       " (71.38, 115.91),\n",
       " (70.82, 114.89),\n",
       " (28.23, 41.25),\n",
       " (26.13, 37.85),\n",
       " (66.96, 107.94),\n",
       " (33.4, 49.76),\n",
       " (35.33, 52.96),\n",
       " (18.61, 25.94),\n",
       " (67.1, 108.19),\n",
       " (31.37, 46.4),\n",
       " (64.1, 102.82),\n",
       " (49.05, 76.33),\n",
       " (137.29, 240.14),\n",
       " (36.31, 54.6),\n",
       " (57.79, 91.61),\n",
       " (52.13, 81.67),\n",
       " (26.79, 38.92),\n",
       " (41.33, 63.08),\n",
       " (60.87, 97.08),\n",
       " (73.04, 118.91),\n",
       " (27.26, 39.68),\n",
       " (46.37, 71.69),\n",
       " (35.52, 53.29),\n",
       " (26.87, 39.05),\n",
       " (56.1, 88.63),\n",
       " (54.93, 86.59),\n",
       " (30.94, 45.68),\n",
       " (46.58, 72.05),\n",
       " (78.31, 128.51),\n",
       " (29.15, 42.76),\n",
       " (23.81, 34.12),\n",
       " (29.38, 43.14),\n",
       " (58.25, 92.42),\n",
       " (72.05, 117.12),\n",
       " (31.03, 45.83),\n",
       " (79.18, 130.1),\n",
       " (69.91, 113.25),\n",
       " (21.36, 30.24),\n",
       " (31.17, 46.07),\n",
       " (36.93, 55.64),\n",
       " (64.12, 102.86),\n",
       " (112.42, 192.22),\n",
       " (49.97, 77.92),\n",
       " (53.83, 84.66),\n",
       " (28.49, 41.68),\n",
       " (34.76, 52.02),\n",
       " (59.13, 93.99),\n",
       " (23.11, 33.02),\n",
       " (29.5, 43.33),\n",
       " (24.3, 34.91),\n",
       " (32.83, 48.81),\n",
       " (24.45, 35.16),\n",
       " (68.8, 111.25),\n",
       " (70.17, 113.72),\n",
       " (76.1, 124.48),\n",
       " (56.38, 89.13),\n",
       " (51.52, 80.61),\n",
       " (71.38, 115.91),\n",
       " (44.54, 68.55),\n",
       " (24.7, 35.56),\n",
       " (138.3, 242.1),\n",
       " (24.85, 35.79),\n",
       " (38.8, 58.78),\n",
       " (27.26, 39.69),\n",
       " (72.53, 117.98),\n",
       " (32.98, 49.05),\n",
       " (71.36, 115.87),\n",
       " (25.69, 37.14),\n",
       " (63.67, 102.06),\n",
       " (124.16, 214.7),\n",
       " (43.22, 66.29),\n",
       " (49.1, 76.42),\n",
       " (27.47, 40.02),\n",
       " (43.85, 67.38),\n",
       " (70.3, 113.96),\n",
       " (29.22, 42.86),\n",
       " (60.51, 96.44),\n",
       " (67.31, 108.57),\n",
       " (27.16, 39.52),\n",
       " (22.86, 32.62),\n",
       " (23.92, 34.31),\n",
       " (30.55, 45.04),\n",
       " (27.41, 39.92),\n",
       " (81.96, 135.2),\n",
       " (39.2, 59.47),\n",
       " (33.01, 49.11),\n",
       " (61.66, 98.48),\n",
       " (39.07, 59.24),\n",
       " (58.89, 93.56),\n",
       " (27.37, 39.86),\n",
       " (27.9, 40.72),\n",
       " (72.16, 117.32),\n",
       " (30.03, 44.19),\n",
       " (33.98, 50.72),\n",
       " (32.09, 47.58),\n",
       " (58.7, 93.22),\n",
       " (17.47, 24.17),\n",
       " (69.01, 111.63),\n",
       " (34.65, 51.83),\n",
       " (48.02, 74.55),\n",
       " (21.86, 31.04),\n",
       " (58.42, 92.73),\n",
       " (24.91, 35.89),\n",
       " (63.41, 101.59),\n",
       " (26.76, 38.87),\n",
       " (20.88, 29.48),\n",
       " (37.58, 56.73),\n",
       " (30.3, 44.63),\n",
       " (25.55, 36.92),\n",
       " (58.8, 93.39),\n",
       " (46.23, 71.46),\n",
       " (38.62, 58.49),\n",
       " (36.78, 55.39),\n",
       " (42.84, 65.64),\n",
       " (43.39, 66.58),\n",
       " (31.24, 46.18),\n",
       " (42.21, 64.57),\n",
       " (49.91, 77.83),\n",
       " (73.02, 118.89),\n",
       " (28.77, 42.14),\n",
       " (39.03, 59.18),\n",
       " (53.28, 83.69),\n",
       " (126.77, 219.73),\n",
       " (50.32, 78.54),\n",
       " (113.92, 195.07),\n",
       " (39.37, 59.75),\n",
       " (50.1, 78.14),\n",
       " (33.51, 49.94),\n",
       " ...)"
      ]
     },
     "execution_count": 1410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate price interval for MAPE median\n",
    "y_pred_interval_cat_reg = tuple([(round(math.exp(el-el*MAPE_median_cat_reg),2),round(math.exp(el+el*MAPE_median_cat_reg),2)) for el in y_test_pred_cat_reg])\n",
    "y_pred_interval_cat_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model and Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model and cv\n",
    "save_model(best_model_cat_reg, title=\"best_model_cat_reg_01\", save=\"joblib\")\n",
    "save_model(grid_cat_reg, title=\"best_cv_cat_reg_01\", save=\"joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model 1: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 128)               15360     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 246,273\n",
      "Trainable params: 246,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model_nn_seq = models.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model_nn_seq.add(\n",
    "    layers.Dense(128,\n",
    "                 input_shape=(X_train_prep.shape[1], ),\n",
    "                 kernel_regularizer=regularizers.l1(0.005),\n",
    "                 activation='relu'))\n",
    "\n",
    "# Hidden Layers\n",
    "model_nn_seq.add(\n",
    "    layers.Dense(256,\n",
    "                 kernel_regularizer=regularizers.l1(0.005),\n",
    "                 activation='relu'))\n",
    "model_nn_seq.add(\n",
    "    layers.Dense(256,\n",
    "                 kernel_regularizer=regularizers.l1(0.005),\n",
    "                 activation='relu'))\n",
    "model_nn_seq.add(\n",
    "    layers.Dense(512,\n",
    "                 kernel_regularizer=regularizers.l1(0.005),\n",
    "                 activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model_nn_seq.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model_nn_seq.compile(loss='mean_absolute_percentage_error',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['mean_absolute_percentage_error'])\n",
    "\n",
    "# Model summary\n",
    "print(model_nn_seq.summary())\n",
    "\n",
    "# Visualize the neural network\n",
    "#SVG(model_to_dot(model_nn_seq, show_layer_names=False, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1332,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create pipeline with preprocessing\n",
    "#pipeline_nn_reg = Pipeline([('preprocessor', preprocessor),\n",
    "#                             ('nn_reg',\n",
    "#                              KerasRegressor(build_fn=model_nn_seq, epochs=20, batch_size=250))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1333,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "#param_grid_nn_reg = {\n",
    "#    'xgb_reg__bootstrap': [True, False],\n",
    "#    'nn_reg__epochs': [10, 20]\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1334,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create and fit GridSearchCV, save \"best_model\"\n",
    "#grid_nn_reg = GridSearchCV(pipeline_nn_reg,\n",
    "#                            param_grid_nn_reg,\n",
    "#                            cv=5,\n",
    "#                            scoring=scoring,\n",
    "#                            return_train_score=True,\n",
    "#                            verbose=4,\n",
    "#                            n_jobs=-1)\n",
    "\n",
    "#grid_nn_reg = grid_nn_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6792 samples, validate on 1698 samples\n",
      "Epoch 1/20\n",
      "6792/6792 [==============================] - 2s 279us/step - loss: 93.3331 - mean_absolute_percentage_error: 34.8009 - val_loss: 67.8246 - val_mean_absolute_percentage_error: 14.6580\n",
      "Epoch 2/20\n",
      "6792/6792 [==============================] - 1s 101us/step - loss: 59.8985 - mean_absolute_percentage_error: 11.4552 - val_loss: 54.2751 - val_mean_absolute_percentage_error: 10.7996\n",
      "Epoch 3/20\n",
      "6792/6792 [==============================] - 1s 99us/step - loss: 49.1774 - mean_absolute_percentage_error: 9.4147 - val_loss: 45.7147 - val_mean_absolute_percentage_error: 9.6148\n",
      "Epoch 4/20\n",
      "6792/6792 [==============================] - 1s 94us/step - loss: 41.9863 - mean_absolute_percentage_error: 8.3728 - val_loss: 40.1864 - val_mean_absolute_percentage_error: 9.0214\n",
      "Epoch 5/20\n",
      "6792/6792 [==============================] - 1s 104us/step - loss: 37.1921 - mean_absolute_percentage_error: 7.8087 - val_loss: 36.4616 - val_mean_absolute_percentage_error: 8.8906\n",
      "Epoch 6/20\n",
      "6792/6792 [==============================] - 1s 85us/step - loss: 33.8173 - mean_absolute_percentage_error: 7.6269 - val_loss: 33.5617 - val_mean_absolute_percentage_error: 8.7999\n",
      "Epoch 7/20\n",
      "6792/6792 [==============================] - 1s 85us/step - loss: 30.8359 - mean_absolute_percentage_error: 7.1850 - val_loss: 30.9606 - val_mean_absolute_percentage_error: 8.4749\n",
      "Epoch 8/20\n",
      "6792/6792 [==============================] - 1s 85us/step - loss: 28.9376 - mean_absolute_percentage_error: 7.3636 - val_loss: 29.1452 - val_mean_absolute_percentage_error: 8.5329\n",
      "Epoch 9/20\n",
      "6792/6792 [==============================] - 1s 87us/step - loss: 26.7868 - mean_absolute_percentage_error: 6.9371 - val_loss: 27.4437 - val_mean_absolute_percentage_error: 8.4096\n",
      "Epoch 10/20\n",
      "6792/6792 [==============================] - 1s 92us/step - loss: 25.2255 - mean_absolute_percentage_error: 6.8357 - val_loss: 26.2704 - val_mean_absolute_percentage_error: 8.5764\n",
      "Epoch 11/20\n",
      "6792/6792 [==============================] - 1s 88us/step - loss: 23.8998 - mean_absolute_percentage_error: 6.7732 - val_loss: 25.0619 - val_mean_absolute_percentage_error: 8.5447\n",
      "Epoch 12/20\n",
      "6792/6792 [==============================] - 1s 87us/step - loss: 22.7369 - mean_absolute_percentage_error: 6.7069 - val_loss: 23.8291 - val_mean_absolute_percentage_error: 8.3335\n",
      "Epoch 13/20\n",
      "6792/6792 [==============================] - 1s 85us/step - loss: 21.7266 - mean_absolute_percentage_error: 6.6577 - val_loss: 22.7677 - val_mean_absolute_percentage_error: 8.1736\n",
      "Epoch 14/20\n",
      "6792/6792 [==============================] - 1s 89us/step - loss: 20.7319 - mean_absolute_percentage_error: 6.5219 - val_loss: 21.9643 - val_mean_absolute_percentage_error: 8.1724\n",
      "Epoch 15/20\n",
      "6792/6792 [==============================] - 1s 93us/step - loss: 19.8683 - mean_absolute_percentage_error: 6.4250 - val_loss: 21.1123 - val_mean_absolute_percentage_error: 8.0471\n",
      "Epoch 16/20\n",
      "6792/6792 [==============================] - 1s 95us/step - loss: 19.1037 - mean_absolute_percentage_error: 6.3455 - val_loss: 20.5346 - val_mean_absolute_percentage_error: 8.1220\n",
      "Epoch 17/20\n",
      "6792/6792 [==============================] - 1s 91us/step - loss: 18.5221 - mean_absolute_percentage_error: 6.3857 - val_loss: 20.0001 - val_mean_absolute_percentage_error: 8.1774\n",
      "Epoch 18/20\n",
      "6792/6792 [==============================] - 1s 92us/step - loss: 17.9931 - mean_absolute_percentage_error: 6.4205 - val_loss: 19.3459 - val_mean_absolute_percentage_error: 8.0546\n",
      "Epoch 19/20\n",
      "6792/6792 [==============================] - 1s 95us/step - loss: 17.7369 - mean_absolute_percentage_error: 6.6736 - val_loss: 18.8753 - val_mean_absolute_percentage_error: 8.0792\n",
      "Epoch 20/20\n",
      "6792/6792 [==============================] - 1s 102us/step - loss: 16.9468 - mean_absolute_percentage_error: 6.3581 - val_loss: 18.4938 - val_mean_absolute_percentage_error: 8.1329\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#model_nn_seq_start = time.time()\n",
    "\n",
    "best_model_nn_reg = model_nn_seq.fit(X_train_prep,\n",
    "                                        y_train,\n",
    "                                        epochs=20,\n",
    "                                        batch_size=256,\n",
    "                                        validation_split=0.2)\n",
    "\n",
    "#model_nn_seq_end = time.time()\n",
    "\n",
    "#print(f\"Time taken to run: {round((model_nn_seq_end - model_nn_seq_start)/60,1)} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_train_pred_nn_reg = model_nn_seq.predict(X_train_prep).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1337,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.14\n",
      "RMSE: 0.37\n",
      "MAE: 0.26\n",
      "R2: 0.58\n",
      "MAPE: 6.53\n",
      "MAPE median: 4.56\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_train, y_train_pred_nn_reg, model=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "y_test_pred_nn_reg = model_nn_seq.predict(X_test_prep).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1339,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.17\n",
      "RMSE: 0.41\n",
      "MAE: 0.31\n",
      "R2: 0.49\n",
      "MAPE: 8.07\n",
      "MAPE median: 6.23\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "model_eval(y_test, y_test_pred_nn_reg, model=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1340,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39021417, 0.4227297 ])"
      ]
     },
     "execution_count": 1340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display confidence interval (scipy stats)\n",
    "confidence = 0.95\n",
    "squared_errors = (y_test_pred_nn_reg - y_test)**2\n",
    "np.sqrt(\n",
    "    stats.t.interval(confidence,\n",
    "                     len(squared_errors) - 1,\n",
    "                     loc=squared_errors.mean(),\n",
    "                     scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Median Price Intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1341,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save MAPE_median as variable\n",
    "MAPE_median_nn_reg = (median_absolute_percentage_error(y_test, y_test_pred_nn_reg))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1342,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43.67, 72.1),\n",
       " (55.56, 94.71),\n",
       " (59.71, 102.77),\n",
       " (30.74, 48.44),\n",
       " (35.0, 56.12),\n",
       " (23.73, 36.13),\n",
       " (42.97, 70.79),\n",
       " (44.16, 73.01),\n",
       " (48.56, 81.32),\n",
       " (38.34, 62.22),\n",
       " (21.07, 31.58),\n",
       " (30.21, 47.5),\n",
       " (61.2, 105.67),\n",
       " (26.18, 40.38),\n",
       " (42.28, 69.51),\n",
       " (62.86, 108.93),\n",
       " (79.12, 141.36),\n",
       " (72.13, 127.3),\n",
       " (50.79, 85.56),\n",
       " (59.56, 102.48),\n",
       " (110.23, 205.81),\n",
       " (54.23, 92.15),\n",
       " (37.56, 60.78),\n",
       " (53.83, 91.39),\n",
       " (24.49, 37.45),\n",
       " (53.25, 90.26),\n",
       " (82.37, 147.95),\n",
       " (39.91, 65.12),\n",
       " (25.84, 39.79),\n",
       " (54.29, 92.27),\n",
       " (25.44, 39.1),\n",
       " (80.46, 144.08),\n",
       " (46.78, 77.94),\n",
       " (23.57, 35.86),\n",
       " (41.23, 67.56),\n",
       " (51.76, 87.4),\n",
       " (33.79, 53.92),\n",
       " (52.76, 89.32),\n",
       " (27.63, 42.93),\n",
       " (27.03, 41.87),\n",
       " (34.39, 55.0),\n",
       " (24.13, 36.83),\n",
       " (45.05, 74.68),\n",
       " (33.72, 53.79),\n",
       " (30.64, 48.27),\n",
       " (39.8, 64.92),\n",
       " (55.7, 94.98),\n",
       " (79.46, 142.05),\n",
       " (72.1, 127.23),\n",
       " (90.09, 163.75),\n",
       " (32.89, 52.29),\n",
       " (119.8, 226.16),\n",
       " (47.77, 79.83),\n",
       " (50.01, 84.07),\n",
       " (31.4, 49.61),\n",
       " (37.25, 60.21),\n",
       " (60.84, 104.98),\n",
       " (37.59, 60.85),\n",
       " (54.07, 91.84),\n",
       " (32.97, 52.45),\n",
       " (57.02, 97.53),\n",
       " (54.26, 92.2),\n",
       " (73.24, 129.51),\n",
       " (48.25, 80.72),\n",
       " (46.33, 77.09),\n",
       " (56.17, 95.9),\n",
       " (20.92, 31.32),\n",
       " (30.34, 47.73),\n",
       " (39.66, 64.66),\n",
       " (44.12, 72.95),\n",
       " (51.94, 87.75),\n",
       " (91.66, 166.98),\n",
       " (30.98, 48.87),\n",
       " (53.38, 90.52),\n",
       " (131.54, 251.42),\n",
       " (32.46, 51.53),\n",
       " (17.38, 25.39),\n",
       " (53.03, 89.84),\n",
       " (41.34, 67.76),\n",
       " (24.64, 37.71),\n",
       " (29.01, 45.36),\n",
       " (22.6, 34.19),\n",
       " (54.54, 92.75),\n",
       " (20.42, 30.49),\n",
       " (83.09, 149.43),\n",
       " (71.54, 126.11),\n",
       " (39.75, 64.82),\n",
       " (57.31, 98.1),\n",
       " (46.14, 76.74),\n",
       " (50.22, 84.47),\n",
       " (72.64, 128.32),\n",
       " (37.41, 60.51),\n",
       " (43.31, 71.43),\n",
       " (166.69, 328.79),\n",
       " (74.11, 131.27),\n",
       " (20.11, 29.96),\n",
       " (124.84, 236.97),\n",
       " (52.18, 88.22),\n",
       " (29.84, 46.84),\n",
       " (21.78, 32.78),\n",
       " (31.15, 49.18),\n",
       " (36.53, 58.91),\n",
       " (27.58, 42.84),\n",
       " (51.6, 87.1),\n",
       " (28.58, 44.6),\n",
       " (53.0, 89.79),\n",
       " (23.11, 35.06),\n",
       " (32.94, 52.39),\n",
       " (36.56, 58.95),\n",
       " (59.21, 101.79),\n",
       " (34.16, 54.59),\n",
       " (19.33, 28.63),\n",
       " (83.35, 149.96),\n",
       " (25.15, 38.59),\n",
       " (90.04, 163.66),\n",
       " (60.0, 103.33),\n",
       " (33.13, 52.73),\n",
       " (102.5, 189.54),\n",
       " (43.57, 71.92),\n",
       " (39.05, 63.53),\n",
       " (29.62, 46.45),\n",
       " (47.52, 79.34),\n",
       " (30.57, 48.14),\n",
       " (40.75, 66.67),\n",
       " (36.85, 59.48),\n",
       " (48.14, 80.51),\n",
       " (39.41, 64.18),\n",
       " (43.08, 71.0),\n",
       " (36.65, 59.11),\n",
       " (24.71, 37.82),\n",
       " (104.93, 194.63),\n",
       " (23.43, 35.62),\n",
       " (30.9, 48.73),\n",
       " (17.26, 25.19),\n",
       " (48.97, 82.1),\n",
       " (40.76, 66.68),\n",
       " (29.33, 45.92),\n",
       " (39.82, 64.95),\n",
       " (146.87, 284.87),\n",
       " (155.66, 304.25),\n",
       " (40.45, 66.11),\n",
       " (45.6, 75.72),\n",
       " (42.71, 70.31),\n",
       " (24.87, 38.1),\n",
       " (48.81, 81.79),\n",
       " (97.53, 179.16),\n",
       " (51.14, 86.23),\n",
       " (32.11, 50.9),\n",
       " (51.58, 87.06),\n",
       " (85.6, 154.55),\n",
       " (94.1, 172.04),\n",
       " (57.92, 99.29),\n",
       " (68.89, 120.85),\n",
       " (24.95, 38.24),\n",
       " (89.36, 162.26),\n",
       " (90.09, 163.75),\n",
       " (14.59, 20.82),\n",
       " (51.39, 86.69),\n",
       " (34.11, 54.5),\n",
       " (95.53, 175.0),\n",
       " (34.77, 55.69),\n",
       " (28.51, 44.48),\n",
       " (89.43, 162.4),\n",
       " (75.8, 134.66),\n",
       " (35.54, 57.09),\n",
       " (123.15, 233.34),\n",
       " (44.86, 74.33),\n",
       " (28.54, 44.53),\n",
       " (26.8, 41.47),\n",
       " (30.58, 48.16),\n",
       " (58.62, 100.65),\n",
       " (176.54, 350.89),\n",
       " (33.96, 54.23),\n",
       " (35.94, 57.83),\n",
       " (26.91, 41.67),\n",
       " (54.8, 93.25),\n",
       " (29.93, 47.0),\n",
       " (51.79, 87.48),\n",
       " (27.28, 42.31),\n",
       " (32.58, 51.74),\n",
       " (55.04, 93.71),\n",
       " (87.3, 158.03),\n",
       " (66.47, 116.04),\n",
       " (42.51, 69.94),\n",
       " (29.4, 46.06),\n",
       " (58.54, 100.48),\n",
       " (38.78, 63.02),\n",
       " (50.83, 85.63),\n",
       " (57.61, 98.69),\n",
       " (35.96, 57.87),\n",
       " (72.54, 128.12),\n",
       " (32.91, 52.32),\n",
       " (87.88, 159.22),\n",
       " (74.41, 131.86),\n",
       " (47.07, 78.49),\n",
       " (88.3, 160.07),\n",
       " (123.3, 233.66),\n",
       " (23.9, 36.43),\n",
       " (69.59, 122.22),\n",
       " (26.52, 40.97),\n",
       " (55.16, 93.94),\n",
       " (35.94, 57.82),\n",
       " (81.38, 145.93),\n",
       " (45.37, 75.29),\n",
       " (33.98, 54.26),\n",
       " (35.91, 57.77),\n",
       " (74.64, 132.32),\n",
       " (151.06, 294.08),\n",
       " (41.7, 68.43),\n",
       " (31.69, 50.14),\n",
       " (159.72, 313.26),\n",
       " (25.8, 39.72),\n",
       " (31.62, 50.01),\n",
       " (85.1, 153.51),\n",
       " (30.14, 47.37),\n",
       " (49.65, 83.38),\n",
       " (28.51, 44.48),\n",
       " (40.78, 66.72),\n",
       " (62.84, 108.88),\n",
       " (60.72, 104.73),\n",
       " (85.42, 154.17),\n",
       " (25.61, 39.4),\n",
       " (59.16, 101.69),\n",
       " (42.21, 69.38),\n",
       " (86.32, 156.01),\n",
       " (24.47, 37.41),\n",
       " (17.71, 25.95),\n",
       " (22.37, 33.79),\n",
       " (73.85, 130.75),\n",
       " (80.77, 144.7),\n",
       " (52.73, 89.26),\n",
       " (79.03, 141.17),\n",
       " (41.23, 67.55),\n",
       " (29.18, 45.67),\n",
       " (45.41, 75.36),\n",
       " (41.19, 67.47),\n",
       " (43.53, 71.85),\n",
       " (57.7, 98.85),\n",
       " (50.59, 85.17),\n",
       " (50.07, 84.18),\n",
       " (58.37, 100.15),\n",
       " (68.44, 119.95),\n",
       " (45.77, 76.04),\n",
       " (128.22, 244.25),\n",
       " (32.03, 50.75),\n",
       " (30.68, 48.33),\n",
       " (26.55, 41.03),\n",
       " (42.7, 70.29),\n",
       " (45.36, 75.27),\n",
       " (53.89, 91.5),\n",
       " (36.6, 59.02),\n",
       " (45.35, 75.24),\n",
       " (41.89, 68.79),\n",
       " (85.36, 154.05),\n",
       " (49.12, 82.38),\n",
       " (70.52, 124.07),\n",
       " (51.04, 86.04),\n",
       " (41.17, 67.45),\n",
       " (27.53, 42.76),\n",
       " (42.69, 70.27),\n",
       " (29.16, 45.64),\n",
       " (31.8, 50.34),\n",
       " (50.24, 84.51),\n",
       " (28.04, 43.65),\n",
       " (31.54, 49.88),\n",
       " (28.4, 44.29),\n",
       " (46.77, 77.92),\n",
       " (34.7, 55.57),\n",
       " (65.6, 114.33),\n",
       " (43.22, 71.27),\n",
       " (23.08, 35.02),\n",
       " (51.85, 87.58),\n",
       " (43.58, 71.94),\n",
       " (60.61, 104.52),\n",
       " (115.36, 216.68),\n",
       " (103.95, 192.57),\n",
       " (94.04, 171.91),\n",
       " (34.16, 54.59),\n",
       " (54.68, 93.01),\n",
       " (53.99, 91.68),\n",
       " (31.98, 50.66),\n",
       " (73.41, 129.86),\n",
       " (48.28, 80.78),\n",
       " (167.08, 329.66),\n",
       " (70.41, 123.86),\n",
       " (46.38, 77.19),\n",
       " (34.69, 55.56),\n",
       " (21.7, 32.65),\n",
       " (38.76, 62.99),\n",
       " (58.26, 99.95),\n",
       " (34.65, 55.48),\n",
       " (88.98, 161.46),\n",
       " (76.89, 136.85),\n",
       " (46.62, 77.65),\n",
       " (83.63, 150.52),\n",
       " (24.12, 36.8),\n",
       " (32.24, 51.14),\n",
       " (57.29, 98.07),\n",
       " (25.69, 39.53),\n",
       " (71.92, 126.88),\n",
       " (29.48, 46.2),\n",
       " (53.27, 90.3),\n",
       " (30.42, 47.87),\n",
       " (43.6, 71.97),\n",
       " (47.92, 80.1),\n",
       " (33.43, 53.27),\n",
       " (69.84, 122.73),\n",
       " (54.52, 92.71),\n",
       " (40.13, 65.52),\n",
       " (74.98, 133.0),\n",
       " (106.94, 198.86),\n",
       " (40.69, 66.55),\n",
       " (99.31, 182.88),\n",
       " (55.66, 94.91),\n",
       " (148.29, 287.98),\n",
       " (30.44, 47.9),\n",
       " (24.12, 36.81),\n",
       " (26.97, 41.77),\n",
       " (39.23, 63.85),\n",
       " (74.68, 132.41),\n",
       " (41.54, 68.12),\n",
       " (61.6, 106.46),\n",
       " (89.52, 162.58),\n",
       " (25.98, 40.05),\n",
       " (113.34, 212.39),\n",
       " (44.2, 73.1),\n",
       " (53.77, 91.27),\n",
       " (51.89, 87.66),\n",
       " (19.48, 28.89),\n",
       " (52.75, 89.3),\n",
       " (62.29, 107.82),\n",
       " (39.12, 63.65),\n",
       " (32.51, 51.62),\n",
       " (51.87, 87.61),\n",
       " (63.58, 110.34),\n",
       " (33.25, 52.94),\n",
       " (23.37, 35.52),\n",
       " (42.26, 69.48),\n",
       " (23.17, 35.17),\n",
       " (42.4, 69.72),\n",
       " (50.46, 84.93),\n",
       " (132.68, 253.88),\n",
       " (24.12, 36.81),\n",
       " (25.23, 38.74),\n",
       " (51.0, 85.95),\n",
       " (32.56, 51.71),\n",
       " (26.76, 41.41),\n",
       " (47.26, 78.86),\n",
       " (52.37, 88.58),\n",
       " (67.55, 118.19),\n",
       " (61.27, 105.82),\n",
       " (50.91, 85.78),\n",
       " (23.67, 36.03),\n",
       " (130.69, 249.58),\n",
       " (19.63, 29.15),\n",
       " (24.07, 36.71),\n",
       " (43.73, 72.21),\n",
       " (50.21, 84.46),\n",
       " (98.78, 181.76),\n",
       " (27.55, 42.8),\n",
       " (39.13, 63.67),\n",
       " (32.43, 51.47),\n",
       " (29.44, 46.13),\n",
       " (27.28, 42.32),\n",
       " (58.74, 100.88),\n",
       " (31.01, 48.92),\n",
       " (53.66, 91.05),\n",
       " (36.09, 58.09),\n",
       " (53.02, 89.82),\n",
       " (67.97, 119.02),\n",
       " (28.84, 45.07),\n",
       " (23.36, 35.5),\n",
       " (42.35, 69.65),\n",
       " (94.06, 171.96),\n",
       " (24.82, 38.01),\n",
       " (33.74, 53.83),\n",
       " (81.05, 145.26),\n",
       " (27.28, 42.32),\n",
       " (39.04, 63.51),\n",
       " (26.48, 40.92),\n",
       " (63.01, 109.23),\n",
       " (61.07, 105.43),\n",
       " (141.36, 272.79),\n",
       " (33.26, 52.97),\n",
       " (26.18, 40.39),\n",
       " (47.62, 79.52),\n",
       " (63.12, 109.45),\n",
       " (26.84, 41.54),\n",
       " (21.56, 32.41),\n",
       " (59.98, 103.3),\n",
       " (82.38, 147.97),\n",
       " (60.74, 104.78),\n",
       " (29.13, 45.57),\n",
       " (33.14, 52.75),\n",
       " (26.66, 41.22),\n",
       " (98.05, 180.23),\n",
       " (49.09, 82.33),\n",
       " (66.83, 116.75),\n",
       " (62.37, 107.98),\n",
       " (26.48, 40.91),\n",
       " (31.65, 50.08),\n",
       " (162.03, 318.39),\n",
       " (23.16, 35.16),\n",
       " (44.99, 74.57),\n",
       " (37.68, 61.0),\n",
       " (34.06, 54.41),\n",
       " (53.67, 91.07),\n",
       " (33.72, 53.8),\n",
       " (25.74, 39.62),\n",
       " (31.04, 48.97),\n",
       " (25.32, 38.88),\n",
       " (61.33, 105.94),\n",
       " (64.27, 111.7),\n",
       " (111.58, 208.67),\n",
       " (27.19, 42.15),\n",
       " (51.48, 86.87),\n",
       " (17.56, 25.68),\n",
       " (52.29, 88.42),\n",
       " (47.68, 79.64),\n",
       " (87.43, 158.28),\n",
       " (24.03, 36.65),\n",
       " (17.95, 26.34),\n",
       " (24.77, 37.92),\n",
       " (61.42, 106.11),\n",
       " (27.76, 43.15),\n",
       " (44.4, 73.46),\n",
       " (33.73, 53.82),\n",
       " (58.45, 100.32),\n",
       " (36.66, 59.13),\n",
       " (45.36, 75.26),\n",
       " (43.2, 71.22),\n",
       " (80.29, 143.73),\n",
       " (45.59, 75.7),\n",
       " (34.26, 54.77),\n",
       " (46.44, 77.31),\n",
       " (26.24, 40.5),\n",
       " (25.74, 39.61),\n",
       " (36.03, 57.99),\n",
       " (27.79, 43.22),\n",
       " (88.29, 160.06),\n",
       " (58.66, 100.72),\n",
       " (58.79, 100.98),\n",
       " (27.02, 41.86),\n",
       " (63.19, 109.58),\n",
       " (40.04, 65.36),\n",
       " (14.63, 20.89),\n",
       " (93.68, 171.17),\n",
       " (21.04, 31.52),\n",
       " (25.55, 39.29),\n",
       " (44.1, 72.92),\n",
       " (58.17, 99.76),\n",
       " (45.39, 75.33),\n",
       " (54.05, 91.8),\n",
       " (31.7, 50.15),\n",
       " (36.92, 59.61),\n",
       " (89.59, 162.73),\n",
       " (81.51, 146.21),\n",
       " (49.3, 82.72),\n",
       " (42.02, 69.03),\n",
       " (39.29, 63.97),\n",
       " (28.32, 44.15),\n",
       " (29.97, 47.07),\n",
       " (49.37, 82.84),\n",
       " (92.07, 167.84),\n",
       " (66.07, 115.25),\n",
       " (23.63, 35.97),\n",
       " (32.07, 50.82),\n",
       " (89.11, 161.73),\n",
       " (24.64, 37.71),\n",
       " (19.46, 28.87),\n",
       " (34.16, 54.59),\n",
       " (37.24, 60.19),\n",
       " (63.29, 109.78),\n",
       " (80.69, 144.53),\n",
       " (41.66, 68.35),\n",
       " (23.18, 35.19),\n",
       " (90.52, 164.65),\n",
       " (30.3, 47.66),\n",
       " (38.46, 62.45),\n",
       " (20.93, 31.34),\n",
       " (34.78, 55.72),\n",
       " (58.25, 99.93),\n",
       " (75.46, 133.97),\n",
       " (32.52, 51.63),\n",
       " (29.89, 46.92),\n",
       " (52.54, 88.9),\n",
       " (25.38, 38.99),\n",
       " (37.17, 60.07),\n",
       " (130.01, 248.11),\n",
       " (38.57, 62.64),\n",
       " (31.0, 48.9),\n",
       " (33.19, 52.83),\n",
       " (23.84, 36.32),\n",
       " (87.14, 157.69),\n",
       " (37.69, 61.03),\n",
       " (32.59, 51.76),\n",
       " (43.07, 70.99),\n",
       " (32.68, 51.91),\n",
       " (52.45, 88.74),\n",
       " (76.12, 135.3),\n",
       " (44.48, 73.63),\n",
       " (99.62, 183.52),\n",
       " (46.44, 77.31),\n",
       " (25.55, 39.28),\n",
       " (20.97, 31.41),\n",
       " (20.45, 30.54),\n",
       " (69.28, 121.61),\n",
       " (60.76, 104.82),\n",
       " (45.44, 75.43),\n",
       " (89.67, 162.9),\n",
       " (33.47, 53.35),\n",
       " (75.99, 135.04),\n",
       " (22.96, 34.81),\n",
       " (57.68, 98.82),\n",
       " (29.05, 45.44),\n",
       " (49.06, 82.27),\n",
       " (89.99, 163.54),\n",
       " (22.71, 34.38),\n",
       " (23.96, 36.54),\n",
       " (29.03, 45.4),\n",
       " (74.74, 132.52),\n",
       " (35.27, 56.6),\n",
       " (60.87, 105.04),\n",
       " (58.83, 101.05),\n",
       " (28.7, 44.82),\n",
       " (53.36, 90.47),\n",
       " (26.45, 40.85),\n",
       " (55.93, 95.44),\n",
       " (83.14, 149.52),\n",
       " (56.3, 96.15),\n",
       " (21.46, 32.25),\n",
       " (45.87, 76.23),\n",
       " (30.61, 48.2),\n",
       " (148.38, 288.19),\n",
       " (24.79, 37.97),\n",
       " (27.11, 42.01),\n",
       " (53.24, 90.24),\n",
       " (41.8, 68.61),\n",
       " (36.06, 58.05),\n",
       " (31.03, 48.96),\n",
       " (86.87, 157.15),\n",
       " (55.7, 94.98),\n",
       " (90.09, 163.75),\n",
       " (65.88, 114.87),\n",
       " (21.43, 32.2),\n",
       " (37.19, 60.1),\n",
       " (55.9, 95.36),\n",
       " (37.97, 61.53),\n",
       " (32.42, 51.45),\n",
       " (45.29, 75.14),\n",
       " (27.78, 43.19),\n",
       " (36.64, 59.11),\n",
       " (34.01, 54.33),\n",
       " (40.74, 66.65),\n",
       " (30.46, 47.94),\n",
       " (136.75, 262.73),\n",
       " (43.44, 71.68),\n",
       " (54.05, 91.8),\n",
       " (65.34, 113.81),\n",
       " (30.15, 47.4),\n",
       " (64.39, 111.94),\n",
       " (32.64, 51.85),\n",
       " (24.42, 37.32),\n",
       " (87.88, 159.22),\n",
       " (66.72, 116.54),\n",
       " (38.53, 62.57),\n",
       " (36.1, 58.12),\n",
       " (52.97, 89.74),\n",
       " (48.07, 80.39),\n",
       " (30.22, 47.51),\n",
       " (40.64, 66.46),\n",
       " (47.51, 79.32),\n",
       " (45.33, 75.21),\n",
       " (49.52, 83.14),\n",
       " (56.63, 96.79),\n",
       " (73.43, 129.9),\n",
       " (28.87, 45.13),\n",
       " (20.54, 30.68),\n",
       " (30.55, 48.1),\n",
       " (24.17, 36.89),\n",
       " (90.29, 164.16),\n",
       " (35.17, 56.43),\n",
       " (63.94, 111.05),\n",
       " (52.6, 89.02),\n",
       " (42.39, 69.71),\n",
       " (49.74, 83.56),\n",
       " (99.19, 182.62),\n",
       " (44.39, 73.46),\n",
       " (62.34, 107.9),\n",
       " (52.39, 88.62),\n",
       " (40.42, 66.05),\n",
       " (37.24, 60.19),\n",
       " (41.74, 68.51),\n",
       " (25.48, 39.16),\n",
       " (68.08, 119.22),\n",
       " (60.15, 103.62),\n",
       " (25.59, 39.36),\n",
       " (50.81, 85.59),\n",
       " (21.78, 32.79),\n",
       " (26.54, 41.01),\n",
       " (52.89, 89.58),\n",
       " (36.46, 58.78),\n",
       " (31.87, 50.46),\n",
       " (44.17, 73.03),\n",
       " (24.77, 37.94),\n",
       " (50.33, 84.67),\n",
       " (44.46, 73.59),\n",
       " (22.0, 33.17),\n",
       " (38.59, 62.69),\n",
       " (80.61, 144.38),\n",
       " (40.31, 65.84),\n",
       " (49.67, 83.43),\n",
       " (58.9, 101.19),\n",
       " (64.91, 112.96),\n",
       " (44.38, 73.43),\n",
       " (27.18, 42.14),\n",
       " (19.95, 29.68),\n",
       " (63.23, 109.66),\n",
       " (52.17, 88.2),\n",
       " (24.36, 37.21),\n",
       " (72.61, 128.26),\n",
       " (36.62, 59.07),\n",
       " (46.13, 76.73),\n",
       " (37.48, 60.64),\n",
       " (39.44, 64.25),\n",
       " (45.53, 75.6),\n",
       " (37.85, 61.31),\n",
       " (24.87, 38.11),\n",
       " (22.44, 33.92),\n",
       " (32.83, 52.18),\n",
       " (57.98, 99.4),\n",
       " (30.97, 48.85),\n",
       " (32.3, 51.23),\n",
       " (30.13, 47.36),\n",
       " (47.81, 79.89),\n",
       " (123.72, 234.56),\n",
       " (94.8, 173.5),\n",
       " (52.75, 89.3),\n",
       " (29.91, 46.96),\n",
       " (27.71, 43.07),\n",
       " (42.05, 69.08),\n",
       " (56.07, 95.71),\n",
       " (62.16, 107.55),\n",
       " (32.08, 50.84),\n",
       " (32.42, 51.46),\n",
       " (60.27, 103.87),\n",
       " (44.61, 73.86),\n",
       " (93.02, 169.81),\n",
       " (40.45, 66.12),\n",
       " (30.25, 47.58),\n",
       " (33.51, 53.42),\n",
       " (40.88, 66.9),\n",
       " (32.96, 52.43),\n",
       " (48.67, 81.52),\n",
       " (92.49, 168.72),\n",
       " (85.75, 154.86),\n",
       " (25.58, 39.33),\n",
       " (19.42, 28.8),\n",
       " (35.43, 56.9),\n",
       " (52.17, 88.2),\n",
       " (19.86, 29.53),\n",
       " (55.87, 95.31),\n",
       " (26.81, 41.49),\n",
       " (51.65, 87.2),\n",
       " (41.65, 68.34),\n",
       " (68.72, 120.51),\n",
       " (32.5, 51.59),\n",
       " (49.79, 83.65),\n",
       " (26.72, 41.34),\n",
       " (25.39, 39.02),\n",
       " (44.61, 73.87),\n",
       " (59.17, 101.72),\n",
       " (121.64, 230.09),\n",
       " (186.99, 374.49),\n",
       " (40.14, 65.54),\n",
       " (52.21, 88.28),\n",
       " (34.78, 55.71),\n",
       " (22.91, 34.72),\n",
       " (94.98, 173.86),\n",
       " (28.62, 44.67),\n",
       " (60.11, 103.55),\n",
       " (67.91, 118.88),\n",
       " (33.04, 52.56),\n",
       " (30.38, 47.8),\n",
       " (33.61, 53.6),\n",
       " (33.63, 53.64),\n",
       " (20.57, 30.74),\n",
       " (67.28, 117.64),\n",
       " (28.76, 44.92),\n",
       " (30.06, 47.23),\n",
       " (31.5, 49.79),\n",
       " (28.28, 44.07),\n",
       " (47.08, 78.52),\n",
       " (25.02, 38.37),\n",
       " (28.24, 44.0),\n",
       " (44.96, 74.52),\n",
       " (28.38, 44.26),\n",
       " (49.86, 83.79),\n",
       " (32.85, 52.23),\n",
       " (25.19, 38.65),\n",
       " (31.46, 49.73),\n",
       " (58.69, 100.79),\n",
       " (39.37, 64.12),\n",
       " (56.49, 96.52),\n",
       " (52.15, 88.15),\n",
       " (26.85, 41.56),\n",
       " (30.58, 48.16),\n",
       " (54.65, 92.97),\n",
       " (46.92, 78.21),\n",
       " (34.58, 55.35),\n",
       " (35.02, 56.14),\n",
       " (48.73, 81.63),\n",
       " (95.16, 174.24),\n",
       " (68.59, 120.24),\n",
       " (29.42, 46.09),\n",
       " (63.44, 110.06),\n",
       " (24.99, 38.32),\n",
       " (27.05, 41.91),\n",
       " (43.02, 70.89),\n",
       " (37.14, 60.02),\n",
       " (60.01, 103.35),\n",
       " (46.14, 76.75),\n",
       " (90.38, 164.35),\n",
       " (53.86, 91.44),\n",
       " (47.88, 80.03),\n",
       " (51.33, 86.59),\n",
       " (54.21, 92.11),\n",
       " (41.29, 67.66),\n",
       " (31.43, 49.68),\n",
       " (38.77, 63.02),\n",
       " (91.96, 167.61),\n",
       " (38.24, 62.03),\n",
       " (38.1, 61.77),\n",
       " (49.72, 83.52),\n",
       " (18.68, 27.55),\n",
       " (32.94, 52.39),\n",
       " (77.81, 138.7),\n",
       " (120.8, 228.3),\n",
       " (68.27, 119.62),\n",
       " (46.13, 76.72),\n",
       " (43.17, 71.17),\n",
       " (30.3, 47.65),\n",
       " (30.96, 48.83),\n",
       " (79.06, 141.23),\n",
       " (121.43, 229.64),\n",
       " (17.89, 26.23),\n",
       " (26.75, 41.38),\n",
       " (34.92, 55.98),\n",
       " (33.82, 53.97),\n",
       " (32.2, 51.06),\n",
       " (98.15, 180.45),\n",
       " (42.24, 69.43),\n",
       " (27.32, 42.39),\n",
       " (28.77, 44.95),\n",
       " (126.2, 239.89),\n",
       " (134.54, 257.93),\n",
       " (100.32, 184.98),\n",
       " (87.43, 158.28),\n",
       " (22.49, 34.0),\n",
       " (33.2, 52.86),\n",
       " (41.75, 68.52),\n",
       " (38.21, 61.98),\n",
       " (55.08, 93.79),\n",
       " (29.57, 46.36),\n",
       " (73.39, 129.81),\n",
       " (47.8, 79.88),\n",
       " (70.71, 124.46),\n",
       " (52.27, 88.39),\n",
       " (53.49, 90.73),\n",
       " (28.58, 44.61),\n",
       " (69.93, 122.91),\n",
       " (31.79, 50.31),\n",
       " (40.19, 65.63),\n",
       " (36.33, 58.54),\n",
       " (28.13, 43.81),\n",
       " (32.64, 51.84),\n",
       " (51.14, 86.22),\n",
       " (20.49, 30.6),\n",
       " (28.28, 44.08),\n",
       " (32.2, 51.06),\n",
       " (44.19, 73.08),\n",
       " (25.61, 39.39),\n",
       " (31.8, 50.33),\n",
       " (81.5, 146.18),\n",
       " (43.63, 72.02),\n",
       " (57.58, 98.62),\n",
       " (42.55, 70.01),\n",
       " (30.5, 48.01),\n",
       " (43.96, 72.64),\n",
       " (58.69, 100.78),\n",
       " (22.91, 34.72),\n",
       " (46.67, 77.74),\n",
       " (50.81, 85.59),\n",
       " (50.58, 85.16),\n",
       " (27.74, 43.12),\n",
       " (33.13, 52.72),\n",
       " (30.89, 48.71),\n",
       " (30.25, 47.57),\n",
       " (29.33, 45.94),\n",
       " (75.85, 134.76),\n",
       " (51.06, 86.08),\n",
       " (26.65, 41.2),\n",
       " (32.43, 51.46),\n",
       " (26.34, 40.67),\n",
       " (27.4, 42.52),\n",
       " (50.22, 84.47),\n",
       " (19.15, 28.34),\n",
       " (65.86, 114.84),\n",
       " (53.28, 90.32),\n",
       " (29.43, 46.11),\n",
       " (46.76, 77.91),\n",
       " (64.98, 113.1),\n",
       " (53.56, 90.85),\n",
       " (27.53, 42.75),\n",
       " (38.57, 62.65),\n",
       " (31.57, 49.93),\n",
       " (43.04, 70.92),\n",
       " (50.67, 85.34),\n",
       " (35.63, 57.25),\n",
       " (47.04, 78.43),\n",
       " (79.65, 142.43),\n",
       " (20.11, 29.95),\n",
       " (32.5, 51.59),\n",
       " (60.78, 104.86),\n",
       " (96.62, 177.26),\n",
       " (59.24, 101.85),\n",
       " (61.68, 106.61),\n",
       " (51.39, 86.71),\n",
       " (44.37, 73.41),\n",
       " (52.63, 89.07),\n",
       " (30.85, 48.65),\n",
       " (46.94, 78.24),\n",
       " (42.8, 70.47),\n",
       " (42.66, 70.22),\n",
       " (30.9, 48.73),\n",
       " (67.38, 117.84),\n",
       " (99.52, 183.31),\n",
       " (41.96, 68.92),\n",
       " (27.77, 43.18),\n",
       " (81.35, 145.88),\n",
       " (40.43, 66.07),\n",
       " (28.34, 44.17),\n",
       " (31.29, 49.43),\n",
       " (64.19, 111.55),\n",
       " (28.61, 44.65),\n",
       " (42.47, 69.85),\n",
       " (17.88, 26.21),\n",
       " (68.32, 119.71),\n",
       " (48.28, 80.79),\n",
       " (58.52, 100.44),\n",
       " (57.68, 98.82),\n",
       " (44.15, 73.01),\n",
       " (57.66, 98.79),\n",
       " (30.59, 48.17),\n",
       " (26.26, 40.52),\n",
       " (33.45, 53.3),\n",
       " (82.09, 147.38),\n",
       " (26.2, 40.43),\n",
       " (67.14, 117.37),\n",
       " (31.46, 49.73),\n",
       " (44.84, 74.3),\n",
       " (56.34, 96.23),\n",
       " (57.42, 98.31),\n",
       " (19.05, 28.18),\n",
       " (63.9, 110.98),\n",
       " (58.82, 101.03),\n",
       " (24.02, 36.63),\n",
       " (24.09, 36.76),\n",
       " (24.37, 37.25),\n",
       " (42.06, 69.1),\n",
       " (54.38, 92.43),\n",
       " (52.87, 89.54),\n",
       " (52.86, 89.51),\n",
       " (63.39, 109.96),\n",
       " (26.24, 40.49),\n",
       " (25.35, 38.94),\n",
       " (63.37, 109.92),\n",
       " (31.56, 49.91),\n",
       " (40.02, 65.32),\n",
       " (17.54, 25.66),\n",
       " (71.06, 125.16),\n",
       " (34.2, 54.66),\n",
       " (62.29, 107.81),\n",
       " (53.43, 90.62),\n",
       " (172.65, 342.13),\n",
       " (40.69, 66.56),\n",
       " (57.33, 98.14),\n",
       " (47.73, 79.75),\n",
       " (26.61, 41.14),\n",
       " (51.32, 86.56),\n",
       " (68.26, 119.58),\n",
       " (63.38, 109.95),\n",
       " (29.46, 46.17),\n",
       " (44.45, 73.56),\n",
       " (37.93, 61.47),\n",
       " (39.15, 63.7),\n",
       " (38.49, 62.5),\n",
       " (60.13, 103.58),\n",
       " (33.64, 53.65),\n",
       " (37.55, 60.76),\n",
       " (82.6, 148.41),\n",
       " (27.53, 42.76),\n",
       " (23.2, 35.22),\n",
       " (23.95, 36.51),\n",
       " (46.24, 76.93),\n",
       " (89.4, 162.34),\n",
       " (32.68, 51.92),\n",
       " (69.95, 122.95),\n",
       " (69.74, 122.52),\n",
       " (28.32, 44.14),\n",
       " (25.46, 39.12),\n",
       " (40.96, 67.05),\n",
       " (83.55, 150.35),\n",
       " (153.45, 299.35),\n",
       " (58.06, 99.55),\n",
       " (61.57, 106.39),\n",
       " (23.36, 35.5),\n",
       " (31.94, 50.59),\n",
       " (53.65, 91.03),\n",
       " (27.04, 41.89),\n",
       " (32.69, 51.94),\n",
       " (31.64, 50.04),\n",
       " (36.36, 58.59),\n",
       " (18.51, 27.26),\n",
       " (60.99, 105.26),\n",
       " (91.48, 166.62),\n",
       " (99.04, 182.3),\n",
       " (63.84, 110.86),\n",
       " (65.2, 113.53),\n",
       " (84.1, 151.48),\n",
       " (33.09, 52.66),\n",
       " (24.51, 37.49),\n",
       " (148.55, 288.57),\n",
       " (25.31, 38.87),\n",
       " (34.34, 54.92),\n",
       " (26.12, 40.28),\n",
       " (79.41, 141.95),\n",
       " (24.83, 38.04),\n",
       " (53.08, 89.93),\n",
       " (25.15, 38.59),\n",
       " (58.34, 100.1),\n",
       " (120.84, 228.38),\n",
       " (42.66, 70.21),\n",
       " (51.69, 87.28),\n",
       " (24.18, 36.91),\n",
       " (46.52, 77.46),\n",
       " (91.65, 166.98),\n",
       " (23.64, 35.98),\n",
       " (33.74, 53.82),\n",
       " (80.65, 144.46),\n",
       " (27.37, 42.48),\n",
       " (24.32, 37.15),\n",
       " (24.19, 36.93),\n",
       " (30.12, 47.34),\n",
       " (31.78, 50.31),\n",
       " (81.38, 145.94),\n",
       " (31.59, 49.96),\n",
       " (33.56, 53.51),\n",
       " (56.18, 95.92),\n",
       " (46.32, 77.08),\n",
       " (46.76, 77.9),\n",
       " (24.24, 37.02),\n",
       " (23.53, 35.79),\n",
       " (80.36, 143.87),\n",
       " (42.56, 70.02),\n",
       " (42.81, 70.49),\n",
       " (29.34, 45.95),\n",
       " (52.15, 88.16),\n",
       " (17.02, 24.8),\n",
       " (53.06, 89.9),\n",
       " (32.84, 52.2),\n",
       " (53.71, 91.14),\n",
       " (21.52, 32.35),\n",
       " (53.09, 89.95),\n",
       " (33.17, 52.81),\n",
       " (61.79, 106.82),\n",
       " (21.99, 33.15),\n",
       " (20.37, 30.4),\n",
       " (36.58, 59.0),\n",
       " (34.16, 54.59),\n",
       " (23.85, 36.33),\n",
       " (63.06, 109.32),\n",
       " (58.44, 100.29),\n",
       " (33.13, 52.72),\n",
       " (44.17, 73.03),\n",
       " (37.35, 60.41),\n",
       " (28.55, 44.56),\n",
       " (27.32, 42.38),\n",
       " (36.73, 59.26),\n",
       " (50.44, 84.88),\n",
       " (98.81, 181.82),\n",
       " (31.87, 50.46),\n",
       " (35.85, 57.67),\n",
       " (60.13, 103.59),\n",
       " (154.66, 302.03),\n",
       " (65.88, 114.88),\n",
       " (169.76, 335.65),\n",
       " (33.61, 53.59),\n",
       " (41.7, 68.42),\n",
       " (34.65, 55.49),\n",
       " ...)"
      ]
     },
     "execution_count": 1342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate price interval for MAPE median\n",
    "y_pred_interval_nn_reg = tuple([(round(math.exp(el-el*MAPE_median_nn_reg),2),round(math.exp(el+el*MAPE_median_nn_reg),2)) for el in y_test_pred_nn_reg])\n",
    "y_pred_interval_nn_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "#model_nn_seq.model_evaluation(model_nn_seq, skip_epochs=2, X_train=X_train, X_test=X_test)\n",
    "\n",
    "#score_nn_seq = model_nn_seq.evaluate(X_train_prep, y_train,verbose=1)\n",
    "#print(score_nn_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model and Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save best model and cv\n",
    "save_model(best_model_nn_reg, title=\"best_model_nn_reg_01\", save=\"joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation with Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_reg = [best_model_xgb_reg, best_model_svm_reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Transform X_test for final evaluation\n",
    "#X_test_prep = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Predict target with \"best model\"\n",
    "#y_pred_rf_reg = best_model_rf_reg.predict(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Final evaluation of \"best model\"\n",
    "#print(\"MSE: {:.2f}\".format(mean_squared_error(y_test, y_pred_rf_reg))),\n",
    "#print(\"RMSE: {:.2f}\".format(mean_squared_error(y_test, y_pred_rf_reg, squared=False))),\n",
    "#print(\"MAE: {:.2f}\".format(mean_absolute_error(y_test, y_pred_rf_reg))),\n",
    "#print(\"R2: {:.2f}\".format(r2_score(y_test, y_pred_rf_reg))),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate best model\n",
    "#fig, axes = plt.subplots(1, 2, figsize = (14, 6))\n",
    "#axes = axes.flatten()\n",
    "\n",
    "#y_pred = best_model.predict(X_test_prep)\n",
    "#axes[0].scatter(y_test, y_pred)\n",
    "#axes[0].set_xlabel('y_test')\n",
    "#axes[0].set_ylabel('y_pred')\n",
    "\n",
    "#coef = best_model.best_estimator_.named_steps['xgb'].coef_\n",
    "#mean_coef = np.mean(coef)\n",
    "#axes[1].plot(coef, 'o')\n",
    "#axes[1].set_xlabel('coefficient index')\n",
    "#axes[1].set_ylabel('coefficient size')\n",
    "#axes[1].axhline(y = mean_coef, color = 'red', linestyle = '--', alpha = 0.5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display confidence interval (scipy stats)\n",
    "#confidence = 0.95\n",
    "#squared_errors = (y_pred_rf_reg - y_test) ** 2\n",
    "#np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "#                         loc=squared_errors.mean(),\n",
    "#                         scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
